{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import statistics\n",
    "\n",
    "# stores your OpenAI API key in an environment variable\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(openai.api_key)\n",
    "\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14113c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_version_markdown(json_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return the 'markdown' field from the object\n",
    "    where 'latest_version' is 1.\n",
    "    \n",
    "    Parameters:\n",
    "        json_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        str: Markdown text of the latest version, or None if not found.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            if item.get(\"latest_version\") == 1:\n",
    "                return item.get(\"markdown\")\n",
    "    elif isinstance(data, dict):\n",
    "        if data.get(\"latest_version\") == 1:\n",
    "            return data.get(\"markdown\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict) and item.get(\"latest_version\") == 1:\n",
    "                        return item.get(\"markdown\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae261d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic bill analysis functions\n",
    "\n",
    "def getCountOfAIWords(bill_text):\n",
    "    \"\"\"\n",
    "    Count the number of times AI-related terms appear as complete words in the bill text.\n",
    "    \"\"\"\n",
    "    ai_terms = {\n",
    "        \"artificial intelligence\": r\"\\bartificial\\s+intelligence\\b\",\n",
    "        \"AI\": r\"\\bAI\\b\",  # Word boundary prevents matching inside other words\n",
    "        \"machine learning\": r\"\\bmachine\\s+learning\\b\",\n",
    "        \"deep learning\": r\"\\bdeep\\s+learning\\b\", \n",
    "        \"neural networks\": r\"\\bneural\\s+networks?\\b\",  # Handles network/networks\n",
    "        \"natural language processing\": r\"\\bnatural\\s+language\\s+processing\\b\",\n",
    "        \"computer vision\": r\"\\bcomputer\\s+vision\\b\",\n",
    "        \"automated decision-making\": r\"\\bautomated\\s+decision-?making\\b\",\n",
    "        \"LLM\": r\"\\bLLM\\b\",\n",
    "        \"large language model\": r\"\\blarge\\s+language\\s+models?\\b\",\n",
    "        \"generative AI\": r\"\\bgenerative\\s+AI\\b\",\n",
    "    }\n",
    "    \n",
    "    ai_count = {}\n",
    "    text_lower = bill_text.lower()\n",
    "    \n",
    "    for term, pattern in ai_terms.items():\n",
    "        matches = re.findall(pattern, text_lower, re.IGNORECASE)\n",
    "        ai_count[term] = len(matches)\n",
    "    \n",
    "    ai_count['total'] = sum(ai_count.values())\n",
    "    return ai_count\n",
    "\n",
    "def extract_ai_context_char_robust(bill_text, char_window=300):\n",
    "    \"\"\"\n",
    "    Character-based extraction with edge case handling.\n",
    "    \"\"\"\n",
    "    if not bill_text:\n",
    "        return []\n",
    "        \n",
    "    ai_pattern = r'\\bartificial\\s+intelligence\\b'\n",
    "    contexts = []\n",
    "    \n",
    "    for match in re.finditer(ai_pattern, bill_text, re.IGNORECASE):\n",
    "        start = max(0, match.start() - char_window)\n",
    "        end = min(len(bill_text), match.end() + char_window)\n",
    "        \n",
    "        # Avoid word truncation at boundaries\n",
    "        if start > 0:\n",
    "            # Find start of word to avoid cutting mid-word\n",
    "            while start > 0 and bill_text[start-1] not in ' \\n\\t.!?':\n",
    "                start -= 1\n",
    "                \n",
    "        if end < len(bill_text):\n",
    "            # Find end of word\n",
    "            while end < len(bill_text) and bill_text[end] not in ' \\n\\t.!?':\n",
    "                end += 1\n",
    "        \n",
    "        context = bill_text[start:end].strip()\n",
    "        \n",
    "        contexts.append({\n",
    "            'position': match.start(),\n",
    "            'char_range': f\"{start}-{end}\",\n",
    "            'context': context,\n",
    "            'matched_text': match.group(),\n",
    "            'at_beginning': start == 0,\n",
    "            'at_end': end >= len(bill_text)\n",
    "        })\n",
    "    \n",
    "    return contexts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8cdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based bill analysis functions\n",
    "\n",
    "async def rateBillOneToTenAsync(bill_text):\n",
    "    \"\"\"\n",
    "    Rate the bill text on a scale of 1 (least) to 10 (most) based on how much the bill is about AI / artificial intelligence.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"On a scale of 1 (least) - 10 (most), rate how much the following bill is about artificial intelligence.\\n\\nYou should respond with just a single integer, without any additional text.\"\"\"    \n",
    "\n",
    "    # OpenAI API call\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"chatgpt-4o-latest\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": bill_text}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return int(response.choices[0].message.content.strip())\n",
    "\n",
    "async def getThreeSentenceSummaryAsync(bill_text):\n",
    "    \"\"\"\n",
    "    Get a three-sentence summary of the bill text.\n",
    "    \"\"\"\n",
    "\n",
    "    # I ended up not summarizing the bills, but this function could be used to do so if you'd like\n",
    "\n",
    "    system_prompt = \"\"\"Summarize the following bill in three sentences: What are the primary topics of the bill, who if anyone will take different action based on the bill, and what actions (if any) are required to be taken by private sector. Be careful to distinguish between requirements, recommendations, positive incentives, etc.\\n\\nYou should respond with just the summary, without any additional text.\"\"\"    \n",
    "\n",
    "    # OpenAI API call\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"chatgpt-4o-latest\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": bill_text}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "async def rateBillOnSpecificPrivateSectorRequirementsAsync(bill_text):\n",
    "    \"\"\"\n",
    "    Rate the bill text on a scale of 1 (least) to 10 (most) based on how much the bill imposes specific requirements about artificial intelligence on private-sector actors, like the companies developing advanced artificial intelligence models.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"On a scale of 1 (least) - 10 (most), rate how much the following bill imposes specific requirements about artificial intelligence on private-sector developers of artificial intelligence, like OpenAI and Anthropic.\\n\\nYou should respond with just a single integer, without any additional text.\"\"\"\n",
    "\n",
    "    # OpenAI API call\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"chatgpt-4o-latest\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": bill_text}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    return int(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27324839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregator analysis functions (run multiple analyses on a bill, analyze multiple bills)\n",
    "\n",
    "# Running analyze_all_bills_in_folder will create a directory structure like:\n",
    "# bill_analysis_results/\n",
    "#   ├── ai_bill_analysis_incremental_20250630_143022.csv  (live progress tracking)\n",
    "#   ├── ai_bill_analysis_master_20250630_143022.json      (cumulative results)\n",
    "#   ├── AK2025000HCR3_analysis.json                       (individual bill result)\n",
    "#   ├── CA2025000SB47_analysis.json                       (individual bill result)\n",
    "#   └── ... (one JSON per analyzed bill)\n",
    "\n",
    "async def analyze_single_bill(json_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a single bill from a JSON file.\n",
    "    Returns a dictionary with analysis results.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'file_path': json_path,\n",
    "        'file_name': os.path.basename(json_path),\n",
    "        'ratings': [],\n",
    "        'average_rating': 0,\n",
    "        'summary': None,\n",
    "        'ai_word_counts': None,\n",
    "        'ai_contexts': None,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    max_file_size = 10 * 1024 * 1024  # 10 MB\n",
    "    \n",
    "    try:\n",
    "        # only do if file size is <10 MB due to OpenAI API rate-limit issues, excludes four bills that are larger than 10 MB\n",
    "        file_size = os.path.getsize(json_path)\n",
    "        if file_size > max_file_size:\n",
    "            result['error'] = \"File size\"\n",
    "            return result\n",
    "\n",
    "        # Get the markdown text\n",
    "        markdown_text = get_latest_version_markdown(json_path)\n",
    "        if not markdown_text:\n",
    "            result['error'] = \"No latest version found in JSON\"\n",
    "            return result\n",
    "        \n",
    "        # # Get an AI-relatedness rating; can get multiple ratings if desired\n",
    "        rating_tasks = [rateBillOneToTenAsync(markdown_text) for _ in range(1)]\n",
    "        # # rating_tasks = [rateBillOneToTenAsync(markdown_text) for _ in range(3)]\n",
    "        ratings = await asyncio.gather(*rating_tasks)\n",
    "        result['ratings'] = ratings\n",
    "        if len(result['ratings']) > 0:\n",
    "            result['average_rating'] = statistics.mean(ratings)\n",
    "        else:\n",
    "            result['average_rating'] = None\n",
    "            \n",
    "        # # Get AI word counts\n",
    "        result['ai_word_counts'] = getCountOfAIWords(markdown_text)\n",
    "        \n",
    "        # # Get AI contexts (the context around mentions of \"artificial intelligence\"; currently it is character-based which isn't quite long enough to make sense of\n",
    "        # would lengthen this out or switch to sentence-based extraction)\n",
    "        result['ai_contexts'] = extract_ai_context_char_robust(markdown_text)\n",
    "\n",
    "        # Requirement assessment section\n",
    "        # (should probably only run this on the subset that are in fact AI-related, in my case an average_rating of 6 or higher)\n",
    "        if result['average_rating'] >= 6:\n",
    "            requirement_assessment = await rateBillOnSpecificPrivateSectorRequirementsAsync(markdown_text)\n",
    "            result['requirement_assessment'] = requirement_assessment\n",
    "            # summarize the bill if you want\n",
    "            # result['summary'] = await getThreeSentenceSummaryAsync(markdown_text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "\n",
    "        print(f\"Error processing {json_path}: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "async def analyze_all_bills_in_folder(folder_path: str, max_concurrent: int = 2, output_dir: str = \"bill_analysis_results\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyze all JSON files in a folder with controlled concurrency.\n",
    "    Saves results incrementally as each bill is processed.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing JSON files\n",
    "        max_concurrent: Maximum number of concurrent API calls\n",
    "        output_dir: Directory to save incremental results\n",
    "    \n",
    "    Returns:\n",
    "        List of analysis results for each bill\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import csv\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create timestamp for this run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # # Initialize CSV file for incremental results\n",
    "    csv_file = output_path / f\"ai_bill_analysis_incremental_{timestamp}.csv\"\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Timestamp', 'File Name', 'Average Rating', 'Rating 1', 'Rating 2', 'Rating 3', 'AI Term Count', 'Summary (truncated)'])\n",
    "    \n",
    "    # Get all JSON files in the folder\n",
    "    json_files = list(Path(folder_path).glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to analyze\")\n",
    "    print(f\"Results will be saved to: {output_path}\")\n",
    "    \n",
    "    # Create a semaphore to limit concurrent requests\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    results = []\n",
    "    error_files = []\n",
    "    \n",
    "    async def analyze_with_semaphore(json_path, index):\n",
    "        async with semaphore:\n",
    "            print(f\"[{index+1}/{len(json_files)}] Analyzing {json_path.name}...\")\n",
    "            result = await analyze_single_bill(str(json_path))\n",
    "\n",
    "            if result['error']:\n",
    "                # add filename to error list for a retry later\n",
    "                error_files.append((json_path.name, result['error']))\n",
    "                print(f\"Error processing {json_path.name}: {result['error']}\")\n",
    "            \n",
    "            # Save individual result as JSON\n",
    "            individual_result_file = output_path / f\"{json_path.stem}_analysis.json\"\n",
    "            with open(individual_result_file, 'w') as f:\n",
    "                json.dump(result, f, indent=2, default=str)\n",
    "            \n",
    "            # # Append to CSV file\n",
    "            with open(csv_file, 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                ai_count = result['ai_word_counts']['total'] if result['ai_word_counts'] else 0\n",
    "                summary_truncated = result['summary'][:200] + '...' if result['summary'] and len(result['summary']) > 200 else result['summary']\n",
    "                writer.writerow([\n",
    "                    datetime.now().isoformat(),\n",
    "                    result['file_name'],\n",
    "                    f\"{result['average_rating']:.1f}\",\n",
    "                    result['ratings'][0] if len(result['ratings']) > 0 else 'N/A',\n",
    "                    result['ratings'][1] if len(result['ratings']) > 1 else 'N/A',\n",
    "                    result['ratings'][2] if len(result['ratings']) > 2 else 'N/A',\n",
    "                    ai_count,\n",
    "                    summary_truncated\n",
    "                ])\n",
    "            \n",
    "            # Update master results file\n",
    "            master_file = output_path / f\"ai_bill_analysis_master_{timestamp}.json\"\n",
    "            results.append(result)\n",
    "            with open(master_file, 'w') as f:\n",
    "                json.dump(results, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"[{index+1}/{len(json_files)}] Completed {json_path.name} - Average rating: {result['average_rating']:.1f} - Saved to {individual_result_file.name}\")\n",
    "            # print for the requirement assessment\n",
    "            if 'requirement_assessment' in result:\n",
    "                # (there's a chance this printline errors out - I ran requirement assessment separately from average rating, so didn't have both printlines intact at once)\n",
    "                print(f\"  - Requirement Assessment: {result['requirement_assessment']}\")\n",
    "            return result\n",
    "    \n",
    "    # Analyze all bills\n",
    "    tasks = [analyze_with_semaphore(json_file, i) for i, json_file in enumerate(json_files)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nAnalysis complete!\")\n",
    "    print(f\"Total bills analyzed: {len(results)}\")\n",
    "    # print(f\"Bills with high AI relevance (≥7): {len(high_ai_bills)}\")\n",
    "    print(f\"\\nAll results saved to: {output_path}\")\n",
    "    print(f\"  - Individual JSONs: *_analysis.json\")\n",
    "    # print(f\"  - Incremental CSV: {csv_file.name}\")\n",
    "    print(f\"  - Master JSON: ai_bill_analysis_master_{timestamp}.json\")\n",
    "    \n",
    "    return results, error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0695543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to run the analysis:\n",
    "\n",
    "folder_path = \"bill_texts\"\n",
    "output_dir = \"bill_texts_results\"  # Results will be saved here incrementally\n",
    "\n",
    "# This will save results as each bill is processed\n",
    "results, error_files = await analyze_all_bills_in_folder(\n",
    "    folder_path, \n",
    "    max_concurrent=2,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# add print-lines as wanted\n",
    "for result in results:\n",
    "    if result['average_rating'] >= 0:\n",
    "        print(f\"\\nFile: {result['file_name']}\")\n",
    "        print(f\"Ratings: {result['ratings']} (Average: {result['average_rating']:.1f})\")\n",
    "        if result['summary']:\n",
    "            print(f\"\\nSummary:\\n{result['summary']}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "# get a file with errors so you know which if any bills to re-run\n",
    "# a few are too long for standard OpenAI API rate-limits, but more commonly they're fine so long as you haven't done too many tokens in close proximity\n",
    "# standard gpt-4o ratelimit is 30K tokens per minute \n",
    "error_file_path = output_dir + \"/error_files.txt\"\n",
    "with open(error_file_path, 'w') as f:\n",
    "    for file_name, error in error_files:\n",
    "        f.write(f\"{file_name}: {error}\\n\")\n",
    "        print(f\"  - {file_name}: {error}\")\n",
    "\n",
    "print(\"Careful! If you are doing any analysis, make sure to exclude bills that are rated 0/10 on AI-relatedness or requirements, as 0/10 is a sign of an error, not an actual rating.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725505f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder with these files in it\n",
    "bills_to_analyze = [\n",
    "    \"FILL_IN_WITH_PATHS.json\",\n",
    "]\n",
    "\n",
    "# create a new folder with these files in it\n",
    "output_folder = \"folder_to_use_as_input_to_analysis\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "for bill in bills_to_analyze:\n",
    "    bill_path = Path(\"bill_texts\") / bill\n",
    "    if bill_path.exists():\n",
    "        with open(bill_path, 'r') as f:\n",
    "            bill_data = json.load(f)\n",
    "            with open(Path(output_folder) / bill, 'w') as out_f:\n",
    "                json.dump(bill_data, out_f, indent=2)\n",
    "    else:\n",
    "        print(f\"Bill file {bill} does not exist in the original folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis of a single bill, specified by path\n",
    "bill_path = \"bill_texts/NY2025000A6453.json\"\n",
    "bill_text = get_latest_version_markdown(bill_path)\n",
    "# now run async analysis on whatever you'd like\n",
    "rating = await rateBillOneToTenAsync(bill_text)\n",
    "print(f\"Rating for {bill_path}: {rating}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "state-ai-bill-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
