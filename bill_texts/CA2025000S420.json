{
  "bill_id": "CA2025000S420",
  "source_url": "http://custom.statenet.com/public/resources.cgi?id=ID:bill:CA2025000S420&cuiq=93d84396-c63b-526a-b152-38b7f79b4cfd&client_md=e4f6fea4-27b4-5d41-b7d3-766fe52569f0",
  "versions": [
    {
      "date": "02/18/2025",
      "label": "Introduced",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:CA2025000S420&verid=CA2025000S420_20250218_0_I&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 CA S 420</td> <td><table><tr><td class=\"label\">Author:</td> <td>Padilla</td></tr> <tr><td class=\"label\">Version:</td> <td>Introduced</td></tr> <tr><td class=\"label\">Version Date:</td> <td>02/18/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">CALIFORNIA LEGISLATURE--2025-2026 REGULAR SESSION</p>\n   <p class=\"left\">\n    <b> Senate Bill </b>\n   </p>\n   <p class=\"right\">\n    <b> No. 420 </b>\n   </p>\n   <p class=\"center\">\n    <b>Introduced by</b> Senator Padilla</p>\n   <p class=\"center\"> February 18, 2025 </p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"center\"> An act relating to artificial intelligence.</p>\n  </div>\n  <a name=\"digest_document_section\"></a><div class=\"digest\">\n   <p class=\"center\">LEGISLATIVE COUNSEL&#39;S DIGEST</p>\n   <p class=\"center\">SB 420, as introduced, Padilla. Individual rights.</p>\n   <p class=\"indent\">The California AI Transparency Act requires a covered provider, as defined, of a generative artificial intelligence system to make available an AI detection tool at no cost to the user that meets certain criteria, including that the tool outputs any system provenance data, as defined, that is detected in the content. The California Consumer Privacy Act of 2018 grants a consumer various rights with respect to personal information that is collected or sold by a business, as defined, including the right to direct a business that sells or shares personal information about the consumer to third parties not to sell or share the consumer&#39;s personal information, as specified.</p>\n   <p class=\"indent\">This bill would express the intent of the Legislature to enact legislation that would relate to strengthening, establishing, and promoting certain rights and values related to artificial intelligence.</p>\n   <p class=\"indent\"> Vote Required: MAJORITY Appropriation: NO Fiscal Committee: NO Local Program: NO Immediate Effect NO Urgency: NO Tax Levy: NO Election: NO Usual Current Expenses: NO Budget Bill: NO Prop 25 Trailer Bill: NO </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">The people of the State of California do enact as follows:</p>\n   </span>\n   <p class=\"indent\">SECTION 1. The Legislature finds and declares all of the following:</p>\n   <p class=\"indent\">(a) (1) Artificial intelligence technologies are becoming an integral part of daily life in California and have profound implications for privacy, equity, fairness, and public safety.</p>\n   <p class=\"indent\">(2) It is critical to protect individuals&#39; rights to safeguard against potential harms, including discrimination, privacy violations, and unchecked automation in critical decisionmaking processes.</p>\n   <p class=\"indent\">(3) A comprehensive set of rights must be established to ensure artificial intelligence technologies align with the public interest and reflect the values of California residents.</p>\n   <p class=\"indent\">(b) (1) Individuals should have the right to receive a clear and accessible explanation about how artificial intelligence systems operate, including the data they use and the decisions they make.</p>\n   <p class=\"indent\"> (2) An entity that uses artificial intelligence systems to make decisions impacting California residents should provide a mechanism to inform individuals of the system&#39;s logic, processing methods, and intended outcomes in a manner that is understandable.</p>\n   <p class=\"indent\">(c) (1) All individuals have the right to control their personal data in relation to artificial intelligence systems. Artificial intelligence systems should operate with the highest standards of data privacy and security, in line with the California Consumer Privacy Act of 2018 and other relevant privacy laws.</p>\n   <p class=\"indent\">(2) Before personal data is used in artificial intelligence systems, entities should obtain informed, explicit consent from individuals, and individuals should have the right to withdraw consent at any time without penalty.</p>\n   <p class=\"indent\">(3) Entities should ensure that personal data used by artificial intelligence systems is anonymized or pseudonymized if feasible, and data retention should be limited to the purposes for which the data was initially collected.</p>\n   <p class=\"indent\">(d) (1) Artificial intelligence systems should not discriminate against individuals based on race, gender, sexual orientation, disability, religion, socioeconomic status, or other protected characteristics under California law.</p>\n   <p class=\"indent\"> (2) Entities deploying artificial intelligence technologies should perform regular audits to identify and address any biases or inequities in their artificial intelligence systems and should ensure that artificial intelligence systems are designed and trained to promote fairness and equal treatment.</p>\n   <p class=\"indent\">(e) (1) Individuals should have the right to hold entities accountable for any harm caused by artificial intelligence systems, and entities should be liable for the actions and decisions made by artificial intelligence technologies they deploy.</p>\n   <p class=\"indent\">(2) An individual or group adversely affected by artificial intelligence-driven decisions should have access to a straightforward and transparent process for seeking redress, including the ability to challenge those decisions through human review and appeal mechanisms.</p>\n   <p class=\"indent\">(f) (1) Individuals should have the right to request human oversight for significant decisions made by artificial intelligence systems that impact them, particularly in areas such as employment, health care, housing, education, and criminal justice.</p>\n   <p class=\"indent\">(2) Artificial intelligence systems in high-stakes decisionmaking contexts should involve human review or intervention before final decisions, ensuring that automated decisions align with human values and public policy goals.</p>\n   <p class=\"indent\">SEC. 2. It is the intent of the Legislature to enact legislation that would relate to strengthening, establishing, and promoting the rights and values described in Section 1 of this act.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 CA S 420 | | Author: | Padilla  \n---|---  \nVersion: | Introduced  \nVersion Date: | 02/18/2025  \n  \nCALIFORNIA LEGISLATURE--2025-2026 REGULAR SESSION\n\n**Senate Bill**\n\n**No. 420**\n\n**Introduced by** Senator Padilla\n\nFebruary 18, 2025\n\nAn act relating to artificial intelligence.\n\nLEGISLATIVE COUNSEL'S DIGEST\n\nSB 420, as introduced, Padilla. Individual rights.\n\nThe California AI Transparency Act requires a covered provider, as defined, of\na generative artificial intelligence system to make available an AI detection\ntool at no cost to the user that meets certain criteria, including that the\ntool outputs any system provenance data, as defined, that is detected in the\ncontent. The California Consumer Privacy Act of 2018 grants a consumer various\nrights with respect to personal information that is collected or sold by a\nbusiness, as defined, including the right to direct a business that sells or\nshares personal information about the consumer to third parties not to sell or\nshare the consumer's personal information, as specified.\n\nThis bill would express the intent of the Legislature to enact legislation\nthat would relate to strengthening, establishing, and promoting certain rights\nand values related to artificial intelligence.\n\nVote Required: MAJORITY Appropriation: NO Fiscal Committee: NO Local Program:\nNO Immediate Effect NO Urgency: NO Tax Levy: NO Election: NO Usual Current\nExpenses: NO Budget Bill: NO Prop 25 Trailer Bill: NO\n\nThe people of the State of California do enact as follows:\n\nSECTION 1. The Legislature finds and declares all of the following:\n\n(a) (1) Artificial intelligence technologies are becoming an integral part of\ndaily life in California and have profound implications for privacy, equity,\nfairness, and public safety.\n\n(2) It is critical to protect individuals' rights to safeguard against\npotential harms, including discrimination, privacy violations, and unchecked\nautomation in critical decisionmaking processes.\n\n(3) A comprehensive set of rights must be established to ensure artificial\nintelligence technologies align with the public interest and reflect the\nvalues of California residents.\n\n(b) (1) Individuals should have the right to receive a clear and accessible\nexplanation about how artificial intelligence systems operate, including the\ndata they use and the decisions they make.\n\n(2) An entity that uses artificial intelligence systems to make decisions\nimpacting California residents should provide a mechanism to inform\nindividuals of the system's logic, processing methods, and intended outcomes\nin a manner that is understandable.\n\n(c) (1) All individuals have the right to control their personal data in\nrelation to artificial intelligence systems. Artificial intelligence systems\nshould operate with the highest standards of data privacy and security, in\nline with the California Consumer Privacy Act of 2018 and other relevant\nprivacy laws.\n\n(2) Before personal data is used in artificial intelligence systems, entities\nshould obtain informed, explicit consent from individuals, and individuals\nshould have the right to withdraw consent at any time without penalty.\n\n(3) Entities should ensure that personal data used by artificial intelligence\nsystems is anonymized or pseudonymized if feasible, and data retention should\nbe limited to the purposes for which the data was initially collected.\n\n(d) (1) Artificial intelligence systems should not discriminate against\nindividuals based on race, gender, sexual orientation, disability, religion,\nsocioeconomic status, or other protected characteristics under California law.\n\n(2) Entities deploying artificial intelligence technologies should perform\nregular audits to identify and address any biases or inequities in their\nartificial intelligence systems and should ensure that artificial intelligence\nsystems are designed and trained to promote fairness and equal treatment.\n\n(e) (1) Individuals should have the right to hold entities accountable for any\nharm caused by artificial intelligence systems, and entities should be liable\nfor the actions and decisions made by artificial intelligence technologies\nthey deploy.\n\n(2) An individual or group adversely affected by artificial intelligence-\ndriven decisions should have access to a straightforward and transparent\nprocess for seeking redress, including the ability to challenge those\ndecisions through human review and appeal mechanisms.\n\n(f) (1) Individuals should have the right to request human oversight for\nsignificant decisions made by artificial intelligence systems that impact\nthem, particularly in areas such as employment, health care, housing,\neducation, and criminal justice.\n\n(2) Artificial intelligence systems in high-stakes decisionmaking contexts\nshould involve human review or intervention before final decisions, ensuring\nthat automated decisions align with human values and public policy goals.\n\nSEC. 2. It is the intent of the Legislature to enact legislation that would\nrelate to strengthening, establishing, and promoting the rights and values\ndescribed in Section 1 of this act.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "03/26/2025",
      "label": "Amended",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:CA2025000S420&verid=CA2025000S420_20250326_0_A&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 CA S 420</td> <td><table><tr><td class=\"label\">Author:</td> <td>Padilla</td></tr> <tr><td class=\"label\">Version:</td> <td>Amended</td></tr> <tr><td class=\"label\">Version Date:</td> <td>03/26/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\"> AMENDED IN SENATE March 26, 2025 </p>\n   <p class=\"center\">CALIFORNIA LEGISLATURE--2025-2026 REGULAR SESSION</p>\n   <p class=\"left\">\n    <b> Senate Bill </b>\n   </p>\n   <p class=\"right\">\n    <b> No. 420 </b>\n   </p>\n   <p class=\"center\">\n    <b>Introduced by</b> Senator Padilla</p>\n   <p class=\"center\"> February 18, 2025 </p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"center\"> An act<u class=\"amendmentInsertedText\"> to add Chapter 24.6 (commencing with Section 22756) to Division 8 of the Business and Professions Code, and to add Article 11 (commencing with Section 10285.8) to Chapter 1 of Part 2 of Division 2 of the Public Contract Code,</u> relating to artificial intelligence.</p>\n  </div>\n  <a name=\"digest_document_section\"></a><div class=\"digest\">\n   <p class=\"center\">LEGISLATIVE COUNSEL&#39;S DIGEST</p>\n   <p class=\"center\">SB 420, as amended, Padilla. <strike class=\"amendmentDeletedText\">Individual rights. </strike>\n    <u class=\"amendmentInsertedText\">Automated decision systems.</u>\n   </p>\n   <p class=\"indent\">The California AI Transparency Act requires a covered provider, as defined, of a generative artificial intelligence system to make available an AI detection tool at no cost to the user that meets certain criteria, including that the tool outputs any system provenance data, as defined, that is detected in the content. The California Consumer Privacy Act of 2018 grants a consumer various rights with respect to personal information that is collected or sold by a business, as defined, including the right to direct a business that sells or shares personal information about the consumer to third parties not to sell or share the consumer&#39;s personal information, as specified.</p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">This bill would express the intent of the Legislature to enact legislation that would relate to strengthening, establishing, and promoting certain rights and values related to artificial intelligence.</strike>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">This bill would generally regulate a developer or a deployer of a high-risk automated decision system, as defined, including by requiring a developer or a deployer to perform an impact assessment on the high-risk automated decision system before making it publicly available or deploying it, as prescribed. The bill would require a state agency to require a developer of a high-risk automated decision system deployed by the state agency to provide to the state agency a copy of the impact assessment and would require the state agency to keep that impact assessment confidential. The bill would also require a developer to provide to the Attorney General or Civil Rights Department, within 30 days of a request from the Attorney General or the Civil Rights Department, a copy of an impact assessment and would require the impact assessment to be kept confidential.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">This bill would authorize the Attorney General or the Civil Rights Department to bring a specified civil action to enforce compliance with the bill, as prescribed, and would authorize a developer or deployer to cure, within 45 days of receiving a certain notice of a violation, the noticed violation and provide an express written statement, made under penalty of perjury, that the violation has been cured. By expanding the scope of the crime of perjury, this bill would impose a state-mandated local program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">This bill would prohibit a state agency from awarding a contract for a high-risk automated decision system to a person who has violated, among other civil rights laws, the bill.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Existing constitutional provisions require that a statute that limits the right of access to the meetings of public bodies or the writings of public officials and agencies be adopted with findings demonstrating the interest protected by the limitation and the need for protecting that interest.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">This bill would make legislative findings to that effect.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">This bill would provide that no reimbursement is required by this act for a specified reason.</u>\n   </p>\n   <p class=\"indent\"> Vote Required: MAJORITY Appropriation: NO Fiscal Committee: <strike class=\"amendmentDeletedText\">NO</strike>\n    <u class=\"amendmentInsertedText\">YES</u> Local Program: <strike class=\"amendmentDeletedText\">NO</strike>\n    <u class=\"amendmentInsertedText\">YES</u> Immediate Effect NO Urgency: NO Tax Levy: NO Election: NO Usual Current Expenses: NO Budget Bill: NO Prop 25 Trailer Bill: NO </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">The people of the State of California do enact as follows:</p>\n   </span>\n   <p class=\"indent\">SECTION 1. The Legislature finds and declares all of the following:</p>\n   <p class=\"indent\">(a) (1) Artificial intelligence technologies are becoming an integral part of daily life in California and have profound implications for privacy, equity, fairness, and public safety.</p>\n   <p class=\"indent\">(2) It is critical to protect individuals&#39; rights to safeguard against potential harms, including discrimination, privacy violations, and unchecked automation in critical decisionmaking processes.</p>\n   <p class=\"indent\">(3) A comprehensive set of rights must be established to ensure artificial intelligence technologies align with the public interest and reflect the values of California residents.</p>\n   <p class=\"indent\">(b) (1) Individuals should have the right to receive a clear and accessible explanation about how artificial intelligence systems operate, including the data they use and the decisions they make.</p>\n   <p class=\"indent\"> (2) An entity that uses artificial intelligence systems to make decisions impacting California residents should provide a mechanism to inform individuals of the system&#39;s logic, processing methods, and intended outcomes in a manner that is understandable.</p>\n   <p class=\"indent\">(c) (1) All individuals have the right to control their personal data in relation to artificial intelligence systems. Artificial intelligence systems should operate with the highest standards of data privacy and security, in line with the California Consumer Privacy Act of 2018 and other relevant privacy laws.</p>\n   <p class=\"indent\">(2) Before personal data is used in artificial intelligence systems, entities should obtain informed, explicit consent from individuals, and individuals should have the right to withdraw consent at any time without penalty.</p>\n   <p class=\"indent\">(3) Entities should ensure that personal data used by artificial intelligence systems is anonymized or pseudonymized if feasible, and data retention should be limited to the purposes for which the data was initially collected.</p>\n   <p class=\"indent\">(d) (1) Artificial intelligence systems should not discriminate against individuals based on race, gender, sexual orientation, disability, religion, socioeconomic status, or other protected characteristics under California law.</p>\n   <p class=\"indent\"> (2) Entities deploying artificial intelligence technologies should perform regular audits to identify and address any biases or inequities in their artificial intelligence systems and should ensure that artificial intelligence systems are designed and trained to promote fairness and equal treatment.</p>\n   <p class=\"indent\">(e) (1) Individuals should have the right to hold entities accountable for any harm caused by artificial intelligence systems, and entities should be liable for the actions and decisions made by artificial intelligence technologies they deploy.</p>\n   <p class=\"indent\">(2) An individual or group adversely affected by artificial intelligence-driven decisions should have access to a straightforward and transparent process for seeking redress, including the ability to challenge those decisions through human review and appeal mechanisms.</p>\n   <p class=\"indent\">(f) (1) Individuals should have the right to request human oversight for significant decisions made by artificial intelligence systems that impact them, particularly in areas such as employment, health care, housing, education, and criminal justice.</p>\n   <p class=\"indent\">(2) Artificial intelligence systems in high-stakes decisionmaking contexts should involve human review or intervention before final decisions, ensuring that automated decisions align with human values and public policy goals.</p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">SEC. 2. It is the intent of the Legislature to enact legislation that would relate to strengthening, establishing, and promoting the rights and values described in Section 1 of this act.</strike>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">SEC. 2. Chapter 24.6 (commencing with Section 22756) is added to Division 8 of the Business and Professions Code, to read:</u>\n   </p>\n   <p class=\"indent\">24.6. Automated Decision Systems</p>\n   <p class=\"indent\">22756. As used in this chapter:</p>\n   <p class=\"indent\">(a) &quot;Algorithmic discrimination&quot; means the condition in which an automated decision system contributes to unlawful discrimination on the basis of a protected classification.</p>\n   <p class=\"indent\">(b) &quot;Artificial intelligence&quot; means an engineered or machine-based system that varies in its level of autonomy and that can, for explicit or implicit objectives, infer from the input it receives how to generate outputs that can influence physical or virtual environments.</p>\n   <p class=\"indent\">(c) (1) &quot;Automated decision system&quot; means a computational process derived from machine learning, statistical modeling, data analytics, or artificial intelligence that issues simplified output, including a score, classification, or recommendation, that is used to assist or replace human discretionary decisionmaking and materially impacts natural persons.</p>\n   <p class=\"indent\">(2) &quot;Automated decision system&quot; does not mean a spam email filter, firewall, antivirus software, identity and access management tool, calculator, database, dataset, or other compilation of data.</p>\n   <p class=\"indent\">(d) &quot;Deployer&quot; means a natural person or entity that uses a high-risk automated decision system in the state.</p>\n   <p class=\"indent\">(e) &quot;Detecting decisionmaking patterns without influencing outcomes&quot; means the act of artificial intelligence analyzing patterns for informational purposes without direct influence on decisions.</p>\n   <p class=\"indent\">(f) &quot;Developer&quot; means a natural person or entity that designs, codes, produces, or substantially modifies a high-risk automated decision system for use in the state.</p>\n   <p class=\"indent\">(g) &quot;Education enrollment or opportunity&quot; means the chance to obtain admission, accreditation, evaluation, certification, vocational training, financial aid, or scholarships with respect to an educational opportunity.</p>\n   <p class=\"indent\">(h) &quot;Employment or employment opportunity&quot; means hiring, salary, wage, or other material term, condition, or privilege of an employee&#39;s employment.</p>\n   <p class=\"indent\">(i) &quot;Health care&quot; means health care services or insurance for health, mental health, dental, or vision.</p>\n   <p class=\"indent\">(j) (1) &quot;High-risk automated decision system&quot; means an automated decision system that is used to assist or replace human discretionary decisions that have a legal or similarly significant effect, including decisions that materially impact access to, or approval for, any of the following:</p>\n   <p class=\"indent\">(A) Education enrollment or opportunity.</p>\n   <p class=\"indent\">(B) Employment or employment opportunity.</p>\n   <p class=\"indent\">(C) Essential utilities.</p>\n   <p class=\"indent\">(D) Temporary, short-term, or long-term housing.</p>\n   <p class=\"indent\">(E) Health care services.</p>\n   <p class=\"indent\">(F) Lending services.</p>\n   <p class=\"indent\">(G) A legal right or service.</p>\n   <p class=\"indent\">(H) An essential government service.</p>\n   <p class=\"indent\">(2) &quot;High-risk automated decision system&quot; does not include an automated decision system that only performs narrow procedural tasks, enhances human activities, detects patterns without influencing decisions, or assists in preparatory tasks for assessment.</p>\n   <p class=\"indent\">(k) &quot;Improving results of previously completed human activities&quot; means the act of artificial intelligence enhancing existing human-performed tasks without altering decisions.</p>\n   <p class=\"indent\">(l) &quot;Narrow procedural task&quot; means a limited, procedural task that has a minimal impact on outcomes.</p>\n   <p class=\"indent\">(m) &quot;Preparatory task for assessment&quot; means a task in which an artificial intelligence aids in a preparatory task for assessment or evaluation without direct decisionmaking authority.</p>\n   <p class=\"indent\"> (n) &quot;Protected classification&quot; means a classification protected under existing law prohibiting discrimination, including, but not limited to, the Fair Employment and Housing Act (Chapter 7 (commencing with Section 12960) of Part 2.8 of Division 3 of Title 2 of the Government Code) or the Unruh Civil Rights Act (Section 51 of the Civil Code).</p>\n   <p class=\"indent\">(o) (1) &quot;State agency&quot; means any of the following:</p>\n   <p class=\"indent\">(A) A state office, department, division, or bureau.</p>\n   <p class=\"indent\">(B) The California State University.</p>\n   <p class=\"indent\">(C) The Board of Parole Hearings.</p>\n   <p class=\"indent\">(D) A board or other professional licensing and regulatory body under the administration or oversight of the Department of Consumer Affairs.</p>\n   <p class=\"indent\">(2) &quot;State agency&quot; does not include the University of California, the Legislature, the judicial branch, or a board that is not described in paragraph (1).</p>\n   <p class=\"indent\">(p) &quot;Substantial modification&quot; means a new version, release, or other significant update that materially changes the functionality or performance of a high-risk automated decision system, including the results of retraining.</p>\n   <p class=\"indent\">22756.1. (a) (1) For a high-risk automated decision system made publicly available for use on or after January 1, 2026, a developer shall perform an impact assessment on the high-risk automated decision system before making the high-risk automated decision system publicly available for use.</p>\n   <p class=\"indent\">(2) For a high-risk automated decision system first made publicly available for use before January 1, 2026, a developer shall perform an impact assessment if the developer makes a substantial modification to the high-risk automated decision system.</p>\n   <p class=\"indent\">(b) (1) Except as provided in paragraph (2), for a high-risk automated decision system first deployed after January 1, 2026, a deployer shall perform an impact assessment within two years of deploying the high-risk automated decision system.</p>\n   <p class=\"indent\">(2) A state agency that is a deployer may opt out of performing an impact assessment if the state agency uses the automated decision system only for its intended use as determined by the developer and all of the following requirements are met:</p>\n   <p class=\"indent\">(A) The state agency does not make a substantial modification to the high-risk automated decision system.</p>\n   <p class=\"indent\">(B) The developer of the high-risk automated decision system is in compliance with Section 10285.8 of the Public Contract Code and subdivision (d).</p>\n   <p class=\"indent\">(C) The state agency does not have a reasonable basis to believe that deployment of the high-risk automated decision system as intended by the developer is likely to result in algorithmic discrimination.</p>\n   <p class=\"indent\">(D) The state agency is in compliance with Section 22756.3.</p>\n   <p class=\"indent\">(c) (1) A developer shall make available to deployers and potential deployers the statements included in the developer&#39;s impact assessment pursuant to paragraph (2).</p>\n   <p class=\"indent\">(2) An impact assessment prepared pursuant to this section shall include all of the following:</p>\n   <p class=\"indent\">(A) A statement of the purpose of the high-risk automated decision system and its intended benefits, intended uses, and intended deployment contexts.</p>\n   <p class=\"indent\">(B) A description of the high-risk automated decision system&#39;s intended outputs.</p>\n   <p class=\"indent\">(C) A summary of the types of data intended to be used as inputs to the high-risk automated decision system and any processing of those data inputs recommended to ensure the intended functioning of the high-risk automated decision system.</p>\n   <p class=\"indent\">(D) A summary of reasonably foreseeable potential disproportionate or unjustified impacts on a protected classification from the intended use by deployers of the high-risk automated decision system.</p>\n   <p class=\"indent\">(E) A developer&#39;s impact assessment shall also include both of the following:</p>\n   <p class=\"indent\">(i) A description of safeguards implemented or other measures taken by the developer to mitigate and guard against risks known to the developer of algorithmic discrimination arising from the use of the high-risk automated decision system.</p>\n   <p class=\"indent\">(ii) A description of how the high-risk automated decision system can be monitored by a deployer for risks of algorithmic discrimination known to the developer.</p>\n   <p class=\"indent\">(F) A statement of the extent to which the deployer&#39;s use of the high-risk automated decision system is consistent with, or varies from, the developer&#39;s statement of the high-risk automated decision system&#39;s purpose and intended benefits, intended uses, and intended deployment contexts.</p>\n   <p class=\"indent\">(G) A description of safeguards implemented or other measures taken to mitigate and guard against any known risks to the deployer of discrimination arising from the high-risk automated decision system.</p>\n   <p class=\"indent\">(H) A description of how the high-risk automated decision system has been, and will be, monitored and evaluated.</p>\n   <p class=\"indent\">(d) (1) A state agency shall require a developer of a high-risk automated decision system deployed by the state agency to provide to the state agency a copy of the impact assessment conducted pursuant to this section.</p>\n   <p class=\"indent\">(2) Notwithstanding any other law, an impact assessment provided to a state agency pursuant to this subdivision shall be kept confidential.</p>\n   <p class=\"indent\">22756.2. (a) If a deployer uses a high-risk automated decision system to make a decision regarding a natural person, the deployer shall notify the natural person of that fact and disclose to that natural person all of the following:</p>\n   <p class=\"indent\">(1) The purpose of the high-risk automated decision system and the specific decision it was used to make.</p>\n   <p class=\"indent\">(2) How the high-risk automated decision system was used to make the decision.</p>\n   <p class=\"indent\">(3) The type of data used by the high-risk automated decision system.</p>\n   <p class=\"indent\">(4) Contact information for the deployer.</p>\n   <p class=\"indent\">(5) A link to the statement required by subdivision (b).</p>\n   <p class=\"indent\">(b) A deployer shall make available on its internet website a statement summarizing all of the following:</p>\n   <p class=\"indent\">(1) The types of high-risk automated decision systems it currently deploys.</p>\n   <p class=\"indent\">(2) How the deployer manages known or reasonably foreseeable risks of algorithmic discrimination arising from the deployment of those high-risk automated decision systems.</p>\n   <p class=\"indent\">(3) The nature and source of the information collected and used by the high-risk automated decision systems deployed by the deployer.</p>\n   <p class=\"indent\">(c) A deployer shall provide, as technically feasible, a natural person that is the subject of a decision made by a high-risk automated decision system an opportunity to appeal that decision for review by a natural person.</p>\n   <p class=\"indent\">22756.3. (a) A developer or a deployer shall establish, document, implement, and maintain a governance program that contains reasonable administrative and technical safeguards to govern the reasonably foreseeable risks of algorithmic discrimination associated with the use, or intended use, of a high-risk automated decision system.</p>\n   <p class=\"indent\">(b) The governance program required by this subdivision shall be appropriately designed with respect to all of the following:</p>\n   <p class=\"indent\">(1) The use, or intended use, of the high-risk automated decision system.</p>\n   <p class=\"indent\">(2) The size, complexity, and resources of the deployer or developer.</p>\n   <p class=\"indent\">(3) The nature, context, and scope of the activities of the deployer or developer in connection with the high-risk automated decision system.</p>\n   <p class=\"indent\">(4) The technical feasibility and cost of available tools, assessments, and other means used by a deployer or developer to map, measure, manage, and govern the risks associated with a high-risk automated decision system.</p>\n   <p class=\"indent\">22756.4. A developer or deployer is not required to disclose information under this chapter if the disclosure of that information would result in the waiver of a legal privilege or the disclosure of a trade secret, as defined in Section 3426.1 of the Civil Code.</p>\n   <p class=\"indent\">22756.5. (a) Except as provided in subdivision (b), a deployer or developer shall not deploy or make available for deployment a high-risk automated decision system if the impact assessment performed pursuant to this chapter determines that the high-risk automated decision system is likely to result in algorithmic discrimination.</p>\n   <p class=\"indent\">(b) (1) A deployer or developer may deploy or make available for deployment a high-risk automated decision system if the impact assessment performed pursuant to this chapter determines that the high-risk automated decision system will result in algorithmic discrimination if the deployer or developer implements safeguards to mitigate the known risks of algorithmic discrimination.</p>\n   <p class=\"indent\"> (2) A deployer or developer acting under the exception provided by paragraph (1) shall perform an updated impact assessment to verify that the algorithmic discrimination has been mitigated and is not reasonably likely to occur.</p>\n   <p class=\"indent\">22756.6. (a) (1) A developer shall provide to the Attorney General or Civil Rights Department, within 30 days of a request from the Attorney General or the Civil Rights Department, a copy of an impact assessment performed pursuant to this chapter.</p>\n   <p class=\"indent\">(2) Notwithstanding any other law, an impact assessment provided to the Attorney General or Civil Rights Department pursuant to this subdivision shall be kept confidential.</p>\n   <p class=\"indent\">(b) The Attorney General or the Civil Rights Department may bring a civil action against a deployer or developer for a violation of this chapter and obtain any of the following relief:</p>\n   <p class=\"indent\">(1) (A) If a developer or deployer fails to conduct an impact assessment as required under this chapter, a civil penalty of two thousand five hundred dollars ($2,500) for a defendant with fewer than 100 employees, five thousand dollars ($5,000) if the defendant has fewer than 500 employees, and ten thousand dollars ($10,000) if the defendant has at least 500 employees.</p>\n   <p class=\"indent\">(B) If a violation is intentional, the civil penalty pursuant to this paragraph shall increase by five hundred dollars ($500) for each day that the defendant is noncompliant.</p>\n   <p class=\"indent\">(2) Injunctive relief.</p>\n   <p class=\"indent\">(3) Reasonable attorney&#39;s fees and costs.</p>\n   <p class=\"indent\">(4) If the violation concerns algorithmic discrimination, a civil penalty of twenty-five thousand dollars ($25,000) per violation.</p>\n   <p class=\"indent\">(c) (1) Before commencing an action pursuant to this section, the Attorney General or the Civil Rights Department shall provide 45 days&#39; written notice to a deployer or developer of any alleged violation of this chapter.</p>\n   <p class=\"indent\">(2) (A) The developer or deployer may cure, within 45 days of receiving the written notice described in paragraph (1), the noticed violation and provide an express written statement, made under penalty of perjury, that the violation has been cured.</p>\n   <p class=\"indent\">(B) If the developer or deployer cures the noticed violation and provides the express written statement pursuant to subparagraph (A), an action shall not be maintained for the noticed violation.</p>\n   <p class=\"indent\">22756.7. This chapter does not apply to either of the following:</p>\n   <p class=\"indent\"> (a) An entity with 50 or fewer employees.</p>\n   <p class=\"indent\">(b) A high-risk automated decision system that has been approved, certified, or cleared by a federal agency that complies with another law that is substantially the same or more stringent than this chapter.</p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">SEC. 3. Article 11 (commencing with Section 10285.8) is added to Chapter 1 of Part 2 of Division 2 of the Public Contract Code, to read:</u>\n   </p>\n   <p class=\"indent\">Article 11.</p>\n   <p class=\"indent\">High-Risk Automated Decision Systems</p>\n   <p class=\"indent\">10285.8. (a) A state agency shall not award a contract for a high-risk automated decision system to a person who has violated any of the following:</p>\n   <p class=\"indent\">(1) The Unruh Civil Rights Act (Section 51 of the Civil Code).</p>\n   <p class=\"indent\">(2) The California Fair Employment and Housing Act (Chapter 7 (commencing with Section 12960) of Part 2.8 of Division 3 of Title 2 of the Government Code).</p>\n   <p class=\"indent\">(3) Chapter 24.6 (commencing with Section 22756) of Division 8 of the Business and Professions Code.</p>\n   <p class=\"indent\">(b) As used in this section, &quot;high-risk automated decision system&quot; has the same meaning as defined in Section 22756 of the Business and Professions Code.</p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">SEC. 4. The Legislature finds and declares that Section 2 of this act, which adds Chapter 24.6 (commencing with Section 22756) of the Business and Professions Code, imposes a limitation on the public&#39;s right of access to the meetings of public bodies or the writings of public officials and agencies within the meaning of Section 3 of Article I of the California Constitution. Pursuant to that constitutional provision, the Legislature makes the following findings to demonstrate the interest protected by this limitation and the need for protecting that interest:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">To avoid unduly disrupting commerce, it is necessary that trade secrets be protected.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">SEC. 5. No reimbursement is required by this act pursuant to Section 6 of Article XIIIB of the California Constitution because the only costs that may be incurred by a local agency or school district will be incurred because this act creates a new crime or infraction, eliminates a crime or infraction, or changes the penalty for a crime or infraction, within the meaning of Section 17556 of the Government Code, or changes the definition of a crime within the meaning of Section 6 of Article XIIIB of the California Constitution.</u>\n   </p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 CA S 420 | | Author: | Padilla  \n---|---  \nVersion: | Amended  \nVersion Date: | 03/26/2025  \n  \nAMENDED IN SENATE March 26, 2025\n\nCALIFORNIA LEGISLATURE--2025-2026 REGULAR SESSION\n\n**Senate Bill**\n\n**No. 420**\n\n**Introduced by** Senator Padilla\n\nFebruary 18, 2025\n\nAn act _to add Chapter 24.6 (commencing with Section 22756) to Division 8 of\nthe Business and Professions Code, and to add Article 11 (commencing with\nSection 10285.8) to Chapter 1 of Part 2 of Division 2 of the Public Contract\nCode,_ relating to artificial intelligence.\n\nLEGISLATIVE COUNSEL'S DIGEST\n\nSB 420, as amended, Padilla. ~~Individual rights.~~ _Automated decision\nsystems._\n\nThe California AI Transparency Act requires a covered provider, as defined, of\na generative artificial intelligence system to make available an AI detection\ntool at no cost to the user that meets certain criteria, including that the\ntool outputs any system provenance data, as defined, that is detected in the\ncontent. The California Consumer Privacy Act of 2018 grants a consumer various\nrights with respect to personal information that is collected or sold by a\nbusiness, as defined, including the right to direct a business that sells or\nshares personal information about the consumer to third parties not to sell or\nshare the consumer's personal information, as specified.\n\n~~This bill would express the intent of the Legislature to enact legislation\nthat would relate to strengthening, establishing, and promoting certain rights\nand values related to artificial intelligence.~~\n\n_This bill would generally regulate a developer or a deployer of a high-risk\nautomated decision system, as defined, including by requiring a developer or a\ndeployer to perform an impact assessment on the high-risk automated decision\nsystem before making it publicly available or deploying it, as prescribed. The\nbill would require a state agency to require a developer of a high-risk\nautomated decision system deployed by the state agency to provide to the state\nagency a copy of the impact assessment and would require the state agency to\nkeep that impact assessment confidential. The bill would also require a\ndeveloper to provide to the Attorney General or Civil Rights Department,\nwithin 30 days of a request from the Attorney General or the Civil Rights\nDepartment, a copy of an impact assessment and would require the impact\nassessment to be kept confidential._\n\n_This bill would authorize the Attorney General or the Civil Rights Department\nto bring a specified civil action to enforce compliance with the bill, as\nprescribed, and would authorize a developer or deployer to cure, within 45\ndays of receiving a certain notice of a violation, the noticed violation and\nprovide an express written statement, made under penalty of perjury, that the\nviolation has been cured. By expanding the scope of the crime of perjury, this\nbill would impose a state-mandated local program._\n\n_This bill would prohibit a state agency from awarding a contract for a high-\nrisk automated decision system to a person who has violated, among other civil\nrights laws, the bill._\n\n_Existing constitutional provisions require that a statute that limits the\nright of access to the meetings of public bodies or the writings of public\nofficials and agencies be adopted with findings demonstrating the interest\nprotected by the limitation and the need for protecting that interest._\n\n_This bill would make legislative findings to that effect._\n\n_The California Constitution requires the state to reimburse local agencies\nand school districts for certain costs mandated by the state. Statutory\nprovisions establish procedures for making that reimbursement._\n\n_This bill would provide that no reimbursement is required by this act for a\nspecified reason._\n\nVote Required: MAJORITY Appropriation: NO Fiscal Committee: ~~NO~~ _YES_ Local\nProgram: ~~NO~~ _YES_ Immediate Effect NO Urgency: NO Tax Levy: NO Election:\nNO Usual Current Expenses: NO Budget Bill: NO Prop 25 Trailer Bill: NO\n\nThe people of the State of California do enact as follows:\n\nSECTION 1. The Legislature finds and declares all of the following:\n\n(a) (1) Artificial intelligence technologies are becoming an integral part of\ndaily life in California and have profound implications for privacy, equity,\nfairness, and public safety.\n\n(2) It is critical to protect individuals' rights to safeguard against\npotential harms, including discrimination, privacy violations, and unchecked\nautomation in critical decisionmaking processes.\n\n(3) A comprehensive set of rights must be established to ensure artificial\nintelligence technologies align with the public interest and reflect the\nvalues of California residents.\n\n(b) (1) Individuals should have the right to receive a clear and accessible\nexplanation about how artificial intelligence systems operate, including the\ndata they use and the decisions they make.\n\n(2) An entity that uses artificial intelligence systems to make decisions\nimpacting California residents should provide a mechanism to inform\nindividuals of the system's logic, processing methods, and intended outcomes\nin a manner that is understandable.\n\n(c) (1) All individuals have the right to control their personal data in\nrelation to artificial intelligence systems. Artificial intelligence systems\nshould operate with the highest standards of data privacy and security, in\nline with the California Consumer Privacy Act of 2018 and other relevant\nprivacy laws.\n\n(2) Before personal data is used in artificial intelligence systems, entities\nshould obtain informed, explicit consent from individuals, and individuals\nshould have the right to withdraw consent at any time without penalty.\n\n(3) Entities should ensure that personal data used by artificial intelligence\nsystems is anonymized or pseudonymized if feasible, and data retention should\nbe limited to the purposes for which the data was initially collected.\n\n(d) (1) Artificial intelligence systems should not discriminate against\nindividuals based on race, gender, sexual orientation, disability, religion,\nsocioeconomic status, or other protected characteristics under California law.\n\n(2) Entities deploying artificial intelligence technologies should perform\nregular audits to identify and address any biases or inequities in their\nartificial intelligence systems and should ensure that artificial intelligence\nsystems are designed and trained to promote fairness and equal treatment.\n\n(e) (1) Individuals should have the right to hold entities accountable for any\nharm caused by artificial intelligence systems, and entities should be liable\nfor the actions and decisions made by artificial intelligence technologies\nthey deploy.\n\n(2) An individual or group adversely affected by artificial intelligence-\ndriven decisions should have access to a straightforward and transparent\nprocess for seeking redress, including the ability to challenge those\ndecisions through human review and appeal mechanisms.\n\n(f) (1) Individuals should have the right to request human oversight for\nsignificant decisions made by artificial intelligence systems that impact\nthem, particularly in areas such as employment, health care, housing,\neducation, and criminal justice.\n\n(2) Artificial intelligence systems in high-stakes decisionmaking contexts\nshould involve human review or intervention before final decisions, ensuring\nthat automated decisions align with human values and public policy goals.\n\n~~SEC. 2. It is the intent of the Legislature to enact legislation that would\nrelate to strengthening, establishing, and promoting the rights and values\ndescribed in Section 1 of this act.~~\n\n_SEC. 2. Chapter 24.6 (commencing with Section 22756) is added to Division 8\nof the Business and Professions Code, to read:_\n\n24.6. Automated Decision Systems\n\n22756\\. As used in this chapter:\n\n(a) \"Algorithmic discrimination\" means the condition in which an automated\ndecision system contributes to unlawful discrimination on the basis of a\nprotected classification.\n\n(b) \"Artificial intelligence\" means an engineered or machine-based system that\nvaries in its level of autonomy and that can, for explicit or implicit\nobjectives, infer from the input it receives how to generate outputs that can\ninfluence physical or virtual environments.\n\n(c) (1) \"Automated decision system\" means a computational process derived from\nmachine learning, statistical modeling, data analytics, or artificial\nintelligence that issues simplified output, including a score, classification,\nor recommendation, that is used to assist or replace human discretionary\ndecisionmaking and materially impacts natural persons.\n\n(2) \"Automated decision system\" does not mean a spam email filter, firewall,\nantivirus software, identity and access management tool, calculator, database,\ndataset, or other compilation of data.\n\n(d) \"Deployer\" means a natural person or entity that uses a high-risk\nautomated decision system in the state.\n\n(e) \"Detecting decisionmaking patterns without influencing outcomes\" means the\nact of artificial intelligence analyzing patterns for informational purposes\nwithout direct influence on decisions.\n\n(f) \"Developer\" means a natural person or entity that designs, codes,\nproduces, or substantially modifies a high-risk automated decision system for\nuse in the state.\n\n(g) \"Education enrollment or opportunity\" means the chance to obtain\nadmission, accreditation, evaluation, certification, vocational training,\nfinancial aid, or scholarships with respect to an educational opportunity.\n\n(h) \"Employment or employment opportunity\" means hiring, salary, wage, or\nother material term, condition, or privilege of an employee's employment.\n\n(i) \"Health care\" means health care services or insurance for health, mental\nhealth, dental, or vision.\n\n(j) (1) \"High-risk automated decision system\" means an automated decision\nsystem that is used to assist or replace human discretionary decisions that\nhave a legal or similarly significant effect, including decisions that\nmaterially impact access to, or approval for, any of the following:\n\n(A) Education enrollment or opportunity.\n\n(B) Employment or employment opportunity.\n\n(C) Essential utilities.\n\n(D) Temporary, short-term, or long-term housing.\n\n(E) Health care services.\n\n(F) Lending services.\n\n(G) A legal right or service.\n\n(H) An essential government service.\n\n(2) \"High-risk automated decision system\" does not include an automated\ndecision system that only performs narrow procedural tasks, enhances human\nactivities, detects patterns without influencing decisions, or assists in\npreparatory tasks for assessment.\n\n(k) \"Improving results of previously completed human activities\" means the act\nof artificial intelligence enhancing existing human-performed tasks without\naltering decisions.\n\n(l) \"Narrow procedural task\" means a limited, procedural task that has a\nminimal impact on outcomes.\n\n(m) \"Preparatory task for assessment\" means a task in which an artificial\nintelligence aids in a preparatory task for assessment or evaluation without\ndirect decisionmaking authority.\n\n(n) \"Protected classification\" means a classification protected under existing\nlaw prohibiting discrimination, including, but not limited to, the Fair\nEmployment and Housing Act (Chapter 7 (commencing with Section 12960) of Part\n2.8 of Division 3 of Title 2 of the Government Code) or the Unruh Civil Rights\nAct (Section 51 of the Civil Code).\n\n(o) (1) \"State agency\" means any of the following:\n\n(A) A state office, department, division, or bureau.\n\n(B) The California State University.\n\n(C) The Board of Parole Hearings.\n\n(D) A board or other professional licensing and regulatory body under the\nadministration or oversight of the Department of Consumer Affairs.\n\n(2) \"State agency\" does not include the University of California, the\nLegislature, the judicial branch, or a board that is not described in\nparagraph (1).\n\n(p) \"Substantial modification\" means a new version, release, or other\nsignificant update that materially changes the functionality or performance of\na high-risk automated decision system, including the results of retraining.\n\n22756.1. (a) (1) For a high-risk automated decision system made publicly\navailable for use on or after January 1, 2026, a developer shall perform an\nimpact assessment on the high-risk automated decision system before making the\nhigh-risk automated decision system publicly available for use.\n\n(2) For a high-risk automated decision system first made publicly available\nfor use before January 1, 2026, a developer shall perform an impact assessment\nif the developer makes a substantial modification to the high-risk automated\ndecision system.\n\n(b) (1) Except as provided in paragraph (2), for a high-risk automated\ndecision system first deployed after January 1, 2026, a deployer shall perform\nan impact assessment within two years of deploying the high-risk automated\ndecision system.\n\n(2) A state agency that is a deployer may opt out of performing an impact\nassessment if the state agency uses the automated decision system only for its\nintended use as determined by the developer and all of the following\nrequirements are met:\n\n(A) The state agency does not make a substantial modification to the high-risk\nautomated decision system.\n\n(B) The developer of the high-risk automated decision system is in compliance\nwith Section 10285.8 of the Public Contract Code and subdivision (d).\n\n(C) The state agency does not have a reasonable basis to believe that\ndeployment of the high-risk automated decision system as intended by the\ndeveloper is likely to result in algorithmic discrimination.\n\n(D) The state agency is in compliance with Section 22756.3.\n\n(c) (1) A developer shall make available to deployers and potential deployers\nthe statements included in the developer's impact assessment pursuant to\nparagraph (2).\n\n(2) An impact assessment prepared pursuant to this section shall include all\nof the following:\n\n(A) A statement of the purpose of the high-risk automated decision system and\nits intended benefits, intended uses, and intended deployment contexts.\n\n(B) A description of the high-risk automated decision system's intended\noutputs.\n\n(C) A summary of the types of data intended to be used as inputs to the high-\nrisk automated decision system and any processing of those data inputs\nrecommended to ensure the intended functioning of the high-risk automated\ndecision system.\n\n(D) A summary of reasonably foreseeable potential disproportionate or\nunjustified impacts on a protected classification from the intended use by\ndeployers of the high-risk automated decision system.\n\n(E) A developer's impact assessment shall also include both of the following:\n\n(i) A description of safeguards implemented or other measures taken by the\ndeveloper to mitigate and guard against risks known to the developer of\nalgorithmic discrimination arising from the use of the high-risk automated\ndecision system.\n\n(ii) A description of how the high-risk automated decision system can be\nmonitored by a deployer for risks of algorithmic discrimination known to the\ndeveloper.\n\n(F) A statement of the extent to which the deployer's use of the high-risk\nautomated decision system is consistent with, or varies from, the developer's\nstatement of the high-risk automated decision system's purpose and intended\nbenefits, intended uses, and intended deployment contexts.\n\n(G) A description of safeguards implemented or other measures taken to\nmitigate and guard against any known risks to the deployer of discrimination\narising from the high-risk automated decision system.\n\n(H) A description of how the high-risk automated decision system has been, and\nwill be, monitored and evaluated.\n\n(d) (1) A state agency shall require a developer of a high-risk automated\ndecision system deployed by the state agency to provide to the state agency a\ncopy of the impact assessment conducted pursuant to this section.\n\n(2) Notwithstanding any other law, an impact assessment provided to a state\nagency pursuant to this subdivision shall be kept confidential.\n\n22756.2. (a) If a deployer uses a high-risk automated decision system to make\na decision regarding a natural person, the deployer shall notify the natural\nperson of that fact and disclose to that natural person all of the following:\n\n(1) The purpose of the high-risk automated decision system and the specific\ndecision it was used to make.\n\n(2) How the high-risk automated decision system was used to make the decision.\n\n(3) The type of data used by the high-risk automated decision system.\n\n(4) Contact information for the deployer.\n\n(5) A link to the statement required by subdivision (b).\n\n(b) A deployer shall make available on its internet website a statement\nsummarizing all of the following:\n\n(1) The types of high-risk automated decision systems it currently deploys.\n\n(2) How the deployer manages known or reasonably foreseeable risks of\nalgorithmic discrimination arising from the deployment of those high-risk\nautomated decision systems.\n\n(3) The nature and source of the information collected and used by the high-\nrisk automated decision systems deployed by the deployer.\n\n(c) A deployer shall provide, as technically feasible, a natural person that\nis the subject of a decision made by a high-risk automated decision system an\nopportunity to appeal that decision for review by a natural person.\n\n22756.3. (a) A developer or a deployer shall establish, document, implement,\nand maintain a governance program that contains reasonable administrative and\ntechnical safeguards to govern the reasonably foreseeable risks of algorithmic\ndiscrimination associated with the use, or intended use, of a high-risk\nautomated decision system.\n\n(b) The governance program required by this subdivision shall be appropriately\ndesigned with respect to all of the following:\n\n(1) The use, or intended use, of the high-risk automated decision system.\n\n(2) The size, complexity, and resources of the deployer or developer.\n\n(3) The nature, context, and scope of the activities of the deployer or\ndeveloper in connection with the high-risk automated decision system.\n\n(4) The technical feasibility and cost of available tools, assessments, and\nother means used by a deployer or developer to map, measure, manage, and\ngovern the risks associated with a high-risk automated decision system.\n\n22756.4. A developer or deployer is not required to disclose information under\nthis chapter if the disclosure of that information would result in the waiver\nof a legal privilege or the disclosure of a trade secret, as defined in\nSection 3426.1 of the Civil Code.\n\n22756.5. (a) Except as provided in subdivision (b), a deployer or developer\nshall not deploy or make available for deployment a high-risk automated\ndecision system if the impact assessment performed pursuant to this chapter\ndetermines that the high-risk automated decision system is likely to result in\nalgorithmic discrimination.\n\n(b) (1) A deployer or developer may deploy or make available for deployment a\nhigh-risk automated decision system if the impact assessment performed\npursuant to this chapter determines that the high-risk automated decision\nsystem will result in algorithmic discrimination if the deployer or developer\nimplements safeguards to mitigate the known risks of algorithmic\ndiscrimination.\n\n(2) A deployer or developer acting under the exception provided by paragraph\n(1) shall perform an updated impact assessment to verify that the algorithmic\ndiscrimination has been mitigated and is not reasonably likely to occur.\n\n22756.6. (a) (1) A developer shall provide to the Attorney General or Civil\nRights Department, within 30 days of a request from the Attorney General or\nthe Civil Rights Department, a copy of an impact assessment performed pursuant\nto this chapter.\n\n(2) Notwithstanding any other law, an impact assessment provided to the\nAttorney General or Civil Rights Department pursuant to this subdivision shall\nbe kept confidential.\n\n(b) The Attorney General or the Civil Rights Department may bring a civil\naction against a deployer or developer for a violation of this chapter and\nobtain any of the following relief:\n\n(1) (A) If a developer or deployer fails to conduct an impact assessment as\nrequired under this chapter, a civil penalty of two thousand five hundred\ndollars ($2,500) for a defendant with fewer than 100 employees, five thousand\ndollars ($5,000) if the defendant has fewer than 500 employees, and ten\nthousand dollars ($10,000) if the defendant has at least 500 employees.\n\n(B) If a violation is intentional, the civil penalty pursuant to this\nparagraph shall increase by five hundred dollars ($500) for each day that the\ndefendant is noncompliant.\n\n(2) Injunctive relief.\n\n(3) Reasonable attorney's fees and costs.\n\n(4) If the violation concerns algorithmic discrimination, a civil penalty of\ntwenty-five thousand dollars ($25,000) per violation.\n\n(c) (1) Before commencing an action pursuant to this section, the Attorney\nGeneral or the Civil Rights Department shall provide 45 days' written notice\nto a deployer or developer of any alleged violation of this chapter.\n\n(2) (A) The developer or deployer may cure, within 45 days of receiving the\nwritten notice described in paragraph (1), the noticed violation and provide\nan express written statement, made under penalty of perjury, that the\nviolation has been cured.\n\n(B) If the developer or deployer cures the noticed violation and provides the\nexpress written statement pursuant to subparagraph (A), an action shall not be\nmaintained for the noticed violation.\n\n22756.7. This chapter does not apply to either of the following:\n\n(a) An entity with 50 or fewer employees.\n\n(b) A high-risk automated decision system that has been approved, certified,\nor cleared by a federal agency that complies with another law that is\nsubstantially the same or more stringent than this chapter.\n\n_SEC. 3. Article 11 (commencing with Section 10285.8) is added to Chapter 1 of\nPart 2 of Division 2 of the Public Contract Code, to read:_\n\nArticle 11.\n\nHigh-Risk Automated Decision Systems\n\n10285.8. (a) A state agency shall not award a contract for a high-risk\nautomated decision system to a person who has violated any of the following:\n\n(1) The Unruh Civil Rights Act (Section 51 of the Civil Code).\n\n(2) The California Fair Employment and Housing Act (Chapter 7 (commencing with\nSection 12960) of Part 2.8 of Division 3 of Title 2 of the Government Code).\n\n(3) Chapter 24.6 (commencing with Section 22756) of Division 8 of the Business\nand Professions Code.\n\n(b) As used in this section, \"high-risk automated decision system\" has the\nsame meaning as defined in Section 22756 of the Business and Professions Code.\n\n_SEC. 4. The Legislature finds and declares that Section 2 of this act, which\nadds Chapter 24.6 (commencing with Section 22756) of the Business and\nProfessions Code, imposes a limitation on the public 's right of access to the\nmeetings of public bodies or the writings of public officials and agencies\nwithin the meaning of Section 3 of Article I of the California Constitution.\nPursuant to that constitutional provision, the Legislature makes the following\nfindings to demonstrate the interest protected by this limitation and the need\nfor protecting that interest:_\n\n_To avoid unduly disrupting commerce, it is necessary that trade secrets be\nprotected._\n\n_SEC. 5. No reimbursement is required by this act pursuant to Section 6 of\nArticle XIIIB of the California Constitution because the only costs that may\nbe incurred by a local agency or school district will be incurred because this\nact creates a new crime or infraction, eliminates a crime or infraction, or\nchanges the penalty for a crime or infraction, within the meaning of Section\n17556 of the Government Code, or changes the definition of a crime within the\nmeaning of Section 6 of Article XIIIB of the California Constitution._\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "05/23/2025",
      "label": "Amended",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:CA2025000S420&verid=CA2025000S420_20250523_0_A&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 CA S 420</td> <td><table><tr><td class=\"label\">Author:</td> <td>Padilla</td></tr> <tr><td class=\"label\">Version:</td> <td>Amended</td></tr> <tr><td class=\"label\">Version Date:</td> <td>05/23/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">Amended IN  Senate  May 23, 2025</p>\n   <p class=\"center\"> Amended IN  Senate  March 26, 2025</p>\n   <p class=\"center\">CALIFORNIA LEGISLATURE--2025-2026 REGULAR SESSION</p>\n   <p class=\"left\">\n    <b> Senate Bill</b>\n   </p>\n   <p class=\"right\">\n    <b>No. 420</b>\n   </p>\n   <p class=\"center\">\n    <b>Introduced by Senator Padilla</b>\n   </p>\n   <p class=\"center\">February 18, 2025</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"center\">\n    <b>An act to add Chapter 24.6 (commencing with Section 22756) to Division 8 of the Business and Professions Code, and to add Article 11 (commencing with Section 10285.8) to Chapter 1 of Part 2 of Division 2 of the Public Contract Code, relating to artificial intelligence.</b>\n   </p>\n  </div>\n  <a name=\"digest_document_section\"></a><div class=\"digest\">\n   <p class=\"center\">LEGISLATIVE COUNSEL&#39;S DIGEST</p>\n   <p class=\"indent\">SB 420, as amended, Padilla. Automated decision systems.</p>\n   <p class=\"indent\">The California AI Transparency Act requires a covered provider, as defined, of a generative artificial intelligence system to make available an AI detection tool at no cost to the user that meets certain criteria, including that the tool outputs any system provenance data, as defined, that is detected in the content. The California Consumer Privacy Act of 2018 grants a consumer various rights with respect to personal information that is collected or sold by a business, as defined, including the right to direct a business that sells or shares personal information about the consumer to third parties not to sell or share the consumer&#39;s personal information, as specified.</p>\n   <p class=\"indent\">This bill would generally regulate a developer or a deployer of a high-risk automated decision system, as defined, including by requiring a developer or a deployer to perform an impact assessment on the high-risk automated decision system before making it publicly available or deploying it, as prescribed. The bill would require a state agency to require a developer of a high-risk automated decision system deployed by the state agency to provide to the state agency a copy of the impact assessment and would require the state agency to keep that impact assessment confidential. The bill would also require a developer to provide to the Attorney General or Civil Rights Department, within 30 days of a request from the Attorney General or the Civil Rights Department, a copy of an impact assessment and would require the impact assessment to be kept confidential.</p>\n   <p class=\"indent\">This bill would authorize the Attorney General or the Civil Rights Department to bring a specified civil action to enforce compliance with the bill, as prescribed, and would authorize<u class=\"amendmentInsertedText\"> the Attorney General or the Civil Rights Department to allow</u> a developer or deployer to cure, within 45 days of receiving a certain notice of a violation, the noticed<strike class=\"amendmentDeletedText\"> violation and provide an express written statement, made under penalty of perjury, that the violation has been cured. By expanding the scope of the crime of perjury, this bill would impose a state-mandated local program.</strike>\n    <u class=\"amendmentInsertedText\"> violation, as prescribed.</u>\n   </p>\n   <p class=\"indent\">This bill would prohibit a state agency from awarding a contract for a high-risk automated decision system<strike class=\"amendmentDeletedText\"> to a person who has violated,</strike>\n    <u class=\"amendmentInsertedText\"> unless the person to whom the contract is awarded has certified that the high-risk automated decision system does not violate,</u> among other civil rights laws, the bill.<u class=\"amendmentInsertedText\"> By expanding the scope of the crime of perjury, this bill would impose a state-mandated local program.</u>\n   </p>\n   <p class=\"indent\">Existing constitutional provisions require that a statute that limits the right of access to the meetings of public bodies or the writings of public officials and agencies be adopted with findings demonstrating the interest protected by the limitation and the need for protecting that interest.</p>\n   <p class=\"indent\">This bill would make legislative findings to that effect.</p>\n   <p class=\"indent\">The California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.</p>\n   <p class=\"indent\">This bill would provide that no reimbursement is required by this act for a specified reason.</p>\n   <p class=\"indent\">Vote: MAJORITY  Appropriation: NO  Fiscal Committee: YES  Local Program: YES</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">The people of the State of California do enact as follows:</p>\n   </span>\n   <p class=\"indent\">SECTION 1. The Legislature finds and declares all of the following:</p>\n   <p class=\"indent\">(a) (1) Artificial intelligence technologies are becoming an integral part of daily life in California and have profound implications for privacy, equity, fairness, and public safety.</p>\n   <p class=\"indent\">(2) It is critical to protect individuals&#39; rights to safeguard against potential harms, including discrimination, privacy violations, and unchecked automation in critical decisionmaking processes.</p>\n   <p class=\"indent\">(3) A comprehensive set of rights must be established to ensure artificial intelligence technologies align with the public interest and reflect the values of California residents.</p>\n   <p class=\"indent\">(b) (1) Individuals should have the right to receive a clear and accessible explanation about how artificial intelligence systems operate, including the data they use and the decisions they make.</p>\n   <p class=\"indent\">(2) An entity that uses artificial intelligence systems to make decisions impacting California residents should provide a mechanism to inform individuals of the system&#39;s logic, processing methods, and intended outcomes in a manner that is understandable.</p>\n   <p class=\"indent\">(c) (1) All individuals have the right to control their personal data in relation to artificial intelligence systems. Artificial intelligence systems should operate with the highest standards of data privacy and security, in line with the California Consumer Privacy Act of 2018 and other relevant privacy laws.</p>\n   <p class=\"indent\">(2) Before personal data is used in artificial intelligence systems, entities should obtain informed, explicit consent from individuals, and individuals should have the right to withdraw consent at any time without penalty.</p>\n   <p class=\"indent\">(3) Entities should ensure that personal data used by artificial intelligence systems is anonymized or pseudonymized if feasible, and data retention should be limited to the purposes for which the data was initially collected.</p>\n   <p class=\"indent\">(d) (1) Artificial intelligence systems should not discriminate against individuals based on race, gender, sexual orientation, disability, religion, socioeconomic status, or other protected characteristics under California law.</p>\n   <p class=\"indent\">(2) Entities deploying artificial intelligence technologies should perform regular audits to identify and address any biases or inequities in their artificial intelligence systems and should ensure that artificial intelligence systems are designed and trained to promote fairness and equal treatment.</p>\n   <p class=\"indent\">(e) (1) Individuals should have the right to hold entities accountable for any harm caused by artificial intelligence systems, and entities should be liable for the actions and decisions made by artificial intelligence technologies they deploy.</p>\n   <p class=\"indent\">(2) An individual or group adversely affected by artificial intelligence-driven decisions should have access to a straightforward and transparent process for seeking redress, including the ability to challenge those decisions through human review and appeal mechanisms.</p>\n   <p class=\"indent\">(f) (1) Individuals should have the right to request human oversight for significant decisions made by artificial intelligence systems that impact them, particularly in areas such as employment, health care, housing, education, and criminal justice.</p>\n   <p class=\"indent\">(2) Artificial intelligence systems in high-stakes decisionmaking contexts should involve human review or intervention before final decisions, ensuring that automated decisions align with human values and public policy goals.</p>\n   <p class=\"indent\">SEC. 2. Chapter 24.6 (commencing with Section 22756) is added to Division 8 of the Business and Professions Code, to read:</p>\n   <p class=\"indent\"> CHAPTER 24.6. Automated Decision Systems</p>\n   <p class=\"indent\">22756. As used in this chapter:</p>\n   <p class=\"indent\">(a) &quot;Algorithmic discrimination&quot; means the condition in which an automated decision system contributes to unlawful discrimination on the basis of a protected classification.</p>\n   <p class=\"indent\">(b) &quot;Artificial intelligence&quot; means an engineered or machine-based system that varies in its level of autonomy and that can, for explicit or implicit objectives, infer from the input it receives how to generate outputs that can influence physical or virtual environments.</p>\n   <p class=\"indent\">(c) (1) &quot;Automated decision system&quot; means a computational process derived from machine learning, statistical modeling, data analytics, or artificial intelligence that issues simplified output, including a score, classification, or recommendation, that is used to assist or replace human discretionary decisionmaking and materially impacts natural persons.</p>\n   <p class=\"indent\">(2) &quot;Automated decision system&quot; does not mean a spam email filter, firewall, antivirus software, identity and access management tool, calculator, database, dataset, or other compilation of data.</p>\n   <p class=\"indent\">(d) &quot;Deployer&quot; means a natural person or entity that uses a high-risk automated decision system in the state.</p>\n   <p class=\"indent\">(e) &quot;Detecting decisionmaking patterns without influencing outcomes&quot; means the act of artificial intelligence analyzing patterns for informational purposes without direct influence on decisions.</p>\n   <p class=\"indent\">(f) &quot;Developer&quot; means a natural person or entity that designs, codes, produces, or<strike class=\"amendmentDeletedText\"> substantially modifies</strike>\n    <u class=\"amendmentInsertedText\"> makes a substantial modification to</u> a high-risk automated decision system for use in the state.</p>\n   <p class=\"indent\">(g) &quot;Education enrollment or opportunity&quot; means the chance to obtain admission, accreditation, evaluation, certification, vocational training, financial aid, or scholarships with respect to an educational opportunity.</p>\n   <p class=\"indent\">(h) &quot;Employment or employment opportunity&quot; means hiring, salary, wage, or other material term, condition, or privilege of an employee&#39;s employment.</p>\n   <p class=\"indent\">(i) &quot;Health care&quot; means health care services or insurance for health, mental health, dental, or vision.</p>\n   <p class=\"indent\">(j) (1) &quot;High-risk automated decision system&quot; means an automated decision system that is used to assist or replace human discretionary decisions that have a legal or similarly significant effect, including decisions that materially impact access to, or approval for, any of the following:</p>\n   <p class=\"indent\">(A) Education enrollment or opportunity.</p>\n   <p class=\"indent\">(B) Employment or employment opportunity.</p>\n   <p class=\"indent\">(C) Essential utilities.</p>\n   <p class=\"indent\">(D) Temporary, short-term, or long-term housing.</p>\n   <p class=\"indent\">(E) Health care services.</p>\n   <p class=\"indent\">(F) Lending services.</p>\n   <p class=\"indent\">(G) A legal right or service.</p>\n   <p class=\"indent\">(H) An essential government service.</p>\n   <p class=\"indent\">(2) &quot;High-risk automated decision system&quot; does not include an automated decision system that only performs narrow procedural tasks, enhances human activities, detects patterns without influencing decisions, or assists in preparatory tasks for assessment.</p>\n   <p class=\"indent\">(k) &quot;Improving results of previously completed human activities&quot; means the act of artificial intelligence enhancing existing human-performed tasks without altering decisions.</p>\n   <p class=\"indent\">(l) &quot;Narrow procedural task&quot; means a limited, procedural task that has a minimal impact on outcomes.</p>\n   <p class=\"indent\">(m) &quot;Preparatory task for assessment&quot; means a task in which an artificial intelligence aids in a preparatory task for assessment or evaluation without direct decisionmaking authority.</p>\n   <p class=\"indent\">(n) &quot;Protected classification&quot; means a classification protected under existing law prohibiting discrimination, including, but not limited to, the<u class=\"amendmentInsertedText\"> California</u> Fair Employment and Housing Act (Chapter 7 (commencing with Section 12960) of Part 2.8 of Division 3 of Title 2 of the Government Code) or the Unruh Civil Rights Act (Section 51 of the Civil Code).</p>\n   <p class=\"indent\">(o) (1) &quot;State agency&quot; means any of the following:</p>\n   <p class=\"indent\">(A) A state office, department, division, or bureau.</p>\n   <p class=\"indent\">(B) The California State University.</p>\n   <p class=\"indent\">(C) The Board of Parole Hearings.</p>\n   <p class=\"indent\">(D) A board or other professional licensing and regulatory body under the administration or oversight of the Department of Consumer Affairs.</p>\n   <p class=\"indent\">(2) &quot;State agency&quot; does not include the University of California, the Legislature, the judicial branch, or a board that is not described in paragraph (1).</p>\n   <p class=\"indent\">(p) &quot;Substantial modification&quot; means a new version, release, or other significant update that materially changes the functionality or performance of a high-risk automated decision system, including the results of retraining.</p>\n   <p class=\"indent\">22756.1. (a) (1) For a high-risk automated decision system made publicly available for use on or after January 1, 2026, a developer shall perform an impact assessment on the high-risk automated decision system before making the high-risk automated decision system publicly available for use.</p>\n   <p class=\"indent\">(2) For a high-risk automated decision system first made publicly available for use before January 1, 2026, a developer shall perform an impact assessment<strike class=\"amendmentDeletedText\"> if the developer makes a substantial modification to the high-risk automated decision system.</strike>\n    <u class=\"amendmentInsertedText\"> on or before January 1, 2028.</u>\n   </p>\n   <p class=\"indent\">(b) (1) Except as provided in paragraph (2), for a high-risk automated decision system first deployed after January 1, 2026, a deployer shall perform an impact assessment within two years of deploying the high-risk automated decision system.</p>\n   <p class=\"indent\">(2) A state agency that is a deployer may opt out of performing an impact assessment if the state agency uses the automated decision system only for its intended use as determined by the developer and all of the following requirements are met:</p>\n   <p class=\"indent\">(A) The state agency does not make a substantial modification to the high-risk automated decision system.</p>\n   <p class=\"indent\">(B) The developer of the high-risk automated decision system is in compliance with Section 10285.8 of the Public Contract Code and subdivision (d).</p>\n   <p class=\"indent\">(C) The state agency does not have a reasonable basis to believe that deployment of the high-risk automated decision system as intended by the developer is likely to result in algorithmic discrimination.</p>\n   <p class=\"indent\">(D) The state agency is in compliance with Section<strike class=\"amendmentDeletedText\"> 22756.3.</strike>\n    <u class=\"amendmentInsertedText\"> 22756.4.</u>\n   </p>\n   <p class=\"indent\">(c) <strike class=\"amendmentDeletedText\">(1) </strike>A developer shall make available to deployers and potential deployers the statements included in the developer&#39;s impact assessment pursuant to<strike class=\"amendmentDeletedText\"> paragraph (2).</strike>\n    <u class=\"amendmentInsertedText\"> Section 22756.2.</u>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(2) An impact assessment prepared pursuant to this section shall include all of the following:</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(A) A statement of the purpose of the high-risk automated decision system and its intended benefits, intended uses, and intended deployment contexts.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(B) A description of the high-risk automated decision system&#39;s intended outputs.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(C) A summary of the types of data intended to be used as inputs to the high-risk automated decision system and any processing of those data inputs recommended to ensure the intended functioning of the high-risk automated decision system.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(D) A summary of reasonably foreseeable potential disproportionate or unjustified impacts on a protected classification from the intended use by deployers of the high-risk automated decision system.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(E) A developer&#39;s impact assessment shall also include both of the following:</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(i) A description of safeguards implemented or other measures taken by the developer to mitigate and guard against risks known to the developer of algorithmic discrimination arising from the use of the high-risk automated decision system.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(ii) A description of how the high-risk automated decision system can be monitored by a deployer for risks of algorithmic discrimination known to the developer.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(F) A statement of the extent to which the deployer&#39;s use of the high-risk automated decision system is consistent with, or varies from, the developer&#39;s statement of the high-risk automated decision system&#39;s purpose and intended benefits, intended uses, and intended deployment contexts.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(G) A description of safeguards implemented or other measures taken to mitigate and guard against any known risks to the deployer of discrimination arising from the high-risk automated decision system.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(H) A description of how the high-risk automated decision system has been, and will be, monitored and evaluated.</strike>\n   </p>\n   <p class=\"indent\">(d) (1) A state agency shall require a developer of a high-risk automated decision system deployed by the state agency to provide to the state agency a copy of the impact assessment conducted pursuant to this section.</p>\n   <p class=\"indent\">(2) Notwithstanding any other law, an impact assessment provided to a state agency pursuant to this subdivision shall be kept confidential.</p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">22756.2. An impact assessment prepared pursuant to this chapter shall include all of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) A statement of the purpose of the high-risk automated decision system and its intended benefits, intended uses, and intended deployment contexts.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) A description of the high-risk automated decision system&#39;s intended outputs.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) A summary of the types of data intended to be used as inputs to the high-risk automated decision system and any processing of those data inputs recommended to ensure the intended functioning of the high-risk automated decision system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) A summary of reasonably foreseeable potential disproportionate or unjustified impacts on a protected classification from the intended use by deployers of the high-risk automated decision system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(e) A developer&#39;s impact assessment shall also include both of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(1) A description of safeguards implemented or other measures taken to mitigate and guard against risks known to the developer of algorithmic discrimination arising from the use of the high-risk automated decision system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(2) A description of how the high-risk automated decision system can be monitored by a deployer for risks of algorithmic discrimination known to the developer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(f) A statement of the extent to which the deployer&#39;s use of the high-risk automated decision system is consistent with, or varies from, the developer&#39;s statement of the high-risk automated decision system&#39;s purpose and intended benefits, intended uses, and intended deployment contexts.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(g) A description of safeguards implemented or other measures taken to mitigate and guard against any known risks of discrimination arising from the high-risk automated decision system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(h) A description of how the high-risk automated decision system has been, and will be, monitored and evaluated.</u>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">22756.2.</strike>\n    <u class=\"amendmentInsertedText\">22756.3.</u> (a) If a deployer uses a high-risk automated decision system to make a decision regarding a natural person, the deployer shall notify the natural person of that fact and disclose to that natural person all of the following:</p>\n   <p class=\"indent\">(1) The purpose of the high-risk automated decision system and the specific decision it was used to make.</p>\n   <p class=\"indent\">(2) How the high-risk automated decision system was used to make the decision.</p>\n   <p class=\"indent\">(3) The type of data used by the high-risk automated decision system.</p>\n   <p class=\"indent\">(4) Contact information for the deployer.</p>\n   <p class=\"indent\">(5) A link to the statement required by subdivision (b).</p>\n   <p class=\"indent\">(b) A deployer shall make available on its internet website a statement summarizing all of the following:</p>\n   <p class=\"indent\">(1) The types of high-risk automated decision systems it currently deploys.</p>\n   <p class=\"indent\">(2) How the deployer manages known or reasonably foreseeable risks of algorithmic discrimination arising from the deployment of those high-risk automated decision systems.</p>\n   <p class=\"indent\">(3) The nature and source of the information collected and used by the high-risk automated decision systems deployed by the deployer.</p>\n   <p class=\"indent\">(c) A deployer shall provide, as technically feasible, a natural person that is the subject of a decision made by a high-risk automated decision system an opportunity to appeal that decision for review by a natural person.</p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">22756.3.</strike>\n    <u class=\"amendmentInsertedText\">22756.4.</u> (a) A developer or a deployer shall establish, document, implement, and maintain a governance program that<strike class=\"amendmentDeletedText\"> contains reasonable administrative and technical safeguards to govern the reasonably foreseeable risks of algorithmic discrimination associated with the use, or intended use, of a high-risk automated decision system.</strike>\n    <u class=\"amendmentInsertedText\"> does all of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(1) Contains reasonable administrative and technical safeguards to govern the reasonably foreseeable risks of algorithmic discrimination associated with the use, or intended use, of a high-risk automated decision system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(2) Aligns with existing standards and frameworks, including the National Institute of Standards and Technology AI Risk Management Framework.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(3) Specifies and incorporates the principles, processes, and personnel used to identify, document, and mitigate foreseeable risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(4) Is regularly reviewed and updated.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(5) Includes a structural framework for documenting, investigating, and resolving incidents.</u>\n   </p>\n   <p class=\"indent\">(b) The governance program required by this<strike class=\"amendmentDeletedText\"> subdivision</strike>\n    <u class=\"amendmentInsertedText\"> section</u> shall be appropriately designed with respect to all of the following:</p>\n   <p class=\"indent\">(1) The use, or intended use, of the high-risk automated decision system.</p>\n   <p class=\"indent\">(2) The size, complexity, and resources of the deployer or developer.</p>\n   <p class=\"indent\">(3) The nature, context, and scope of the activities of the deployer or developer in connection with the high-risk automated decision system.</p>\n   <p class=\"indent\">(4) The technical feasibility and cost of available tools, assessments, and other means used by a deployer or developer to map, measure, manage, and govern the risks associated with a high-risk automated decision system.</p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">22756.4.</strike>\n    <u class=\"amendmentInsertedText\">22756.5. (a) </u>A developer or deployer is not required to disclose information under this chapter if the disclosure of that information would result in the waiver of a legal privilege or the disclosure of a trade secret, as defined in Section 3426.1 of the Civil Code.</p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) If a disclosure is not made because the disclosure would reveal a trade secret, the developer or deployer shall notify the party to whom the disclosure would have otherwise been made of the basis for which the disclosure was not made.</u>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">22756.5.</strike>\n    <u class=\"amendmentInsertedText\">22756.6.</u> (a) Except as provided in subdivision (b), a deployer or developer shall not deploy or make available for deployment a high-risk automated decision system if the impact assessment performed pursuant to this chapter determines that the high-risk automated decision system is likely to result in algorithmic discrimination.</p>\n   <p class=\"indent\">(b) (1) A deployer or developer may deploy or make available for deployment a high-risk automated decision system if the impact assessment performed pursuant to this chapter determines that the high-risk automated decision system will result in algorithmic discrimination if the deployer or developer implements safeguards to mitigate the known risks of algorithmic discrimination.</p>\n   <p class=\"indent\">(2) A deployer or developer acting under the exception provided by paragraph (1) shall perform an updated impact assessment to verify that the algorithmic discrimination has been mitigated and is not reasonably likely to occur.</p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">22756.6.</strike>\n    <u class=\"amendmentInsertedText\">22756.7.</u> (a) (1) A developer shall provide to the Attorney General or Civil Rights Department, within 30 days of a request from the Attorney General or the Civil Rights Department, a copy of an impact assessment performed pursuant to this chapter.</p>\n   <p class=\"indent\">(2) Notwithstanding any other law, an impact assessment provided to the Attorney General or Civil Rights Department pursuant to this subdivision shall be kept confidential.</p>\n   <p class=\"indent\">(b) The Attorney General or the Civil Rights Department may bring a civil action against a deployer or developer for a violation of this chapter and obtain any of the following relief:</p>\n   <p class=\"indent\">(1) (A) If a developer or deployer fails to conduct an impact assessment as required under this chapter, a civil penalty of two thousand five hundred dollars ($2,500) for a defendant with fewer than 100 employees, five thousand dollars ($5,000) if the defendant has fewer than 500 employees, and ten thousand dollars ($10,000) if the defendant has at least 500 employees.</p>\n   <p class=\"indent\">(B) If a violation is intentional, the civil penalty pursuant to this paragraph shall increase by five hundred dollars ($500) for each day that the defendant is noncompliant.</p>\n   <p class=\"indent\">(2) Injunctive relief.</p>\n   <p class=\"indent\">(3) Reasonable attorney&#39;s fees and costs.</p>\n   <p class=\"indent\">(4) If the violation concerns algorithmic discrimination, a civil penalty of twenty-five thousand dollars ($25,000) per violation.</p>\n   <p class=\"indent\">(c) (1) Before commencing an action pursuant to this section, the Attorney General or the Civil Rights Department shall provide 45 days&#39; written notice to a deployer or developer of any alleged violation of this chapter.</p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(2) (A) The developer or deployer may cure, within 45 days of receiving the written notice described in paragraph (1), the noticed violation and provide an express written statement, made under penalty of perjury, that the violation has been cured.</strike>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">(B) If the developer or deployer cures the noticed violation and provides the express written statement pursuant to subparagraph (A), an action shall not be maintained for the noticed violation.</strike>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(2) The Attorney General or the Civil Rights Department may, at its discretion, provide to a developer or a deployer with a time period to cure the alleged violation after considering all of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(A) A lack of intent to commit the violation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(B) Voluntary efforts undertaken to cure the alleged violation before being notified of the violation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(C) The size and economic resources of the noncompliant developer or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(D) The size and scope of the impact of the decisions made by an automated decision system related to the violation.</u>\n   </p>\n   <p class=\"indent\">\n    <strike class=\"amendmentDeletedText\">22756.7.</strike>\n    <u class=\"amendmentInsertedText\">22756.8.</u> This chapter does not apply to either of the following:</p>\n   <p class=\"indent\">(a) An entity with 50 or fewer employees.</p>\n   <p class=\"indent\">(b) A high-risk automated decision system that has been approved, certified, or cleared by a federal agency that complies with another law that is substantially the same or more stringent than this chapter.</p>\n   <p class=\"indent\">SEC. 3. Article 11 (commencing with Section 10285.8) is added to Chapter 1 of Part 2 of Division 2 of the Public Contract Code, to read:</p>\n   <p class=\"indent\"> Article 11. High-Risk Automated Decision Systems</p>\n   <p class=\"indent\">10285.8. (a) A state agency shall not award a contract for a high-risk automated decision system<strike class=\"amendmentDeletedText\"> to a person who has violated</strike>\n    <u class=\"amendmentInsertedText\"> unless the person to whom the contract is awarded has certified that the high-risk automated decision system does not violate</u> any of the following:</p>\n   <p class=\"indent\">(1) The Unruh Civil Rights Act (Section 51 of the Civil Code).</p>\n   <p class=\"indent\">(2) The California Fair Employment and Housing Act (Chapter 7 (commencing with Section 12960) of Part 2.8 of Division 3 of Title 2 of the Government Code).</p>\n   <p class=\"indent\">(3) Chapter 24.6 (commencing with Section 22756) of Division 8 of the Business and Professions Code.</p>\n   <p class=\"indent\">(b) As used in this section, &quot;high-risk automated decision system&quot; has the same meaning as defined in Section 22756 of the Business and Professions Code.</p>\n   <p class=\"indent\">SEC. 4. The Legislature finds and declares that Section 2 of this act, which adds Chapter 24.6 (commencing with Section 22756)<u class=\"amendmentInsertedText\"> to Division 8</u> of the Business and Professions Code, imposes a limitation on the public&#39;s right of access to the meetings of public bodies or the writings of public officials and agencies within the meaning of Section 3 of Article I of the California Constitution. Pursuant to that constitutional provision, the Legislature makes the following findings to demonstrate the interest protected by this limitation and the need for protecting that interest:</p>\n   <p class=\"indent\">To avoid unduly disrupting commerce, it is necessary that trade secrets be protected.</p>\n   <p class=\"indent\">SEC. 5. No reimbursement is required by this act pursuant to Section 6 of Article XIIIB of the California Constitution because the only costs that may be incurred by a local agency or school district will be incurred because this act creates a new crime or infraction, eliminates a crime or infraction, or changes the penalty for a crime or infraction, within the meaning of Section 17556 of the Government Code, or changes the definition of a crime within the meaning of Section 6 of Article XIIIB of the California Constitution.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 CA S 420 | | Author: | Padilla  \n---|---  \nVersion: | Amended  \nVersion Date: | 05/23/2025  \n  \nAmended IN Senate May 23, 2025\n\nAmended IN Senate March 26, 2025\n\nCALIFORNIA LEGISLATURE--2025-2026 REGULAR SESSION\n\n**Senate Bill**\n\n**No. 420**\n\n**Introduced by Senator Padilla**\n\nFebruary 18, 2025\n\n**An act to add Chapter 24.6 (commencing with Section 22756) to Division 8 of\nthe Business and Professions Code, and to add Article 11 (commencing with\nSection 10285.8) to Chapter 1 of Part 2 of Division 2 of the Public Contract\nCode, relating to artificial intelligence.**\n\nLEGISLATIVE COUNSEL'S DIGEST\n\nSB 420, as amended, Padilla. Automated decision systems.\n\nThe California AI Transparency Act requires a covered provider, as defined, of\na generative artificial intelligence system to make available an AI detection\ntool at no cost to the user that meets certain criteria, including that the\ntool outputs any system provenance data, as defined, that is detected in the\ncontent. The California Consumer Privacy Act of 2018 grants a consumer various\nrights with respect to personal information that is collected or sold by a\nbusiness, as defined, including the right to direct a business that sells or\nshares personal information about the consumer to third parties not to sell or\nshare the consumer's personal information, as specified.\n\nThis bill would generally regulate a developer or a deployer of a high-risk\nautomated decision system, as defined, including by requiring a developer or a\ndeployer to perform an impact assessment on the high-risk automated decision\nsystem before making it publicly available or deploying it, as prescribed. The\nbill would require a state agency to require a developer of a high-risk\nautomated decision system deployed by the state agency to provide to the state\nagency a copy of the impact assessment and would require the state agency to\nkeep that impact assessment confidential. The bill would also require a\ndeveloper to provide to the Attorney General or Civil Rights Department,\nwithin 30 days of a request from the Attorney General or the Civil Rights\nDepartment, a copy of an impact assessment and would require the impact\nassessment to be kept confidential.\n\nThis bill would authorize the Attorney General or the Civil Rights Department\nto bring a specified civil action to enforce compliance with the bill, as\nprescribed, and would authorize _the Attorney General or the Civil Rights\nDepartment to allow_ a developer or deployer to cure, within 45 days of\nreceiving a certain notice of a violation, the noticed~~violation and provide\nan express written statement, made under penalty of perjury, that the\nviolation has been cured. By expanding the scope of the crime of perjury, this\nbill would impose a state-mandated local program.~~ _violation, as\nprescribed._\n\nThis bill would prohibit a state agency from awarding a contract for a high-\nrisk automated decision system~~to a person who has violated,~~ _unless the\nperson to whom the contract is awarded has certified that the high-risk\nautomated decision system does not violate,_ among other civil rights laws,\nthe bill._By expanding the scope of the crime of perjury, this bill would\nimpose a state-mandated local program._\n\nExisting constitutional provisions require that a statute that limits the\nright of access to the meetings of public bodies or the writings of public\nofficials and agencies be adopted with findings demonstrating the interest\nprotected by the limitation and the need for protecting that interest.\n\nThis bill would make legislative findings to that effect.\n\nThe California Constitution requires the state to reimburse local agencies and\nschool districts for certain costs mandated by the state. Statutory provisions\nestablish procedures for making that reimbursement.\n\nThis bill would provide that no reimbursement is required by this act for a\nspecified reason.\n\nVote: MAJORITY Appropriation: NO Fiscal Committee: YES Local Program: YES\n\nThe people of the State of California do enact as follows:\n\nSECTION 1. The Legislature finds and declares all of the following:\n\n(a) (1) Artificial intelligence technologies are becoming an integral part of\ndaily life in California and have profound implications for privacy, equity,\nfairness, and public safety.\n\n(2) It is critical to protect individuals' rights to safeguard against\npotential harms, including discrimination, privacy violations, and unchecked\nautomation in critical decisionmaking processes.\n\n(3) A comprehensive set of rights must be established to ensure artificial\nintelligence technologies align with the public interest and reflect the\nvalues of California residents.\n\n(b) (1) Individuals should have the right to receive a clear and accessible\nexplanation about how artificial intelligence systems operate, including the\ndata they use and the decisions they make.\n\n(2) An entity that uses artificial intelligence systems to make decisions\nimpacting California residents should provide a mechanism to inform\nindividuals of the system's logic, processing methods, and intended outcomes\nin a manner that is understandable.\n\n(c) (1) All individuals have the right to control their personal data in\nrelation to artificial intelligence systems. Artificial intelligence systems\nshould operate with the highest standards of data privacy and security, in\nline with the California Consumer Privacy Act of 2018 and other relevant\nprivacy laws.\n\n(2) Before personal data is used in artificial intelligence systems, entities\nshould obtain informed, explicit consent from individuals, and individuals\nshould have the right to withdraw consent at any time without penalty.\n\n(3) Entities should ensure that personal data used by artificial intelligence\nsystems is anonymized or pseudonymized if feasible, and data retention should\nbe limited to the purposes for which the data was initially collected.\n\n(d) (1) Artificial intelligence systems should not discriminate against\nindividuals based on race, gender, sexual orientation, disability, religion,\nsocioeconomic status, or other protected characteristics under California law.\n\n(2) Entities deploying artificial intelligence technologies should perform\nregular audits to identify and address any biases or inequities in their\nartificial intelligence systems and should ensure that artificial intelligence\nsystems are designed and trained to promote fairness and equal treatment.\n\n(e) (1) Individuals should have the right to hold entities accountable for any\nharm caused by artificial intelligence systems, and entities should be liable\nfor the actions and decisions made by artificial intelligence technologies\nthey deploy.\n\n(2) An individual or group adversely affected by artificial intelligence-\ndriven decisions should have access to a straightforward and transparent\nprocess for seeking redress, including the ability to challenge those\ndecisions through human review and appeal mechanisms.\n\n(f) (1) Individuals should have the right to request human oversight for\nsignificant decisions made by artificial intelligence systems that impact\nthem, particularly in areas such as employment, health care, housing,\neducation, and criminal justice.\n\n(2) Artificial intelligence systems in high-stakes decisionmaking contexts\nshould involve human review or intervention before final decisions, ensuring\nthat automated decisions align with human values and public policy goals.\n\nSEC. 2. Chapter 24.6 (commencing with Section 22756) is added to Division 8 of\nthe Business and Professions Code, to read:\n\nCHAPTER 24.6. Automated Decision Systems\n\n22756\\. As used in this chapter:\n\n(a) \"Algorithmic discrimination\" means the condition in which an automated\ndecision system contributes to unlawful discrimination on the basis of a\nprotected classification.\n\n(b) \"Artificial intelligence\" means an engineered or machine-based system that\nvaries in its level of autonomy and that can, for explicit or implicit\nobjectives, infer from the input it receives how to generate outputs that can\ninfluence physical or virtual environments.\n\n(c) (1) \"Automated decision system\" means a computational process derived from\nmachine learning, statistical modeling, data analytics, or artificial\nintelligence that issues simplified output, including a score, classification,\nor recommendation, that is used to assist or replace human discretionary\ndecisionmaking and materially impacts natural persons.\n\n(2) \"Automated decision system\" does not mean a spam email filter, firewall,\nantivirus software, identity and access management tool, calculator, database,\ndataset, or other compilation of data.\n\n(d) \"Deployer\" means a natural person or entity that uses a high-risk\nautomated decision system in the state.\n\n(e) \"Detecting decisionmaking patterns without influencing outcomes\" means the\nact of artificial intelligence analyzing patterns for informational purposes\nwithout direct influence on decisions.\n\n(f) \"Developer\" means a natural person or entity that designs, codes,\nproduces, or~~substantially modifies~~ _makes a substantial modification to_ a\nhigh-risk automated decision system for use in the state.\n\n(g) \"Education enrollment or opportunity\" means the chance to obtain\nadmission, accreditation, evaluation, certification, vocational training,\nfinancial aid, or scholarships with respect to an educational opportunity.\n\n(h) \"Employment or employment opportunity\" means hiring, salary, wage, or\nother material term, condition, or privilege of an employee's employment.\n\n(i) \"Health care\" means health care services or insurance for health, mental\nhealth, dental, or vision.\n\n(j) (1) \"High-risk automated decision system\" means an automated decision\nsystem that is used to assist or replace human discretionary decisions that\nhave a legal or similarly significant effect, including decisions that\nmaterially impact access to, or approval for, any of the following:\n\n(A) Education enrollment or opportunity.\n\n(B) Employment or employment opportunity.\n\n(C) Essential utilities.\n\n(D) Temporary, short-term, or long-term housing.\n\n(E) Health care services.\n\n(F) Lending services.\n\n(G) A legal right or service.\n\n(H) An essential government service.\n\n(2) \"High-risk automated decision system\" does not include an automated\ndecision system that only performs narrow procedural tasks, enhances human\nactivities, detects patterns without influencing decisions, or assists in\npreparatory tasks for assessment.\n\n(k) \"Improving results of previously completed human activities\" means the act\nof artificial intelligence enhancing existing human-performed tasks without\naltering decisions.\n\n(l) \"Narrow procedural task\" means a limited, procedural task that has a\nminimal impact on outcomes.\n\n(m) \"Preparatory task for assessment\" means a task in which an artificial\nintelligence aids in a preparatory task for assessment or evaluation without\ndirect decisionmaking authority.\n\n(n) \"Protected classification\" means a classification protected under existing\nlaw prohibiting discrimination, including, but not limited to, the\n_California_ Fair Employment and Housing Act (Chapter 7 (commencing with\nSection 12960) of Part 2.8 of Division 3 of Title 2 of the Government Code) or\nthe Unruh Civil Rights Act (Section 51 of the Civil Code).\n\n(o) (1) \"State agency\" means any of the following:\n\n(A) A state office, department, division, or bureau.\n\n(B) The California State University.\n\n(C) The Board of Parole Hearings.\n\n(D) A board or other professional licensing and regulatory body under the\nadministration or oversight of the Department of Consumer Affairs.\n\n(2) \"State agency\" does not include the University of California, the\nLegislature, the judicial branch, or a board that is not described in\nparagraph (1).\n\n(p) \"Substantial modification\" means a new version, release, or other\nsignificant update that materially changes the functionality or performance of\na high-risk automated decision system, including the results of retraining.\n\n22756.1. (a) (1) For a high-risk automated decision system made publicly\navailable for use on or after January 1, 2026, a developer shall perform an\nimpact assessment on the high-risk automated decision system before making the\nhigh-risk automated decision system publicly available for use.\n\n(2) For a high-risk automated decision system first made publicly available\nfor use before January 1, 2026, a developer shall perform an impact\nassessment~~if the developer makes a substantial modification to the high-risk\nautomated decision system.~~ _on or before January 1, 2028._\n\n(b) (1) Except as provided in paragraph (2), for a high-risk automated\ndecision system first deployed after January 1, 2026, a deployer shall perform\nan impact assessment within two years of deploying the high-risk automated\ndecision system.\n\n(2) A state agency that is a deployer may opt out of performing an impact\nassessment if the state agency uses the automated decision system only for its\nintended use as determined by the developer and all of the following\nrequirements are met:\n\n(A) The state agency does not make a substantial modification to the high-risk\nautomated decision system.\n\n(B) The developer of the high-risk automated decision system is in compliance\nwith Section 10285.8 of the Public Contract Code and subdivision (d).\n\n(C) The state agency does not have a reasonable basis to believe that\ndeployment of the high-risk automated decision system as intended by the\ndeveloper is likely to result in algorithmic discrimination.\n\n(D) The state agency is in compliance with Section~~22756.3.~~ _22756.4._\n\n(c) ~~(1)~~ A developer shall make available to deployers and potential\ndeployers the statements included in the developer's impact assessment\npursuant to~~paragraph (2).~~ _Section 22756.2._\n\n~~(2) An impact assessment prepared pursuant to this section shall include all\nof the following:~~\n\n~~(A) A statement of the purpose of the high-risk automated decision system\nand its intended benefits, intended uses, and intended deployment contexts.~~\n\n~~(B) A description of the high-risk automated decision system 's intended\noutputs.~~\n\n~~(C) A summary of the types of data intended to be used as inputs to the\nhigh-risk automated decision system and any processing of those data inputs\nrecommended to ensure the intended functioning of the high-risk automated\ndecision system.~~\n\n~~(D) A summary of reasonably foreseeable potential disproportionate or\nunjustified impacts on a protected classification from the intended use by\ndeployers of the high-risk automated decision system.~~\n\n~~(E) A developer 's impact assessment shall also include both of the\nfollowing:~~\n\n~~(i) A description of safeguards implemented or other measures taken by the\ndeveloper to mitigate and guard against risks known to the developer of\nalgorithmic discrimination arising from the use of the high-risk automated\ndecision system.~~\n\n~~(ii) A description of how the high-risk automated decision system can be\nmonitored by a deployer for risks of algorithmic discrimination known to the\ndeveloper.~~\n\n~~(F) A statement of the extent to which the deployer 's use of the high-risk\nautomated decision system is consistent with, or varies from, the developer's\nstatement of the high-risk automated decision system's purpose and intended\nbenefits, intended uses, and intended deployment contexts.~~\n\n~~(G) A description of safeguards implemented or other measures taken to\nmitigate and guard against any known risks to the deployer of discrimination\narising from the high-risk automated decision system.~~\n\n~~(H) A description of how the high-risk automated decision system has been,\nand will be, monitored and evaluated.~~\n\n(d) (1) A state agency shall require a developer of a high-risk automated\ndecision system deployed by the state agency to provide to the state agency a\ncopy of the impact assessment conducted pursuant to this section.\n\n(2) Notwithstanding any other law, an impact assessment provided to a state\nagency pursuant to this subdivision shall be kept confidential.\n\n_22756.2. An impact assessment prepared pursuant to this chapter shall include\nall of the following:_\n\n_(a) A statement of the purpose of the high-risk automated decision system and\nits intended benefits, intended uses, and intended deployment contexts._\n\n_(b) A description of the high-risk automated decision system 's intended\noutputs._\n\n_(c) A summary of the types of data intended to be used as inputs to the high-\nrisk automated decision system and any processing of those data inputs\nrecommended to ensure the intended functioning of the high-risk automated\ndecision system._\n\n_(d) A summary of reasonably foreseeable potential disproportionate or\nunjustified impacts on a protected classification from the intended use by\ndeployers of the high-risk automated decision system._\n\n_(e) A developer 's impact assessment shall also include both of the\nfollowing:_\n\n_(1) A description of safeguards implemented or other measures taken to\nmitigate and guard against risks known to the developer of algorithmic\ndiscrimination arising from the use of the high-risk automated decision\nsystem._\n\n_(2) A description of how the high-risk automated decision system can be\nmonitored by a deployer for risks of algorithmic discrimination known to the\ndeveloper._\n\n_(f) A statement of the extent to which the deployer 's use of the high-risk\nautomated decision system is consistent with, or varies from, the developer's\nstatement of the high-risk automated decision system's purpose and intended\nbenefits, intended uses, and intended deployment contexts._\n\n_(g) A description of safeguards implemented or other measures taken to\nmitigate and guard against any known risks of discrimination arising from the\nhigh-risk automated decision system._\n\n_(h) A description of how the high-risk automated decision system has been,\nand will be, monitored and evaluated._\n\n~~22756.2.~~ _22756.3._ (a) If a deployer uses a high-risk automated decision\nsystem to make a decision regarding a natural person, the deployer shall\nnotify the natural person of that fact and disclose to that natural person all\nof the following:\n\n(1) The purpose of the high-risk automated decision system and the specific\ndecision it was used to make.\n\n(2) How the high-risk automated decision system was used to make the decision.\n\n(3) The type of data used by the high-risk automated decision system.\n\n(4) Contact information for the deployer.\n\n(5) A link to the statement required by subdivision (b).\n\n(b) A deployer shall make available on its internet website a statement\nsummarizing all of the following:\n\n(1) The types of high-risk automated decision systems it currently deploys.\n\n(2) How the deployer manages known or reasonably foreseeable risks of\nalgorithmic discrimination arising from the deployment of those high-risk\nautomated decision systems.\n\n(3) The nature and source of the information collected and used by the high-\nrisk automated decision systems deployed by the deployer.\n\n(c) A deployer shall provide, as technically feasible, a natural person that\nis the subject of a decision made by a high-risk automated decision system an\nopportunity to appeal that decision for review by a natural person.\n\n~~22756.3.~~ _22756.4._ (a) A developer or a deployer shall establish,\ndocument, implement, and maintain a governance program that~~contains\nreasonable administrative and technical safeguards to govern the reasonably\nforeseeable risks of algorithmic discrimination associated with the use, or\nintended use, of a high-risk automated decision system.~~ _does all of the\nfollowing:_\n\n_(1) Contains reasonable administrative and technical safeguards to govern the\nreasonably foreseeable risks of algorithmic discrimination associated with the\nuse, or intended use, of a high-risk automated decision system._\n\n_(2) Aligns with existing standards and frameworks, including the National\nInstitute of Standards and Technology AI Risk Management Framework._\n\n_(3) Specifies and incorporates the principles, processes, and personnel used\nto identify, document, and mitigate foreseeable risks of algorithmic\ndiscrimination._\n\n_(4) Is regularly reviewed and updated._\n\n_(5) Includes a structural framework for documenting, investigating, and\nresolving incidents._\n\n(b) The governance program required by this~~subdivision~~ _section_ shall be\nappropriately designed with respect to all of the following:\n\n(1) The use, or intended use, of the high-risk automated decision system.\n\n(2) The size, complexity, and resources of the deployer or developer.\n\n(3) The nature, context, and scope of the activities of the deployer or\ndeveloper in connection with the high-risk automated decision system.\n\n(4) The technical feasibility and cost of available tools, assessments, and\nother means used by a deployer or developer to map, measure, manage, and\ngovern the risks associated with a high-risk automated decision system.\n\n~~22756.4.~~ _22756.5. (a)_ A developer or deployer is not required to\ndisclose information under this chapter if the disclosure of that information\nwould result in the waiver of a legal privilege or the disclosure of a trade\nsecret, as defined in Section 3426.1 of the Civil Code.\n\n_(b) If a disclosure is not made because the disclosure would reveal a trade\nsecret, the developer or deployer shall notify the party to whom the\ndisclosure would have otherwise been made of the basis for which the\ndisclosure was not made._\n\n~~22756.5.~~ _22756.6._ (a) Except as provided in subdivision (b), a deployer\nor developer shall not deploy or make available for deployment a high-risk\nautomated decision system if the impact assessment performed pursuant to this\nchapter determines that the high-risk automated decision system is likely to\nresult in algorithmic discrimination.\n\n(b) (1) A deployer or developer may deploy or make available for deployment a\nhigh-risk automated decision system if the impact assessment performed\npursuant to this chapter determines that the high-risk automated decision\nsystem will result in algorithmic discrimination if the deployer or developer\nimplements safeguards to mitigate the known risks of algorithmic\ndiscrimination.\n\n(2) A deployer or developer acting under the exception provided by paragraph\n(1) shall perform an updated impact assessment to verify that the algorithmic\ndiscrimination has been mitigated and is not reasonably likely to occur.\n\n~~22756.6.~~ _22756.7._ (a) (1) A developer shall provide to the Attorney\nGeneral or Civil Rights Department, within 30 days of a request from the\nAttorney General or the Civil Rights Department, a copy of an impact\nassessment performed pursuant to this chapter.\n\n(2) Notwithstanding any other law, an impact assessment provided to the\nAttorney General or Civil Rights Department pursuant to this subdivision shall\nbe kept confidential.\n\n(b) The Attorney General or the Civil Rights Department may bring a civil\naction against a deployer or developer for a violation of this chapter and\nobtain any of the following relief:\n\n(1) (A) If a developer or deployer fails to conduct an impact assessment as\nrequired under this chapter, a civil penalty of two thousand five hundred\ndollars ($2,500) for a defendant with fewer than 100 employees, five thousand\ndollars ($5,000) if the defendant has fewer than 500 employees, and ten\nthousand dollars ($10,000) if the defendant has at least 500 employees.\n\n(B) If a violation is intentional, the civil penalty pursuant to this\nparagraph shall increase by five hundred dollars ($500) for each day that the\ndefendant is noncompliant.\n\n(2) Injunctive relief.\n\n(3) Reasonable attorney's fees and costs.\n\n(4) If the violation concerns algorithmic discrimination, a civil penalty of\ntwenty-five thousand dollars ($25,000) per violation.\n\n(c) (1) Before commencing an action pursuant to this section, the Attorney\nGeneral or the Civil Rights Department shall provide 45 days' written notice\nto a deployer or developer of any alleged violation of this chapter.\n\n~~(2) (A) The developer or deployer may cure, within 45 days of receiving the\nwritten notice described in paragraph (1), the noticed violation and provide\nan express written statement, made under penalty of perjury, that the\nviolation has been cured.~~\n\n~~(B) If the developer or deployer cures the noticed violation and provides\nthe express written statement pursuant to subparagraph (A), an action shall\nnot be maintained for the noticed violation.~~\n\n_(2) The Attorney General or the Civil Rights Department may, at its\ndiscretion, provide to a developer or a deployer with a time period to cure\nthe alleged violation after considering all of the following:_\n\n_(A) A lack of intent to commit the violation._\n\n_(B) Voluntary efforts undertaken to cure the alleged violation before being\nnotified of the violation._\n\n_(C) The size and economic resources of the noncompliant developer or\ndeployer._\n\n_(D) The size and scope of the impact of the decisions made by an automated\ndecision system related to the violation._\n\n~~22756.7.~~ _22756.8._ This chapter does not apply to either of the\nfollowing:\n\n(a) An entity with 50 or fewer employees.\n\n(b) A high-risk automated decision system that has been approved, certified,\nor cleared by a federal agency that complies with another law that is\nsubstantially the same or more stringent than this chapter.\n\nSEC. 3. Article 11 (commencing with Section 10285.8) is added to Chapter 1 of\nPart 2 of Division 2 of the Public Contract Code, to read:\n\nArticle 11. High-Risk Automated Decision Systems\n\n10285.8. (a) A state agency shall not award a contract for a high-risk\nautomated decision system~~to a person who has violated~~ _unless the person\nto whom the contract is awarded has certified that the high-risk automated\ndecision system does not violate_ any of the following:\n\n(1) The Unruh Civil Rights Act (Section 51 of the Civil Code).\n\n(2) The California Fair Employment and Housing Act (Chapter 7 (commencing with\nSection 12960) of Part 2.8 of Division 3 of Title 2 of the Government Code).\n\n(3) Chapter 24.6 (commencing with Section 22756) of Division 8 of the Business\nand Professions Code.\n\n(b) As used in this section, \"high-risk automated decision system\" has the\nsame meaning as defined in Section 22756 of the Business and Professions Code.\n\nSEC. 4. The Legislature finds and declares that Section 2 of this act, which\nadds Chapter 24.6 (commencing with Section 22756)_to Division 8_ of the\nBusiness and Professions Code, imposes a limitation on the public's right of\naccess to the meetings of public bodies or the writings of public officials\nand agencies within the meaning of Section 3 of Article I of the California\nConstitution. Pursuant to that constitutional provision, the Legislature makes\nthe following findings to demonstrate the interest protected by this\nlimitation and the need for protecting that interest:\n\nTo avoid unduly disrupting commerce, it is necessary that trade secrets be\nprotected.\n\nSEC. 5. No reimbursement is required by this act pursuant to Section 6 of\nArticle XIIIB of the California Constitution because the only costs that may\nbe incurred by a local agency or school district will be incurred because this\nact creates a new crime or infraction, eliminates a crime or infraction, or\nchanges the penalty for a crime or infraction, within the meaning of Section\n17556 of the Government Code, or changes the definition of a crime within the\nmeaning of Section 6 of Article XIIIB of the California Constitution.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": true
    }
  ]
}