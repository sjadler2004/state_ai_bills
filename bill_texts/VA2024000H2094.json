{
  "bill_id": "VA2024000H2094",
  "source_url": "http://custom.statenet.com/public/resources.cgi?id=ID:bill:VA2024000H2094&cuiq=93d84396-c63b-526a-b152-38b7f79b4cfd&client_md=e4f6fea4-27b4-5d41-b7d3-766fe52569f0",
  "versions": [
    {
      "date": "01/07/2025",
      "label": "Prefiled",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:VA2024000H2094&verid=VA2024000H2094_20250107_0_F&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2024 VA H 2094</td> <td><table><tr><td class=\"label\">Author:</td> <td>Maldonado</td></tr> <tr><td class=\"label\">Version:</td> <td>Prefiled</td></tr> <tr><td class=\"label\">Version Date:</td> <td>01/07/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">\n    <b>HOUSE BILL NO. 2094</b>\n   </p>\n   <p class=\"center\">Offered January 8, 2025</p>\n   <p class=\"center\">Prefiled January 7, 2025</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"indent\">\n    <i>A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-613, relating to high-risk artificial intelligence; development, deployment, and use; civil penalties.</i>\n   </p>\n   <p class=\"center\">Patron--Maldonado</p>\n   <p class=\"center\">Committee Referral Pending</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">\n     <b>Be it enacted by the General Assembly of Virginia:</b>\n    </p>\n   </span>\n   <p class=\"indent\">\n    <b>1. That the Code of Virginia is amended by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-613, as follows:</b>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">CHAPTER 58.</u>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-607. Definitions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">As used in this chapter, unless the context requires a different meaning:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Algorithmic discrimination&quot; means the use of an artificial intelligence system that results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, sexual orientation, veteran status, or other classification protected under state or federal law. &quot;Algorithmic discrimination&quot; does not include (i) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; (ii) the expansion of an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or (iii) an act or omission by or on behalf of a private club or other establishment not in fact open to the public, as set forth in Title II of the Civil Rights Act of 1964, 42 U.S.C. &sect; 2000a(e), as amended from time to time.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Artificial intelligence system&quot; means any machine learning-based system that, for any explicit or implicit objective, infers from the inputs such system receives how to generate outputs, including content, decisions, predictions, and recommendations, that can influence physical or virtual environments.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consequential decision&quot; means any decision that has a material legal, or similarly significant, effect on the provision or denial to any consumer of, or the cost or terms of, (i) parole, probation, a pardon, or any other release from incarceration or supervision, (ii) education enrollment or an education opportunity, (iii) employment, (iv) a financial or lending service, (v) health care services, (vi) housing, (vii) insurance, or (viii) a legal service.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consumer&quot; means a natural person who is a resident of the Commonwealth and is acting only in an individual or household context. &quot;Consumer&quot; does not include a natural person acting in a commercial or employment context.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Deployer&quot; means any person doing business in the Commonwealth that deploys or uses a high-risk artificial intelligence system to make a consequential decision in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Developer&quot; means any person doing business in the Commonwealth that develops or intentionally and substantially modifies a high-risk artificial intelligence system that is offered, sold, leased, given, or otherwise provided to consumers in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Distributor&quot; means a person doing business in the Commonwealth, other than a developer, that makes an artificial intelligence system available in the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Foundation model&quot; means a machine learning model that (i) is trained on broad data at scale, (ii) is designed for generality of output, and (iii) can be adapted to a wide range of distinctive tasks.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;General-purpose artificial intelligence model&quot; means any form of artificial intelligence system that (i) displays significant generality, (ii) is capable of competently performing a wide range of distinct tasks, and (iii) can be integrated into a variety of downstream applications or systems. &quot;General-purpose artificial intelligence model&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is released on the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence&quot; means artificial intelligence capable of emulating the structure and characteristics of input data in order to generate derived synthetic content, including audio, images, text, and videos.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence system&quot; means any artificial intelligence system or service that incorporates generative artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that is specifically intended to autonomously make, or be a substantial factor in making, a consequential decision. A system or service is not a &quot;high-risk artificial intelligence system&quot; if it is intended to (i) perform a narrow procedural task, (ii) improve the result of a previously completed human activity, (iii) detect any decision-making patterns or any deviations from pre-existing decision-making patterns, or (iv) perform a preparatory task to an assessment relevant to a consequential decision. &quot;High-risk artificial intelligence system&quot; does not include any of the following technologies:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. Anti-fraud technology that does not use facial recognition technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Anti-malware technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Anti-virus technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Artificial intelligence-enabled video games;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. Calculators;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. Cybersecurity technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. Databases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. Data storage;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. Firewall technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. Internet domain registration;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. Internet website loading;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. Networking;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. Spam and robocall filtering;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. Spell-checking technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. Spreadsheets;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">16. Web caching;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">17. Web hosting or any similar technology; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">18. Technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an accepted use policy that prohibits generating content that is discriminatory or harmful.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Integrator&quot; means a person that knowingly integrates an artificial intelligence system into a software application and places such software application on the market. An &quot;integrator&quot; does not include a person offering information technology infrastructure.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Intentional and substantial modification&quot; means any deliberate change made to (i) an artificial intelligence system that results in any new reasonably foreseeable risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects compliance of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. &quot;Intentional and substantial modification&quot; does not include any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (a) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (b) such change (1) is made to such high-risk artificial intelligence system as a result of any learning described in clause (a), and (2) was predetermined by the deployer or the third party contracted by the deployer and concluded and included within the initial impact assessment of such high-risk artificial intelligence system as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Machine learning&quot; means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Person&quot; includes any individual, corporation, partnership, association, cooperative, limited liability company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. &quot;Person&quot; does not include any government or political subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Principal basis&quot; means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Red-teaming&quot; means an exercise that is conducted to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Significant update&quot; means any new version, new release, or other update to a high-risk artificial intelligence system that results in significant changes to such high-risk artificial intelligence system&#39;s use case or key functionality and that results in any new or reasonably foreseeable risk of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Social media platform&quot; means an electronic medium or service where users may create, share, or view user-generated content, including videos, photographs, blogs, podcasts, messages, emails, or website profiles or locations, and create a personal account.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Substantial factor&quot; means a factor that is (i) the principal basis for making a consequential decision, (ii) capable of altering the outcome of a consequential decision, and (iii) generated by an artificial intelligence system. &quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as the principal basis to make a consequential decision concerning the consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Synthetic digital content&quot; means any digital content, including any audio, image, text, or video, that is produced or manipulated by a generative artificial intelligence system, including a general-purpose artificial intelligence model.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique, or process, that (i) derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by, other persons who can obtain economic value from its disclosure or use and (ii) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-608. Operating standards for developers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. No developer of a high-risk artificial intelligence system shall offer, sell, lease, give, or otherwise provide to a deployer, or other developer, a high-risk artificial intelligence system unless the developer makes available to the deployer or other developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement disclosing the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Documentation disclosing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">a. The known or reasonably known limitations of such high-risk artificial intelligence system, including any and all known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">b. The purpose of such high-risk artificial intelligence system and the intended benefits and uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">c. A summary describing how such high-risk artificial intelligence system was evaluated for performance before such high-risk artificial intelligence system was licensed, sold, leased, given, or otherwise made available to a deployer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">d. The measures the developer has taken to mitigate reasonable foreseeable risks of algorithmic discrimination that the developer knows arises from deployment or use of such high-risk artificial intelligence system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">e. How an individual can use such high-risk artificial intelligence system and monitor the performance of such high-risk artificial intelligence system for any risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Documentation describing (i) how the high-risk artificial intelligence system was evaluated for performance and for mitigation of algorithmic discrimination before such system was made available to the deployer; (ii) the data governance measures used to cover the training data sets and the measures used to examine the suitability of data sources, possible biases of data sources, and appropriate mitigation; (iii) the intended outputs of the high-risk artificial intelligence system; (iv) the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and (v) how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when such system is used to make, or is a substantial factor in making, a consequential decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any additional documentation that is reasonably necessary to assist the deployer in understanding the outputs and monitoring performance of the high-risk artificial intelligence system for risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Each developer that offers, sells, leases, gives, or otherwise makes available to a deployer a high-risk artificial intelligence system shall make available to the deployer, to the extent feasible and necessary, information and documentation through artifacts such as model cards or impact assessments, and such documentation and information shall enable the deployer or a third party contracted by the deployer to complete an impact assessment as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. A developer that also serves as a deployer for any high-risk artificial intelligence system shall not be required to generate the documentation required by this section unless such high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer or as otherwise required by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Nothing in this section shall be construed to require a developer to disclose any trade secret.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each developer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. 1. Each developer of a high-risk artificial intelligence system, including a general purpose artificial intelligence model, that generates or manipulates synthetic digital content shall ensure that the outputs of such high-risk artificial intelligence system are marked and detectable, in a manner that is detectable by consumers and complies with any applicable accessibility requirements, as synthetic digital content no later than the time that consumers who did not create such outputs first interact with or are exposed to such output;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. If such synthetic digital content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for marking outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The marking of outputs required by subdivision 1 shall not apply to (i) synthetic digital content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic digital content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-609. Operating standards for deployers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each deployer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought on or after such date by the Attorney General pursuant to &sect; 59.1-613, there shall be a rebuttable presumption that a deployer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the deployer complied with the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has designed and implemented a risk management policy and program for such high-risk artificial intelligence system. The risk management policy shall specify the principles, processes, and personnel that the deployer shall use in maintaining the risk management program to identify, mitigate, and document any risk of algorithmic discrimination that is a reasonably foreseeable consequence of deploying or using such high-risk artificial intelligence system to make a consequential decision. Each risk management policy and program designed, implemented, and maintained pursuant to this subsection shall be (i) at least as stringent as the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems and (ii) reasonable considering (a) the size and complexity of the deployer; (b) the nature and scope of the high-risk artificial intelligence systems deployed and used by the deployer, including the intended uses of such high-risk artificial intelligence systems; (c) the sensitivity and volume of data processed in connection with the high-risk artificial intelligence systems deployed and used by the deployer; and (d) the cost to the deployer to implement and maintain such risk management program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Except as provided in this subsection, no deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has completed an impact assessment for such high-risk artificial intelligence system. The deployer shall complete an impact assessment for a high-risk artificial intelligence system (i) before the deployer initially deploys such high-risk artificial intelligence system and (ii) not later than 90 days after each significant update to such high-risk artificial intelligence system is made available.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each impact assessment completed pursuant to this subsection shall include, at a minimum:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement by the deployer disclosing (i) the purpose, intended use cases and deployment context of, and benefits afforded by the high-risk artificial intelligence system and (ii) whether the deployment or use of the high-risk artificial intelligence system poses any known or reasonably foreseeable risk of algorithmic discrimination and, if so, (a) the nature of such algorithmic discrimination and (b) the steps that have been taken, to the extent feasible, to mitigate such risk;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. For each post-deployment impact assessment completed pursuant to this subsection, whether the intended use cases of the high-risk artificial intelligence system as updated were consistent with, or varied from, the developer&#39;s intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A description of (i) the categories of data the high-risk artificial intelligence system processes as inputs and (ii) the outputs such high-risk artificial intelligence system produces;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. If the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. A list of any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. A description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that such high-risk artificial intelligence system is in use when such high-risk artificial intelligence system is in use; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. A description of any post-deployment monitoring performed and user safeguards provided concerning such high-risk artificial intelligence system, including any oversight process established by the deployer to address issues arising from deployment or use of such high-risk artificial intelligence system as such issues arise.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed or used by a deployer. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations. If a deployer completes an impact assessment for the purpose of complying with another applicable law or regulation, such impact assessment shall be deemed to satisfy the requirements established in this subsection if such impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection. A deployer that completes an impact assessment pursuant to this subsection shall maintain such impact assessment and all records concerning such impact assessment for three years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Not later than the time that a deployer uses a high-risk artificial intelligence system to interact with a consumer, the deployer shall disclose to the consumer that the deployer is interacting with an artificial intelligence system disclosing (i) the purpose of such high-risk artificial intelligence system, (ii) the nature of such system, (iii) the nature of the consequential decision, (iv) the contact information for the deployer, and (v) a description of the artificial intelligence system in plain language of such system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">If such consequential decision is adverse to such consumer, the deployer shall provide to the consumer (a) a statement disclosing the principal reason or reasons for the consequential decision, including (1) the degree to which and manner in which the high-risk artificial intelligence system contributed to the consequential decision, (2) the type of data that was processed by such system in making the consequential decision, and (3) the sources of such data; (b) an opportunity to correct any incorrect personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and (c) an opportunity to appeal such adverse consequential decision concerning the consumer arising from the deployment of such system. Any such appeal shall allow for human review, if technically feasible, unless providing the opportunity for appeal is not in the best interest of the consumer, including instances in which any delay might pose a risk to the life or safety of such consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each deployer shall make available, in a manner that is clear and readily available, a statement summarizing how such deployer manages any reasonably foreseeable risk of algorithmic discrimination that may arise from the use or deployment of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each deployer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. Any deployer who performs an intentional and substantial modification to any high-risk artificial system shall comply with the documentation and disclosure requirements for developers pursuant to subsections A through F of &sect; 59.1-608.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-610. Operating standards for integrators of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each integrator of a high-risk artificial intelligence system shall develop and adopt an acceptable use policy, which shall limit the use of the high-risk artificial intelligence system to mitigate known risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each integrator of a high-risk artificial intelligence system shall provide to the deployer clear, conspicuous notice of (i) the name or other identifier of the high-risk artificial intelligence system integrated into a software application provided to the deployer; (ii) the name and contact information of the developer of the high-risk artificial intelligence system integrated into a software application provided to the deployer; (iii) whether the integrator has adjusted the model weights of the high-risk artificial intelligence system integrated into the software application by exposing it to additional data, a summary of the adjustment process, and how such process and the resulting system were evaluated for risk of algorithmic discrimination; (iv) a summary of any other non-substantial modifications made by the integrator; and (v) the integrator&#39;s acceptable use policy.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-611. Operating standards for distributors of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each distributor of a high-risk artificial intelligence system shall use reasonable care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. If a distributor of a high-risk artificial intelligence system considers or has reason to consider that a high-risk artificial intelligence system is not in compliance with any requirement of this chapter, it shall immediately withdraw, disable, or recall, as appropriate, the high-risk artificial intelligence system from the market until such system has been brought into compliance with the requirements of this chapter. The distributor shall inform the developers of the high-risk artificial intelligence system concerned and, where applicable, the deployer of any such system&#39;s noncompliance with this chapter and the withdrawal, disablement, or recall of such system. </u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-612. Exemptions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Nothing in this chapter shall be construed to restrict a developer&#39;s, integrator&#39;s, distributor&#39;s or deployer&#39;s ability to (i) comply with federal, state, or municipal ordinances or regulations; (ii) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by federal, state, local, or other governmental authorities; (iii) cooperate with law-enforcement agencies concerning conduct or activity that the developer, integrator, distributor, or deployer reasonably and in good faith believes may violate federal, state, or local law, ordinances, or regulations; (iv) investigate, establish, exercise, prepare for, or defend legal claims; (v) provide a product or service specifically requested by a consumer; (vi) perform under a contract to which a consumer is a party, including fulfilling the terms of a written warranty; (vii) take steps at the request of a consumer prior to entering into a contract; (viii) take immediate steps to protect an interest that is essential for the life or physical safety of the consumer or another individual; (ix) prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, or malicious or deceptive activities; (x) take actions to prevent, detect, protect against, report, or respond to the production, generation, incorporation, or synthesization of child sex abuse material, or any illegal activity, preserve the integrity or security of systems, or investigate, report, or prosecute those responsible for any such action; (xi) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is approved, monitored, and governed by an institutional review board that determines, or similar independent oversight entities that determine, (a) that the expected benefits of the research outweigh the risks associated with such research and (b) whether the developer, integrator, distributor, or deployer has implemented reasonable safeguards to mitigate the risks associated with such research; (xii) assist another developer, integrator, distributor, or deployer with any of the obligations imposed by this chapter; or (xiii) take any action that is in the public interest in the areas of public health, community health, or population health, but solely to the extent that such action is subject to suitable and specific measures to safeguard the public.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall not restrict a developer&#39;s or deployer&#39;s ability to (i) conduct internal research to develop, improve, or repair products, services, or technologies; (ii) effectuate a product recall; (iii) identify and repair technical errors that impair existing or intended functionality; or (iv) perform internal operations that are reasonably aligned with the expectations of the consumer or reasonably anticipated based on the consumer&#39;s existing relationship with the developer, integrator, or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Nothing in this chapter shall be construed to impose any obligation on a developer, integrator, distributor, or deployer to disclose trade secrets or information protected from disclosure by state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall not apply where compliance by the developer, integrator, distributor, or deployer with such obligations would violate an evidentiary privilege under federal law or the laws of the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this chapter shall be construed to impose any obligation on a developer, integrator, distributor, or deployer that adversely affects the legally protected rights or freedoms of any person, including the rights of any person to freedom of speech or freedom of the press guaranteed in the First Amendment to the Constitution of the United States or under the Virginia Human Rights Act (&sect; 2.2-3900 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall not apply to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the U.S. Department of Commerce, the U.S. Department of Defense, and the National Aeronautics and Space Administration, unless such artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For the purposes of this subsection:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Affiliate&quot; means the same as that term is defined in &sect; 6.2-1800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Bank&quot; means the same as that term is defined in &sect; 6.2-800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Credit union&quot; means the same as that term is defined in &sect; 6.2-1300.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Federal credit union&quot; means a credit union duly organized under federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state bank&quot; means the same as that term is defined in &sect; 6.2-836.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state credit union&quot; means a credit union organized and doing business in another state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Subsidiary&quot; means the same as that term is defined in &sect; 6.2-700.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall be deemed satisfied for any bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or any affiliate or subsidiary thereof if such bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or affiliate or subsidiary is subject to examination by any state or federal prudential regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations (i) impose requirements that are substantially equivalent to, and at least as stringent as, the requirements set forth in this chapter, and (ii) at a minimum, require such bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or affiliate or subsidiary to (a) regularly audit such bank&#39;s, out-of-state bank&#39;s, credit union&#39;s, federal credit union&#39;s, out-of-state credit union&#39;s, or affiliate&#39;s or subsidiary&#39;s use of high-risk artificial intelligence systems for compliance with state and federal anti-discrimination laws and regulations applicable to such bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or affiliate or subsidiary and (b) mitigate any algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. For purposes of this subsection, &quot;insurer&quot; means the same as that term is defined in &sect; 38.2-100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The provisions of this chapter shall not apply to any insurer, or any high-risk artificial intelligence system developed or deployed by an insurer for use in the business of insurance, if such insurer is regulated and supervised by the State Corporation Commission or a comparable federal regulating body and subject to examination by such entity under any existing statutes, rules, or regulations pertaining to unfair trade practices and unfair discrimination prohibited under Chapter 5 (&sect; 38.2-500 et seq.) of Title 38.2, or published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations aid in the prevention and mitigation of algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. Nothing in this chapter shall be construed to delegate existing regulatory oversight of the business of insurance to any department or agency other than the Bureau of Insurance of the Virginia State Corporation Commission.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. The provisions of this chapter shall not apply to the development of an artificial intelligence system that is used exclusively for research, training, testing, or other pre-deployment activities performed by active participants of any sandbox software or sandbox environment established and subject to oversight by a designated agency or other government entity and that is in compliance with the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">J. The provisions of this chapter shall not apply to a developer, integrator, distributor, or deployer, or other person who develops, deploys, puts into service, or intentionally modifies, as applicable, a high-risk artificial intelligence system that (i) has been approved, authorized, certified, cleared, developed, or granted by a federal agency acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency or (ii) is in compliance with standards established by a federal agency or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">K. The provisions of this chapter shall not apply to a developer, integrator, distributor, deployer, or other person that is a covered entity within the meaning of the federal Health Insurance Portability and Accountability Act of 1996 (42 U.S.C. &sect; 1320d et seq.) and the regulations promulgated under such federal act, as both may be amended from time to time, and is providing (i) health care recommendations that (a) are generated by an artificial intelligence system and (b) require a health care provider to take action to implement the recommendations or (ii) services utilizing an artificial intelligence system for an administrative, financial, quality measurement, security, or performance improvement function.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">L. If a developer, integrator, distributor, or deployer engages in any action authorized by an exemption set forth in this section, the developer, integrator, distributor, or deployer bears the burden of demonstrating that such action qualifies for such exemption.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-613. Enforcement; civil penalty.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. The Attorney General shall have exclusive authority to enforce the provisions of this chapter. </u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any violation of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of &sect; 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer, integrator, distributor, or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand shall not constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be exempt from disclosure under the Virginia Freedom of Information Act (&sect; 2.2-3700 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any violation of this chapter. The circuit court having jurisdiction may enjoin such violation notwithstanding the existence of an adequate alternative remedy at law. In any action brought pursuant to this chapter, it shall not be necessary that damages be proved.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Any person who violates the provisions of this chapter shall be subject to a civil penalty in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil penalty in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil penalties shall be paid into the Literary Fund.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each violation of this chapter shall constitute a separate violation and shall be subject to any civil penalties imposed under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The Attorney General may require that a developer disclose to the Attorney General any statement or documentation described in this chapter if such statement or documentation is relevant to an investigation conducted by the Attorney General. The Attorney General may also require that a deployer disclose to the Attorney General any risk management policy designed and implemented, impact assessment completed, or record maintained pursuant to this chapter if such risk management policy, impact assessment, or record is relevant to an investigation conducted by the Attorney General.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. In an action brought by the Attorney General pursuant to this section, it shall be an affirmative defense that the developer, integrator, distributor, or deployer (i) discovers a violation of any provision of this chapter through red-teaming; (ii) no later than 45 days after discovering such violation (a) cures such violation and (b) provides notice to the Attorney General in a form and manner as prescribed by the Attorney General that such violation has been cured and evidence that any harm caused by such violation has been mitigated; and (iii) is otherwise in compliance with the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Prior to causing an action against a developer, integrator, distributor, or deployer for a violation of this chapter pursuant to subsection C, the Attorney General shall determine, in consultation with the developer, integrator, distributor, or deployer, if it is possible to cure the violation. If it is possible to cure such violation, the Attorney General may issue a notice of violation to the developer, integrator, distributor, or deployer and afford the developer, integrator, distributor, or deployer the opportunity to cure such violation within 45 days of the receipt of such notice of violation. In determining whether to grant such opportunity to cure such violation, the Attorney General shall consider (i) the number of violations, (ii) the size and complexity of the developer, integrator, distributor, or deployer; (iii) the nature and extent of the developer&#39;s, integrator&#39;s, distributor&#39;s, or deployer&#39;s business; (iv) the substantial likelihood of injury to the public; (v) the safety of persons or property; (vi) whether such violation was likely caused by human or technical error. If the developer, integrator, distributor, or deployer fails to cure such violation within 45 days of the receipt of such notice of violation, the Attorney General may proceed with such action.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Nothing in this chapter shall create a private cause of action in favor of any person aggrieved by a violation of this chapter.</u>\n   </p>\n   <effective_clause>\n    <p class=\"indent\">\n     <b>2. That the provisions of this act shall become effective on July 1, 2026.</b>\n    </p>\n   </effective_clause>\n   <p class=\"indent\">\n    <b>3. That the provisions of this act shall apply only to a violation committed or a cause of action accruing on or after July 1, 2026.</b>\n   </p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2024 VA H 2094 | | Author: | Maldonado  \n---|---  \nVersion: | Prefiled  \nVersion Date: | 01/07/2025  \n  \n**HOUSE BILL NO. 2094**\n\nOffered January 8, 2025\n\nPrefiled January 7, 2025\n\n_A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-613,\nrelating to high-risk artificial intelligence; development, deployment, and\nuse; civil penalties._\n\nPatron--Maldonado\n\nCommittee Referral Pending\n\n**Be it enacted by the General Assembly of Virginia:**\n\n**1\\. That the Code of Virginia is amended by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-613, as\nfollows:**\n\n_CHAPTER 58._\n\n_HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT._\n\n**_§ 59.1-607. Definitions._ **\n\n_As used in this chapter, unless the context requires a different meaning:_\n\n_\" Algorithmic discrimination\" means the use of an artificial intelligence\nsystem that results in an unlawful differential treatment or impact that\ndisfavors an individual or group of individuals on the basis of their actual\nor perceived age, color, disability, ethnicity, genetic information, limited\nproficiency in the English language, national origin, race, religion,\nreproductive health, sex, sexual orientation, veteran status, or other\nclassification protected under state or federal law. \"Algorithmic\ndiscrimination\" does not include (i) the offer, license, or use of a high-risk\nartificial intelligence system by a developer or deployer for the sole purpose\nof the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; (ii) the expansion of an applicant, customer, or participant pool to\nincrease diversity or redress historical discrimination; or (iii) an act or\nomission by or on behalf of a private club or other establishment not in fact\nopen to the public, as set forth in Title II of the Civil Rights Act of 1964,\n42 U.S.C. § 2000a(e), as amended from time to time._\n\n_\" Artificial intelligence system\" means any machine learning-based system\nthat, for any explicit or implicit objective, infers from the inputs such\nsystem receives how to generate outputs, including content, decisions,\npredictions, and recommendations, that can influence physical or virtual\nenvironments._\n\n_\" Consequential decision\" means any decision that has a material legal, or\nsimilarly significant, effect on the provision or denial to any consumer of,\nor the cost or terms of, (i) parole, probation, a pardon, or any other release\nfrom incarceration or supervision, (ii) education enrollment or an education\nopportunity, (iii) employment, (iv) a financial or lending service, (v) health\ncare services, (vi) housing, (vii) insurance, or (viii) a legal service._\n\n_\" Consumer\" means a natural person who is a resident of the Commonwealth and\nis acting only in an individual or household context. \"Consumer\" does not\ninclude a natural person acting in a commercial or employment context._\n\n_\" Deployer\" means any person doing business in the Commonwealth that deploys\nor uses a high-risk artificial intelligence system to make a consequential\ndecision in the Commonwealth._\n\n_\" Developer\" means any person doing business in the Commonwealth that\ndevelops or intentionally and substantially modifies a high-risk artificial\nintelligence system that is offered, sold, leased, given, or otherwise\nprovided to consumers in the Commonwealth._\n\n_\" Distributor\" means a person doing business in the Commonwealth, other than\na developer, that makes an artificial intelligence system available in the\nmarket._\n\n_\" Foundation model\" means a machine learning model that (i) is trained on\nbroad data at scale, (ii) is designed for generality of output, and (iii) can\nbe adapted to a wide range of distinctive tasks._\n\n_\" General-purpose artificial intelligence model\" means any form of artificial\nintelligence system that (i) displays significant generality, (ii) is capable\nof competently performing a wide range of distinct tasks, and (iii) can be\nintegrated into a variety of downstream applications or systems. \"General-\npurpose artificial intelligence model\" does not include any artificial\nintelligence model that is used for development, prototyping, and research\nactivities before such artificial intelligence model is released on the\nmarket._\n\n_\" Generative artificial intelligence\" means artificial intelligence capable\nof emulating the structure and characteristics of input data in order to\ngenerate derived synthetic content, including audio, images, text, and\nvideos._\n\n_\" Generative artificial intelligence system\" means any artificial\nintelligence system or service that incorporates generative artificial\nintelligence._\n\n_\" High-risk artificial intelligence system\" means any artificial intelligence\nsystem that is specifically intended to autonomously make, or be a substantial\nfactor in making, a consequential decision. A system or service is not a\n\"high-risk artificial intelligence system\" if it is intended to (i) perform a\nnarrow procedural task, (ii) improve the result of a previously completed\nhuman activity, (iii) detect any decision-making patterns or any deviations\nfrom pre-existing decision-making patterns, or (iv) perform a preparatory task\nto an assessment relevant to a consequential decision. \"High-risk artificial\nintelligence system\" does not include any of the following technologies:_\n\n_1\\. Anti-fraud technology that does not use facial recognition technology;_\n\n_2\\. Anti-malware technology;_\n\n_3\\. Anti-virus technology;_\n\n_4\\. Artificial intelligence-enabled video games;_\n\n_5\\. Calculators;_\n\n_6\\. Cybersecurity technology;_\n\n_7\\. Databases;_\n\n_8\\. Data storage;_\n\n_9\\. Firewall technology;_\n\n_10\\. Internet domain registration;_\n\n_11\\. Internet website loading;_\n\n_12\\. Networking;_\n\n_13\\. Spam and robocall filtering;_\n\n_14\\. Spell-checking technology;_\n\n_15\\. Spreadsheets;_\n\n_16\\. Web caching;_\n\n_17\\. Web hosting or any similar technology; or_\n\n_18\\. Technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an accepted use\npolicy that prohibits generating content that is discriminatory or harmful._\n\n_\" Integrator\" means a person that knowingly integrates an artificial\nintelligence system into a software application and places such software\napplication on the market. An \"integrator\" does not include a person offering\ninformation technology infrastructure._\n\n_\" Intentional and substantial modification\" means any deliberate change made\nto (i) an artificial intelligence system that results in any new reasonably\nforeseeable risk of algorithmic discrimination or (ii) a general-purpose\nartificial intelligence model that affects compliance of the general-purpose\nartificial intelligence model, materially changes the purpose of the general-\npurpose artificial intelligence model, or results in any new reasonably\nforeseeable risk of algorithmic discrimination. \"Intentional and substantial\nmodification\" does not include any change made to a high-risk artificial\nintelligence system, or the performance of a high-risk artificial intelligence\nsystem, if (a) the high-risk artificial intelligence system continues to learn\nafter such high-risk artificial intelligence system is offered, sold, leased,\nlicensed, given, or otherwise made available to a deployer, or deployed, and\n(b) such change (1) is made to such high-risk artificial intelligence system\nas a result of any learning described in clause (a), and (2) was predetermined\nby the deployer or the third party contracted by the deployer and concluded\nand included within the initial impact assessment of such high-risk artificial\nintelligence system as required in § 59.1-609._\n\n_\" Machine learning\" means the development of algorithms to build data-derived\nstatistical models that are capable of drawing inferences from previously\nunseen data without explicit human instruction._\n\n_\" Person\" includes any individual, corporation, partnership, association,\ncooperative, limited liability company, trust, joint venture, or any other\nlegal or commercial entity and any successor, representative, agent, agency,\nor instrumentality thereof. \"Person\" does not include any government or\npolitical subdivision._\n\n_\" Principal basis\" means the use of an output of a high-risk artificial\nintelligence system to make a decision without (i) human review, oversight,\ninvolvement, or intervention or (ii) meaningful consideration by a human._\n\n_\" Red-teaming\" means an exercise that is conducted to identify the potential\nadverse behaviors or outcomes of an artificial intelligence system, identify\nhow such behaviors or outcomes occur, and stress test the safeguards against\nsuch behaviors or outcomes._\n\n_\" Significant update\" means any new version, new release, or other update to\na high-risk artificial intelligence system that results in significant changes\nto such high-risk artificial intelligence system's use case or key\nfunctionality and that results in any new or reasonably foreseeable risk of\nalgorithmic discrimination._\n\n_\" Social media platform\" means an electronic medium or service where users\nmay create, share, or view user-generated content, including videos,\nphotographs, blogs, podcasts, messages, emails, or website profiles or\nlocations, and create a personal account._\n\n_\" Substantial factor\" means a factor that is (i) the principal basis for\nmaking a consequential decision, (ii) capable of altering the outcome of a\nconsequential decision, and (iii) generated by an artificial intelligence\nsystem. \"Substantial factor\" includes any use of an artificial intelligence\nsystem to generate any content, decision, prediction, or recommendation\nconcerning a consumer that is used as the principal basis to make a\nconsequential decision concerning the consumer._\n\n_\" Synthetic digital content\" means any digital content, including any audio,\nimage, text, or video, that is produced or manipulated by a generative\nartificial intelligence system, including a general-purpose artificial\nintelligence model._\n\n_\" Trade secret\" means information, including a formula, pattern, compilation,\nprogram, device, method, technique, or process, that (i) derives independent\neconomic value, actual or potential, from not being generally known to, and\nnot being readily ascertainable by proper means by, other persons who can\nobtain economic value from its disclosure or use and (ii) is the subject of\nefforts that are reasonable under the circumstances to maintain its secrecy._\n\n**_§ 59.1-608. Operating standards for developers of high-risk artificial\nintelligence systems._ **\n\n_A. No developer of a high-risk artificial intelligence system shall offer,\nsell, lease, give, or otherwise provide to a deployer, or other developer, a\nhigh-risk artificial intelligence system unless the developer makes available\nto the deployer or other developer:_\n\n_1\\. A statement disclosing the intended uses of such high-risk artificial\nintelligence system;_\n\n_2\\. Documentation disclosing the following:_\n\n_a. The known or reasonably known limitations of such high-risk artificial\nintelligence system, including any and all known or reasonably foreseeable\nrisks of algorithmic discrimination arising from the intended uses of such\nhigh-risk artificial intelligence system;_\n\n_b. The purpose of such high-risk artificial intelligence system and the\nintended benefits and uses of such high-risk artificial intelligence system;_\n\n_c. A summary describing how such high-risk artificial intelligence system was\nevaluated for performance before such high-risk artificial intelligence system\nwas licensed, sold, leased, given, or otherwise made available to a deployer;_\n\n_d. The measures the developer has taken to mitigate reasonable foreseeable\nrisks of algorithmic discrimination that the developer knows arises from\ndeployment or use of such high-risk artificial intelligence system; and_\n\n_e. How an individual can use such high-risk artificial intelligence system\nand monitor the performance of such high-risk artificial intelligence system\nfor any risk of algorithmic discrimination;_\n\n_3\\. Documentation describing (i) how the high-risk artificial intelligence\nsystem was evaluated for performance and for mitigation of algorithmic\ndiscrimination before such system was made available to the deployer; (ii) the\ndata governance measures used to cover the training data sets and the measures\nused to examine the suitability of data sources, possible biases of data\nsources, and appropriate mitigation; (iii) the intended outputs of the high-\nrisk artificial intelligence system; (iv) the measures the developer has taken\nto mitigate known or reasonably foreseeable risks of algorithmic\ndiscrimination that may arise from the reasonably foreseeable deployment of\nthe high-risk artificial intelligence system; and (v) how the high-risk\nartificial intelligence system should be used, not be used, and be monitored\nby an individual when such system is used to make, or is a substantial factor\nin making, a consequential decision; and_\n\n_4\\. Any additional documentation that is reasonably necessary to assist the\ndeployer in understanding the outputs and monitoring performance of the high-\nrisk artificial intelligence system for risks of algorithmic discrimination._\n\n_B. Each developer that offers, sells, leases, gives, or otherwise makes\navailable to a deployer a high-risk artificial intelligence system shall make\navailable to the deployer, to the extent feasible and necessary, information\nand documentation through artifacts such as model cards or impact assessments,\nand such documentation and information shall enable the deployer or a third\nparty contracted by the deployer to complete an impact assessment as required\nin § 59.1-609._\n\n_C. A developer that also serves as a deployer for any high-risk artificial\nintelligence system shall not be required to generate the documentation\nrequired by this section unless such high-risk artificial intelligence system\nis provided to an unaffiliated entity acting as a deployer or as otherwise\nrequired by law._\n\n_D. Nothing in this section shall be construed to require a developer to\ndisclose any trade secret._\n\n_E. High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_F. For any disclosure required pursuant to this section, each developer\nshall, no later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_G. 1. Each developer of a high-risk artificial intelligence system, including\na general purpose artificial intelligence model, that generates or manipulates\nsynthetic digital content shall ensure that the outputs of such high-risk\nartificial intelligence system are marked and detectable, in a manner that is\ndetectable by consumers and complies with any applicable accessibility\nrequirements, as synthetic digital content no later than the time that\nconsumers who did not create such outputs first interact with or are exposed\nto such output;_\n\n_2\\. If such synthetic digital content is an audio, image, or video format\nthat forms part of an evidently artistic, creative, satirical, fictional\nanalogous work or program, such requirement for marking outputs of high-risk\nartificial intelligence systems pursuant to subdivision 1 shall be limited to\na manner that does not hinder the display or enjoyment of such work or\nprogram._\n\n_3\\. The marking of outputs required by subdivision 1 shall not apply to (i)\nsynthetic digital content that consists exclusively of text, is published to\ninform the public on any matter of public interest, or is unlikely to mislead\na reasonable person consuming such synthetic digital content or (ii) the\noutputs of a high-risk artificial intelligence system that performs an\nassistive function for standard editing, does not substantially alter the\ninput data provided by the developer, or is used to detect, prevent,\ninvestigate, or prosecute any crime as authorized by law._\n\n**_§ 59.1-609. Operating standards for deployers of high-risk artificial\nintelligence systems._ **\n\n_A. Each deployer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought on or after such date by the Attorney General pursuant to § 59.1-613,\nthere shall be a rebuttable presumption that a deployer of a high-risk\nartificial intelligence system used a reasonable duty of care as required by\nthis subsection if the deployer complied with the requirements of this\nsection._\n\n_B. No deployer shall deploy or use a high-risk artificial intelligence system\nto make a consequential decision unless the deployer has designed and\nimplemented a risk management policy and program for such high-risk artificial\nintelligence system. The risk management policy shall specify the principles,\nprocesses, and personnel that the deployer shall use in maintaining the risk\nmanagement program to identify, mitigate, and document any risk of algorithmic\ndiscrimination that is a reasonably foreseeable consequence of deploying or\nusing such high-risk artificial intelligence system to make a consequential\ndecision. Each risk management policy and program designed, implemented, and\nmaintained pursuant to this subsection shall be (i) at least as stringent as\nthe latest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems and (ii) reasonable considering (a) the size\nand complexity of the deployer; (b) the nature and scope of the high-risk\nartificial intelligence systems deployed and used by the deployer, including\nthe intended uses of such high-risk artificial intelligence systems; (c) the\nsensitivity and volume of data processed in connection with the high-risk\nartificial intelligence systems deployed and used by the deployer; and (d) the\ncost to the deployer to implement and maintain such risk management program._\n\n_C. Except as provided in this subsection, no deployer shall deploy or use a\nhigh-risk artificial intelligence system to make a consequential decision\nunless the deployer has completed an impact assessment for such high-risk\nartificial intelligence system. The deployer shall complete an impact\nassessment for a high-risk artificial intelligence system (i) before the\ndeployer initially deploys such high-risk artificial intelligence system and\n(ii) not later than 90 days after each significant update to such high-risk\nartificial intelligence system is made available._\n\n_Each impact assessment completed pursuant to this subsection shall include,\nat a minimum:_\n\n_1\\. A statement by the deployer disclosing (i) the purpose, intended use\ncases and deployment context of, and benefits afforded by the high-risk\nartificial intelligence system and (ii) whether the deployment or use of the\nhigh-risk artificial intelligence system poses any known or reasonably\nforeseeable risk of algorithmic discrimination and, if so, (a) the nature of\nsuch algorithmic discrimination and (b) the steps that have been taken, to the\nextent feasible, to mitigate such risk;_\n\n_2\\. For each post-deployment impact assessment completed pursuant to this\nsubsection, whether the intended use cases of the high-risk artificial\nintelligence system as updated were consistent with, or varied from, the\ndeveloper 's intended uses of such high-risk artificial intelligence system;_\n\n_3\\. A description of (i) the categories of data the high-risk artificial\nintelligence system processes as inputs and (ii) the outputs such high-risk\nartificial intelligence system produces;_\n\n_4\\. If the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize such high-risk artificial intelligence system;_\n\n_5\\. A list of any metrics used to evaluate the performance and known\nlimitations of the high-risk artificial intelligence system;_\n\n_6\\. A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that such high-risk artificial intelligence system is in use when\nsuch high-risk artificial intelligence system is in use; and_\n\n_7\\. A description of any post-deployment monitoring performed and user\nsafeguards provided concerning such high-risk artificial intelligence system,\nincluding any oversight process established by the deployer to address issues\narising from deployment or use of such high-risk artificial intelligence\nsystem as such issues arise._\n\n_A single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed or used by a deployer. High-risk\nartificial intelligence systems that are in conformity with the latest version\nof the Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology, Standard ISO/IEC 42001 of the\nInternational Organization for Standardization, or another nationally or\ninternationally recognized risk management framework for artificial\nintelligence systems, or parts thereof, shall be presumed to be in conformity\nwith related requirements set out in this section and in associated\nregulations. If a deployer completes an impact assessment for the purpose of\ncomplying with another applicable law or regulation, such impact assessment\nshall be deemed to satisfy the requirements established in this subsection if\nsuch impact assessment is reasonably similar in scope and effect to the impact\nassessment that would otherwise be completed pursuant to this subsection. A\ndeployer that completes an impact assessment pursuant to this subsection shall\nmaintain such impact assessment and all records concerning such impact\nassessment for three years._\n\n_D. Not later than the time that a deployer uses a high-risk artificial\nintelligence system to interact with a consumer, the deployer shall disclose\nto the consumer that the deployer is interacting with an artificial\nintelligence system disclosing (i) the purpose of such high-risk artificial\nintelligence system, (ii) the nature of such system, (iii) the nature of the\nconsequential decision, (iv) the contact information for the deployer, and (v)\na description of the artificial intelligence system in plain language of such\nsystem._\n\n_If such consequential decision is adverse to such consumer, the deployer\nshall provide to the consumer (a) a statement disclosing the principal reason\nor reasons for the consequential decision, including (1) the degree to which\nand manner in which the high-risk artificial intelligence system contributed\nto the consequential decision, (2) the type of data that was processed by such\nsystem in making the consequential decision, and (3) the sources of such data;\n(b) an opportunity to correct any incorrect personal data that the high-risk\nartificial intelligence system processed in making, or as a substantial factor\nin making, the consequential decision; and (c) an opportunity to appeal such\nadverse consequential decision concerning the consumer arising from the\ndeployment of such system. Any such appeal shall allow for human review, if\ntechnically feasible, unless providing the opportunity for appeal is not in\nthe best interest of the consumer, including instances in which any delay\nmight pose a risk to the life or safety of such consumer._\n\n_E. Each deployer shall make available, in a manner that is clear and readily\navailable, a statement summarizing how such deployer manages any reasonably\nforeseeable risk of algorithmic discrimination that may arise from the use or\ndeployment of the high-risk artificial intelligence system._\n\n_F. For any disclosure required pursuant to this section, each deployer shall,\nno later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_G. Any deployer who performs an intentional and substantial modification to\nany high-risk artificial system shall comply with the documentation and\ndisclosure requirements for developers pursuant to subsections A through F of\n§ 59.1-608._\n\n**_§ 59.1-610. Operating standards for integrators of high-risk artificial\nintelligence systems._ **\n\n_Each integrator of a high-risk artificial intelligence system shall develop\nand adopt an acceptable use policy, which shall limit the use of the high-risk\nartificial intelligence system to mitigate known risks of algorithmic\ndiscrimination._\n\n_Each integrator of a high-risk artificial intelligence system shall provide\nto the deployer clear, conspicuous notice of (i) the name or other identifier\nof the high-risk artificial intelligence system integrated into a software\napplication provided to the deployer; (ii) the name and contact information of\nthe developer of the high-risk artificial intelligence system integrated into\na software application provided to the deployer; (iii) whether the integrator\nhas adjusted the model weights of the high-risk artificial intelligence system\nintegrated into the software application by exposing it to additional data, a\nsummary of the adjustment process, and how such process and the resulting\nsystem were evaluated for risk of algorithmic discrimination; (iv) a summary\nof any other non-substantial modifications made by the integrator; and (v) the\nintegrator 's acceptable use policy._\n\n**_§ 59.1-611. Operating standards for distributors of high-risk artificial\nintelligence systems._ **\n\n_Each distributor of a high-risk artificial intelligence system shall use\nreasonable care to protect consumers from any known or reasonably foreseeable\nrisks of algorithmic discrimination. If a distributor of a high-risk\nartificial intelligence system considers or has reason to consider that a\nhigh-risk artificial intelligence system is not in compliance with any\nrequirement of this chapter, it shall immediately withdraw, disable, or\nrecall, as appropriate, the high-risk artificial intelligence system from the\nmarket until such system has been brought into compliance with the\nrequirements of this chapter. The distributor shall inform the developers of\nthe high-risk artificial intelligence system concerned and, where applicable,\nthe deployer of any such system 's noncompliance with this chapter and the\nwithdrawal, disablement, or recall of such system. _\n\n**_§ 59.1-612. Exemptions._ **\n\n_A. Nothing in this chapter shall be construed to restrict a developer 's,\nintegrator's, distributor's or deployer's ability to (i) comply with federal,\nstate, or municipal ordinances or regulations; (ii) comply with a civil,\ncriminal, or regulatory inquiry, investigation, subpoena, or summons by\nfederal, state, local, or other governmental authorities; (iii) cooperate with\nlaw-enforcement agencies concerning conduct or activity that the developer,\nintegrator, distributor, or deployer reasonably and in good faith believes may\nviolate federal, state, or local law, ordinances, or regulations; (iv)\ninvestigate, establish, exercise, prepare for, or defend legal claims; (v)\nprovide a product or service specifically requested by a consumer; (vi)\nperform under a contract to which a consumer is a party, including fulfilling\nthe terms of a written warranty; (vii) take steps at the request of a consumer\nprior to entering into a contract; (viii) take immediate steps to protect an\ninterest that is essential for the life or physical safety of the consumer or\nanother individual; (ix) prevent, detect, protect against, or respond to\nsecurity incidents, identity theft, fraud, harassment, or malicious or\ndeceptive activities; (x) take actions to prevent, detect, protect against,\nreport, or respond to the production, generation, incorporation, or\nsynthesization of child sex abuse material, or any illegal activity, preserve\nthe integrity or security of systems, or investigate, report, or prosecute\nthose responsible for any such action; (xi) engage in public or peer-reviewed\nscientific or statistical research in the public interest that adheres to all\nother applicable ethics and privacy laws and is approved, monitored, and\ngoverned by an institutional review board that determines, or similar\nindependent oversight entities that determine, (a) that the expected benefits\nof the research outweigh the risks associated with such research and (b)\nwhether the developer, integrator, distributor, or deployer has implemented\nreasonable safeguards to mitigate the risks associated with such research;\n(xii) assist another developer, integrator, distributor, or deployer with any\nof the obligations imposed by this chapter; or (xiii) take any action that is\nin the public interest in the areas of public health, community health, or\npopulation health, but solely to the extent that such action is subject to\nsuitable and specific measures to safeguard the public._\n\n_B. The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall not restrict a developer 's or deployer's\nability to (i) conduct internal research to develop, improve, or repair\nproducts, services, or technologies; (ii) effectuate a product recall; (iii)\nidentify and repair technical errors that impair existing or intended\nfunctionality; or (iv) perform internal operations that are reasonably aligned\nwith the expectations of the consumer or reasonably anticipated based on the\nconsumer's existing relationship with the developer, integrator, or deployer._\n\n_C. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper, integrator, distributor, or deployer to disclose trade secrets or\ninformation protected from disclosure by state or federal law._\n\n_D. The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall not apply where compliance by the developer,\nintegrator, distributor, or deployer with such obligations would violate an\nevidentiary privilege under federal law or the laws of the Commonwealth._\n\n_E. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper, integrator, distributor, or deployer that adversely affects the\nlegally protected rights or freedoms of any person, including the rights of\nany person to freedom of speech or freedom of the press guaranteed in the\nFirst Amendment to the Constitution of the United States or under the Virginia\nHuman Rights Act ( § 2.2-3900 et seq.)._\n\n_F. The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall not apply to any artificial intelligence\nsystem that is acquired by or for the federal government or any federal agency\nor department, including the U.S. Department of Commerce, the U.S. Department\nof Defense, and the National Aeronautics and Space Administration, unless such\nartificial intelligence system is a high-risk artificial intelligence system\nthat is used to make, or is a substantial factor in making, a decision\nconcerning employment or housing._\n\n_G. For the purposes of this subsection:_\n\n_\" Affiliate\" means the same as that term is defined in § 6.2-1800._\n\n_\" Bank\" means the same as that term is defined in § 6.2-800._\n\n_\" Credit union\" means the same as that term is defined in § 6.2-1300._\n\n_\" Federal credit union\" means a credit union duly organized under federal\nlaw._\n\n_\" Out-of-state bank\" means the same as that term is defined in § 6.2-836._\n\n_\" Out-of-state credit union\" means a credit union organized and doing\nbusiness in another state._\n\n_\" Subsidiary\" means the same as that term is defined in § 6.2-700._\n\n_The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall be deemed satisfied for any bank, out-of-state\nbank, credit union, federal credit union, out-of-state credit union, or any\naffiliate or subsidiary thereof if such bank, out-of-state bank, credit union,\nfederal credit union, out-of-state credit union, or affiliate or subsidiary is\nsubject to examination by any state or federal prudential regulator under any\npublished guidance or regulations that apply to the use of high-risk\nartificial intelligence systems and such guidance or regulations (i) impose\nrequirements that are substantially equivalent to, and at least as stringent\nas, the requirements set forth in this chapter, and (ii) at a minimum, require\nsuch bank, out-of-state bank, credit union, federal credit union, out-of-state\ncredit union, or affiliate or subsidiary to (a) regularly audit such bank 's,\nout-of-state bank's, credit union's, federal credit union's, out-of-state\ncredit union's, or affiliate's or subsidiary's use of high-risk artificial\nintelligence systems for compliance with state and federal anti-discrimination\nlaws and regulations applicable to such bank, out-of-state bank, credit union,\nfederal credit union, out-of-state credit union, or affiliate or subsidiary\nand (b) mitigate any algorithmic discrimination caused by the use of a high-\nrisk artificial intelligence system or any risk of algorithmic discrimination\nthat is reasonably foreseeable as a result of the use of a high-risk\nartificial intelligence system._\n\n_H. For purposes of this subsection, \"insurer\" means the same as that term is\ndefined in § 38.2-100._\n\n_The provisions of this chapter shall not apply to any insurer, or any high-\nrisk artificial intelligence system developed or deployed by an insurer for\nuse in the business of insurance, if such insurer is regulated and supervised\nby the State Corporation Commission or a comparable federal regulating body\nand subject to examination by such entity under any existing statutes, rules,\nor regulations pertaining to unfair trade practices and unfair discrimination\nprohibited under Chapter 5 ( § 38.2-500 et seq.) of Title 38.2, or published\nguidance or regulations that apply to the use of high-risk artificial\nintelligence systems and such guidance or regulations aid in the prevention\nand mitigation of algorithmic discrimination caused by the use of a high-risk\nartificial intelligence system or any risk of algorithmic discrimination that\nis reasonably foreseeable as a result of the use of a high-risk artificial\nintelligence system. Nothing in this chapter shall be construed to delegate\nexisting regulatory oversight of the business of insurance to any department\nor agency other than the Bureau of Insurance of the Virginia State Corporation\nCommission._\n\n_I. The provisions of this chapter shall not apply to the development of an\nartificial intelligence system that is used exclusively for research,\ntraining, testing, or other pre-deployment activities performed by active\nparticipants of any sandbox software or sandbox environment established and\nsubject to oversight by a designated agency or other government entity and\nthat is in compliance with the provisions of this chapter._\n\n_J. The provisions of this chapter shall not apply to a developer, integrator,\ndistributor, or deployer, or other person who develops, deploys, puts into\nservice, or intentionally modifies, as applicable, a high-risk artificial\nintelligence system that (i) has been approved, authorized, certified,\ncleared, developed, or granted by a federal agency acting within the scope of\nthe federal agency 's authority, or by a regulated entity subject to the\nsupervision and regulation of the Federal Housing Finance Agency or (ii) is in\ncompliance with standards established by a federal agency or by a regulated\nentity subject to the supervision and regulation of the Federal Housing\nFinance Agency, if the standards are substantially equivalent or more\nstringent than the requirements of this chapter._\n\n_K. The provisions of this chapter shall not apply to a developer, integrator,\ndistributor, deployer, or other person that is a covered entity within the\nmeaning of the federal Health Insurance Portability and Accountability Act of\n1996 (42 U.S.C. § 1320d et seq.) and the regulations promulgated under such\nfederal act, as both may be amended from time to time, and is providing (i)\nhealth care recommendations that (a) are generated by an artificial\nintelligence system and (b) require a health care provider to take action to\nimplement the recommendations or (ii) services utilizing an artificial\nintelligence system for an administrative, financial, quality measurement,\nsecurity, or performance improvement function._\n\n_L. If a developer, integrator, distributor, or deployer engages in any action\nauthorized by an exemption set forth in this section, the developer,\nintegrator, distributor, or deployer bears the burden of demonstrating that\nsuch action qualifies for such exemption._\n\n**_§ 59.1-613. Enforcement; civil penalty._ **\n\n_A. The Attorney General shall have exclusive authority to enforce the\nprovisions of this chapter._\n\n_B. Whenever the Attorney General has reasonable cause to believe that any\nperson has engaged in or is engaging in any violation of this chapter, the\nAttorney General is empowered to issue a civil investigative demand. The\nprovisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative\ndemands issued pursuant to this section. In rendering and furnishing any\ninformation requested pursuant to a civil investigative demand issued pursuant\nto this section, a developer, integrator, distributor, or deployer may redact\nor omit any trade secrets or information protected from disclosure by state or\nfederal law. To the extent that any information requested pursuant to a civil\ninvestigative demand issued pursuant to this section is subject to attorney-\nclient privilege or work-product protection, disclosure of such information\npursuant to the civil investigative demand shall not constitute a waiver of\nsuch privilege or protection. Any information, statement, or documentation\nprovided to the Attorney General pursuant to this section shall be exempt from\ndisclosure under the Virginia Freedom of Information Act (§ 2.2-3700 et\nseq.)._\n\n_C. Notwithstanding any contrary provision of law, the Attorney General may\ncause an action to be brought in the appropriate circuit court in the name of\nthe Commonwealth to enjoin any violation of this chapter. The circuit court\nhaving jurisdiction may enjoin such violation notwithstanding the existence of\nan adequate alternative remedy at law. In any action brought pursuant to this\nchapter, it shall not be necessary that damages be proved._\n\n_D. Any person who violates the provisions of this chapter shall be subject to\na civil penalty in an amount not to exceed $1,000 plus reasonable attorney\nfees, expenses, and costs, as determined by the court. Any person who\nwillfully violates the provisions of this chapter shall be subject to a civil\npenalty in an amount not less than $1,000 and not more than $10,000 plus\nreasonable attorney fees, expenses, and costs, as determined by the court.\nSuch civil penalties shall be paid into the Literary Fund._\n\n_E. Each violation of this chapter shall constitute a separate violation and\nshall be subject to any civil penalties imposed under this section._\n\n_F. The Attorney General may require that a developer disclose to the Attorney\nGeneral any statement or documentation described in this chapter if such\nstatement or documentation is relevant to an investigation conducted by the\nAttorney General. The Attorney General may also require that a deployer\ndisclose to the Attorney General any risk management policy designed and\nimplemented, impact assessment completed, or record maintained pursuant to\nthis chapter if such risk management policy, impact assessment, or record is\nrelevant to an investigation conducted by the Attorney General._\n\n_G. In an action brought by the Attorney General pursuant to this section, it\nshall be an affirmative defense that the developer, integrator, distributor,\nor deployer (i) discovers a violation of any provision of this chapter through\nred-teaming; (ii) no later than 45 days after discovering such violation (a)\ncures such violation and (b) provides notice to the Attorney General in a form\nand manner as prescribed by the Attorney General that such violation has been\ncured and evidence that any harm caused by such violation has been mitigated;\nand (iii) is otherwise in compliance with the requirements of this chapter._\n\n_H. Prior to causing an action against a developer, integrator, distributor,\nor deployer for a violation of this chapter pursuant to subsection C, the\nAttorney General shall determine, in consultation with the developer,\nintegrator, distributor, or deployer, if it is possible to cure the violation.\nIf it is possible to cure such violation, the Attorney General may issue a\nnotice of violation to the developer, integrator, distributor, or deployer and\nafford the developer, integrator, distributor, or deployer the opportunity to\ncure such violation within 45 days of the receipt of such notice of violation.\nIn determining whether to grant such opportunity to cure such violation, the\nAttorney General shall consider (i) the number of violations, (ii) the size\nand complexity of the developer, integrator, distributor, or deployer; (iii)\nthe nature and extent of the developer 's, integrator's, distributor's, or\ndeployer's business; (iv) the substantial likelihood of injury to the public;\n(v) the safety of persons or property; (vi) whether such violation was likely\ncaused by human or technical error. If the developer, integrator, distributor,\nor deployer fails to cure such violation within 45 days of the receipt of such\nnotice of violation, the Attorney General may proceed with such action._\n\n_I. Nothing in this chapter shall create a private cause of action in favor of\nany person aggrieved by a violation of this chapter._\n\n**2\\. That the provisions of this act shall become effective on July 1,\n2026.**\n\n**3\\. That the provisions of this act shall apply only to a violation\ncommitted or a cause of action accruing on or after July 1, 2026.**\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "01/08/2025",
      "label": "Introduced",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:VA2024000H2094&verid=VA2024000H2094_20250108_0_I&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2024 VA H 2094</td> <td><table><tr><td class=\"label\">Author:</td> <td>Maldonado</td></tr> <tr><td class=\"label\">Version:</td> <td>Introduced</td></tr> <tr><td class=\"label\">Version Date:</td> <td>01/08/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">\n    <b>HOUSE BILL NO. 2094</b>\n   </p>\n   <p class=\"center\">Offered January 8, 2025</p>\n   <p class=\"center\">Prefiled January 7, 2025</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"indent\">\n    <i>A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-613, relating to high-risk artificial intelligence; development, deployment, and use; civil penalties.</i>\n   </p>\n   <p class=\"center\">Patron--Maldonado</p>\n   <p class=\"center\">Committee Referral Pending</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">\n     <b>Be it enacted by the General Assembly of Virginia:</b>\n    </p>\n   </span>\n   <p class=\"indent\">\n    <b>1. That the Code of Virginia is amended by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-613, as follows:</b>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">CHAPTER 58.</u>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-607. Definitions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">As used in this chapter, unless the context requires a different meaning:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Algorithmic discrimination&quot; means the use of an artificial intelligence system that results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, sexual orientation, veteran status, or other classification protected under state or federal law. &quot;Algorithmic discrimination&quot; does not include (i) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; (ii) the expansion of an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or (iii) an act or omission by or on behalf of a private club or other establishment not in fact open to the public, as set forth in Title II of the Civil Rights Act of 1964, 42 U.S.C. &sect; 2000a(e), as amended from time to time.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Artificial intelligence system&quot; means any machine learning-based system that, for any explicit or implicit objective, infers from the inputs such system receives how to generate outputs, including content, decisions, predictions, and recommendations, that can influence physical or virtual environments.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consequential decision&quot; means any decision that has a material legal, or similarly significant, effect on the provision or denial to any consumer of, or the cost or terms of, (i) parole, probation, a pardon, or any other release from incarceration or supervision, (ii) education enrollment or an education opportunity, (iii) employment, (iv) a financial or lending service, (v) health care services, (vi) housing, (vii) insurance, or (viii) a legal service.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consumer&quot; means a natural person who is a resident of the Commonwealth and is acting only in an individual or household context. &quot;Consumer&quot; does not include a natural person acting in a commercial or employment context.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Deployer&quot; means any person doing business in the Commonwealth that deploys or uses a high-risk artificial intelligence system to make a consequential decision in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Developer&quot; means any person doing business in the Commonwealth that develops or intentionally and substantially modifies a high-risk artificial intelligence system that is offered, sold, leased, given, or otherwise provided to consumers in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Distributor&quot; means a person doing business in the Commonwealth, other than a developer, that makes an artificial intelligence system available in the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Foundation model&quot; means a machine learning model that (i) is trained on broad data at scale, (ii) is designed for generality of output, and (iii) can be adapted to a wide range of distinctive tasks.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;General-purpose artificial intelligence model&quot; means any form of artificial intelligence system that (i) displays significant generality, (ii) is capable of competently performing a wide range of distinct tasks, and (iii) can be integrated into a variety of downstream applications or systems. &quot;General-purpose artificial intelligence model&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is released on the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence&quot; means artificial intelligence capable of emulating the structure and characteristics of input data in order to generate derived synthetic content, including audio, images, text, and videos.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence system&quot; means any artificial intelligence system or service that incorporates generative artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that is specifically intended to autonomously make, or be a substantial factor in making, a consequential decision. A system or service is not a &quot;high-risk artificial intelligence system&quot; if it is intended to (i) perform a narrow procedural task, (ii) improve the result of a previously completed human activity, (iii) detect any decision-making patterns or any deviations from pre-existing decision-making patterns, or (iv) perform a preparatory task to an assessment relevant to a consequential decision. &quot;High-risk artificial intelligence system&quot; does not include any of the following technologies:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. Anti-fraud technology that does not use facial recognition technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Anti-malware technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Anti-virus technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Artificial intelligence-enabled video games;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. Calculators;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. Cybersecurity technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. Databases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. Data storage;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. Firewall technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. Internet domain registration;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. Internet website loading;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. Networking;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. Spam and robocall filtering;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. Spell-checking technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. Spreadsheets;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">16. Web caching;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">17. Web hosting or any similar technology; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">18. Technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an accepted use policy that prohibits generating content that is discriminatory or harmful.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Integrator&quot; means a person that knowingly integrates an artificial intelligence system into a software application and places such software application on the market. An &quot;integrator&quot; does not include a person offering information technology infrastructure.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Intentional and substantial modification&quot; means any deliberate change made to (i) an artificial intelligence system that results in any new reasonably foreseeable risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects compliance of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. &quot;Intentional and substantial modification&quot; does not include any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (a) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (b) such change (1) is made to such high-risk artificial intelligence system as a result of any learning described in clause (a), and (2) was predetermined by the deployer or the third party contracted by the deployer and concluded and included within the initial impact assessment of such high-risk artificial intelligence system as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Machine learning&quot; means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Person&quot; includes any individual, corporation, partnership, association, cooperative, limited liability company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. &quot;Person&quot; does not include any government or political subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Principal basis&quot; means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Red-teaming&quot; means an exercise that is conducted to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Significant update&quot; means any new version, new release, or other update to a high-risk artificial intelligence system that results in significant changes to such high-risk artificial intelligence system&#39;s use case or key functionality and that results in any new or reasonably foreseeable risk of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Social media platform&quot; means an electronic medium or service where users may create, share, or view user-generated content, including videos, photographs, blogs, podcasts, messages, emails, or website profiles or locations, and create a personal account.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Substantial factor&quot; means a factor that is (i) the principal basis for making a consequential decision, (ii) capable of altering the outcome of a consequential decision, and (iii) generated by an artificial intelligence system. &quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as the principal basis to make a consequential decision concerning the consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Synthetic digital content&quot; means any digital content, including any audio, image, text, or video, that is produced or manipulated by a generative artificial intelligence system, including a general-purpose artificial intelligence model.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique, or process, that (i) derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by, other persons who can obtain economic value from its disclosure or use and (ii) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-608. Operating standards for developers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. No developer of a high-risk artificial intelligence system shall offer, sell, lease, give, or otherwise provide to a deployer, or other developer, a high-risk artificial intelligence system unless the developer makes available to the deployer or other developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement disclosing the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Documentation disclosing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">a. The known or reasonably known limitations of such high-risk artificial intelligence system, including any and all known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">b. The purpose of such high-risk artificial intelligence system and the intended benefits and uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">c. A summary describing how such high-risk artificial intelligence system was evaluated for performance before such high-risk artificial intelligence system was licensed, sold, leased, given, or otherwise made available to a deployer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">d. The measures the developer has taken to mitigate reasonable foreseeable risks of algorithmic discrimination that the developer knows arises from deployment or use of such high-risk artificial intelligence system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">e. How an individual can use such high-risk artificial intelligence system and monitor the performance of such high-risk artificial intelligence system for any risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Documentation describing (i) how the high-risk artificial intelligence system was evaluated for performance and for mitigation of algorithmic discrimination before such system was made available to the deployer; (ii) the data governance measures used to cover the training data sets and the measures used to examine the suitability of data sources, possible biases of data sources, and appropriate mitigation; (iii) the intended outputs of the high-risk artificial intelligence system; (iv) the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and (v) how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when such system is used to make, or is a substantial factor in making, a consequential decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any additional documentation that is reasonably necessary to assist the deployer in understanding the outputs and monitoring performance of the high-risk artificial intelligence system for risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Each developer that offers, sells, leases, gives, or otherwise makes available to a deployer a high-risk artificial intelligence system shall make available to the deployer, to the extent feasible and necessary, information and documentation through artifacts such as model cards or impact assessments, and such documentation and information shall enable the deployer or a third party contracted by the deployer to complete an impact assessment as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. A developer that also serves as a deployer for any high-risk artificial intelligence system shall not be required to generate the documentation required by this section unless such high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer or as otherwise required by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Nothing in this section shall be construed to require a developer to disclose any trade secret.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each developer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. 1. Each developer of a high-risk artificial intelligence system, including a general purpose artificial intelligence model, that generates or manipulates synthetic digital content shall ensure that the outputs of such high-risk artificial intelligence system are marked and detectable, in a manner that is detectable by consumers and complies with any applicable accessibility requirements, as synthetic digital content no later than the time that consumers who did not create such outputs first interact with or are exposed to such output;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. If such synthetic digital content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for marking outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The marking of outputs required by subdivision 1 shall not apply to (i) synthetic digital content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic digital content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-609. Operating standards for deployers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each deployer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought on or after such date by the Attorney General pursuant to &sect; 59.1-613, there shall be a rebuttable presumption that a deployer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the deployer complied with the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has designed and implemented a risk management policy and program for such high-risk artificial intelligence system. The risk management policy shall specify the principles, processes, and personnel that the deployer shall use in maintaining the risk management program to identify, mitigate, and document any risk of algorithmic discrimination that is a reasonably foreseeable consequence of deploying or using such high-risk artificial intelligence system to make a consequential decision. Each risk management policy and program designed, implemented, and maintained pursuant to this subsection shall be (i) at least as stringent as the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems and (ii) reasonable considering (a) the size and complexity of the deployer; (b) the nature and scope of the high-risk artificial intelligence systems deployed and used by the deployer, including the intended uses of such high-risk artificial intelligence systems; (c) the sensitivity and volume of data processed in connection with the high-risk artificial intelligence systems deployed and used by the deployer; and (d) the cost to the deployer to implement and maintain such risk management program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Except as provided in this subsection, no deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has completed an impact assessment for such high-risk artificial intelligence system. The deployer shall complete an impact assessment for a high-risk artificial intelligence system (i) before the deployer initially deploys such high-risk artificial intelligence system and (ii) not later than 90 days after each significant update to such high-risk artificial intelligence system is made available.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each impact assessment completed pursuant to this subsection shall include, at a minimum:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement by the deployer disclosing (i) the purpose, intended use cases and deployment context of, and benefits afforded by the high-risk artificial intelligence system and (ii) whether the deployment or use of the high-risk artificial intelligence system poses any known or reasonably foreseeable risk of algorithmic discrimination and, if so, (a) the nature of such algorithmic discrimination and (b) the steps that have been taken, to the extent feasible, to mitigate such risk;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. For each post-deployment impact assessment completed pursuant to this subsection, whether the intended use cases of the high-risk artificial intelligence system as updated were consistent with, or varied from, the developer&#39;s intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A description of (i) the categories of data the high-risk artificial intelligence system processes as inputs and (ii) the outputs such high-risk artificial intelligence system produces;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. If the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. A list of any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. A description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that such high-risk artificial intelligence system is in use when such high-risk artificial intelligence system is in use; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. A description of any post-deployment monitoring performed and user safeguards provided concerning such high-risk artificial intelligence system, including any oversight process established by the deployer to address issues arising from deployment or use of such high-risk artificial intelligence system as such issues arise.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed or used by a deployer. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations. If a deployer completes an impact assessment for the purpose of complying with another applicable law or regulation, such impact assessment shall be deemed to satisfy the requirements established in this subsection if such impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection. A deployer that completes an impact assessment pursuant to this subsection shall maintain such impact assessment and all records concerning such impact assessment for three years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Not later than the time that a deployer uses a high-risk artificial intelligence system to interact with a consumer, the deployer shall disclose to the consumer that the deployer is interacting with an artificial intelligence system disclosing (i) the purpose of such high-risk artificial intelligence system, (ii) the nature of such system, (iii) the nature of the consequential decision, (iv) the contact information for the deployer, and (v) a description of the artificial intelligence system in plain language of such system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">If such consequential decision is adverse to such consumer, the deployer shall provide to the consumer (a) a statement disclosing the principal reason or reasons for the consequential decision, including (1) the degree to which and manner in which the high-risk artificial intelligence system contributed to the consequential decision, (2) the type of data that was processed by such system in making the consequential decision, and (3) the sources of such data; (b) an opportunity to correct any incorrect personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and (c) an opportunity to appeal such adverse consequential decision concerning the consumer arising from the deployment of such system. Any such appeal shall allow for human review, if technically feasible, unless providing the opportunity for appeal is not in the best interest of the consumer, including instances in which any delay might pose a risk to the life or safety of such consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each deployer shall make available, in a manner that is clear and readily available, a statement summarizing how such deployer manages any reasonably foreseeable risk of algorithmic discrimination that may arise from the use or deployment of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each deployer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. Any deployer who performs an intentional and substantial modification to any high-risk artificial system shall comply with the documentation and disclosure requirements for developers pursuant to subsections A through F of &sect; 59.1-608.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-610. Operating standards for integrators of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each integrator of a high-risk artificial intelligence system shall develop and adopt an acceptable use policy, which shall limit the use of the high-risk artificial intelligence system to mitigate known risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each integrator of a high-risk artificial intelligence system shall provide to the deployer clear, conspicuous notice of (i) the name or other identifier of the high-risk artificial intelligence system integrated into a software application provided to the deployer; (ii) the name and contact information of the developer of the high-risk artificial intelligence system integrated into a software application provided to the deployer; (iii) whether the integrator has adjusted the model weights of the high-risk artificial intelligence system integrated into the software application by exposing it to additional data, a summary of the adjustment process, and how such process and the resulting system were evaluated for risk of algorithmic discrimination; (iv) a summary of any other non-substantial modifications made by the integrator; and (v) the integrator&#39;s acceptable use policy.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-611. Operating standards for distributors of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each distributor of a high-risk artificial intelligence system shall use reasonable care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. If a distributor of a high-risk artificial intelligence system considers or has reason to consider that a high-risk artificial intelligence system is not in compliance with any requirement of this chapter, it shall immediately withdraw, disable, or recall, as appropriate, the high-risk artificial intelligence system from the market until such system has been brought into compliance with the requirements of this chapter. The distributor shall inform the developers of the high-risk artificial intelligence system concerned and, where applicable, the deployer of any such system&#39;s noncompliance with this chapter and the withdrawal, disablement, or recall of such system. </u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-612. Exemptions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Nothing in this chapter shall be construed to restrict a developer&#39;s, integrator&#39;s, distributor&#39;s or deployer&#39;s ability to (i) comply with federal, state, or municipal ordinances or regulations; (ii) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by federal, state, local, or other governmental authorities; (iii) cooperate with law-enforcement agencies concerning conduct or activity that the developer, integrator, distributor, or deployer reasonably and in good faith believes may violate federal, state, or local law, ordinances, or regulations; (iv) investigate, establish, exercise, prepare for, or defend legal claims; (v) provide a product or service specifically requested by a consumer; (vi) perform under a contract to which a consumer is a party, including fulfilling the terms of a written warranty; (vii) take steps at the request of a consumer prior to entering into a contract; (viii) take immediate steps to protect an interest that is essential for the life or physical safety of the consumer or another individual; (ix) prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, or malicious or deceptive activities; (x) take actions to prevent, detect, protect against, report, or respond to the production, generation, incorporation, or synthesization of child sex abuse material, or any illegal activity, preserve the integrity or security of systems, or investigate, report, or prosecute those responsible for any such action; (xi) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is approved, monitored, and governed by an institutional review board that determines, or similar independent oversight entities that determine, (a) that the expected benefits of the research outweigh the risks associated with such research and (b) whether the developer, integrator, distributor, or deployer has implemented reasonable safeguards to mitigate the risks associated with such research; (xii) assist another developer, integrator, distributor, or deployer with any of the obligations imposed by this chapter; or (xiii) take any action that is in the public interest in the areas of public health, community health, or population health, but solely to the extent that such action is subject to suitable and specific measures to safeguard the public.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall not restrict a developer&#39;s or deployer&#39;s ability to (i) conduct internal research to develop, improve, or repair products, services, or technologies; (ii) effectuate a product recall; (iii) identify and repair technical errors that impair existing or intended functionality; or (iv) perform internal operations that are reasonably aligned with the expectations of the consumer or reasonably anticipated based on the consumer&#39;s existing relationship with the developer, integrator, or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Nothing in this chapter shall be construed to impose any obligation on a developer, integrator, distributor, or deployer to disclose trade secrets or information protected from disclosure by state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall not apply where compliance by the developer, integrator, distributor, or deployer with such obligations would violate an evidentiary privilege under federal law or the laws of the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this chapter shall be construed to impose any obligation on a developer, integrator, distributor, or deployer that adversely affects the legally protected rights or freedoms of any person, including the rights of any person to freedom of speech or freedom of the press guaranteed in the First Amendment to the Constitution of the United States or under the Virginia Human Rights Act (&sect; 2.2-3900 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall not apply to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the U.S. Department of Commerce, the U.S. Department of Defense, and the National Aeronautics and Space Administration, unless such artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For the purposes of this subsection:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Affiliate&quot; means the same as that term is defined in &sect; 6.2-1800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Bank&quot; means the same as that term is defined in &sect; 6.2-800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Credit union&quot; means the same as that term is defined in &sect; 6.2-1300.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Federal credit union&quot; means a credit union duly organized under federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state bank&quot; means the same as that term is defined in &sect; 6.2-836.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state credit union&quot; means a credit union organized and doing business in another state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Subsidiary&quot; means the same as that term is defined in &sect; 6.2-700.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The obligations imposed on developers, integrators, distributors, or deployers by this chapter shall be deemed satisfied for any bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or any affiliate or subsidiary thereof if such bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or affiliate or subsidiary is subject to examination by any state or federal prudential regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations (i) impose requirements that are substantially equivalent to, and at least as stringent as, the requirements set forth in this chapter, and (ii) at a minimum, require such bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or affiliate or subsidiary to (a) regularly audit such bank&#39;s, out-of-state bank&#39;s, credit union&#39;s, federal credit union&#39;s, out-of-state credit union&#39;s, or affiliate&#39;s or subsidiary&#39;s use of high-risk artificial intelligence systems for compliance with state and federal anti-discrimination laws and regulations applicable to such bank, out-of-state bank, credit union, federal credit union, out-of-state credit union, or affiliate or subsidiary and (b) mitigate any algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. For purposes of this subsection, &quot;insurer&quot; means the same as that term is defined in &sect; 38.2-100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The provisions of this chapter shall not apply to any insurer, or any high-risk artificial intelligence system developed or deployed by an insurer for use in the business of insurance, if such insurer is regulated and supervised by the State Corporation Commission or a comparable federal regulating body and subject to examination by such entity under any existing statutes, rules, or regulations pertaining to unfair trade practices and unfair discrimination prohibited under Chapter 5 (&sect; 38.2-500 et seq.) of Title 38.2, or published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations aid in the prevention and mitigation of algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. Nothing in this chapter shall be construed to delegate existing regulatory oversight of the business of insurance to any department or agency other than the Bureau of Insurance of the Virginia State Corporation Commission.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. The provisions of this chapter shall not apply to the development of an artificial intelligence system that is used exclusively for research, training, testing, or other pre-deployment activities performed by active participants of any sandbox software or sandbox environment established and subject to oversight by a designated agency or other government entity and that is in compliance with the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">J. The provisions of this chapter shall not apply to a developer, integrator, distributor, or deployer, or other person who develops, deploys, puts into service, or intentionally modifies, as applicable, a high-risk artificial intelligence system that (i) has been approved, authorized, certified, cleared, developed, or granted by a federal agency acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency or (ii) is in compliance with standards established by a federal agency or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">K. The provisions of this chapter shall not apply to a developer, integrator, distributor, deployer, or other person that is a covered entity within the meaning of the federal Health Insurance Portability and Accountability Act of 1996 (42 U.S.C. &sect; 1320d et seq.) and the regulations promulgated under such federal act, as both may be amended from time to time, and is providing (i) health care recommendations that (a) are generated by an artificial intelligence system and (b) require a health care provider to take action to implement the recommendations or (ii) services utilizing an artificial intelligence system for an administrative, financial, quality measurement, security, or performance improvement function.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">L. If a developer, integrator, distributor, or deployer engages in any action authorized by an exemption set forth in this section, the developer, integrator, distributor, or deployer bears the burden of demonstrating that such action qualifies for such exemption.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-613. Enforcement; civil penalty.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. The Attorney General shall have exclusive authority to enforce the provisions of this chapter. </u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any violation of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of &sect; 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer, integrator, distributor, or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand shall not constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be exempt from disclosure under the Virginia Freedom of Information Act (&sect; 2.2-3700 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any violation of this chapter. The circuit court having jurisdiction may enjoin such violation notwithstanding the existence of an adequate alternative remedy at law. In any action brought pursuant to this chapter, it shall not be necessary that damages be proved.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Any person who violates the provisions of this chapter shall be subject to a civil penalty in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil penalty in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil penalties shall be paid into the Literary Fund.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each violation of this chapter shall constitute a separate violation and shall be subject to any civil penalties imposed under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The Attorney General may require that a developer disclose to the Attorney General any statement or documentation described in this chapter if such statement or documentation is relevant to an investigation conducted by the Attorney General. The Attorney General may also require that a deployer disclose to the Attorney General any risk management policy designed and implemented, impact assessment completed, or record maintained pursuant to this chapter if such risk management policy, impact assessment, or record is relevant to an investigation conducted by the Attorney General.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. In an action brought by the Attorney General pursuant to this section, it shall be an affirmative defense that the developer, integrator, distributor, or deployer (i) discovers a violation of any provision of this chapter through red-teaming; (ii) no later than 45 days after discovering such violation (a) cures such violation and (b) provides notice to the Attorney General in a form and manner as prescribed by the Attorney General that such violation has been cured and evidence that any harm caused by such violation has been mitigated; and (iii) is otherwise in compliance with the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Prior to causing an action against a developer, integrator, distributor, or deployer for a violation of this chapter pursuant to subsection C, the Attorney General shall determine, in consultation with the developer, integrator, distributor, or deployer, if it is possible to cure the violation. If it is possible to cure such violation, the Attorney General may issue a notice of violation to the developer, integrator, distributor, or deployer and afford the developer, integrator, distributor, or deployer the opportunity to cure such violation within 45 days of the receipt of such notice of violation. In determining whether to grant such opportunity to cure such violation, the Attorney General shall consider (i) the number of violations, (ii) the size and complexity of the developer, integrator, distributor, or deployer; (iii) the nature and extent of the developer&#39;s, integrator&#39;s, distributor&#39;s, or deployer&#39;s business; (iv) the substantial likelihood of injury to the public; (v) the safety of persons or property; (vi) whether such violation was likely caused by human or technical error. If the developer, integrator, distributor, or deployer fails to cure such violation within 45 days of the receipt of such notice of violation, the Attorney General may proceed with such action.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Nothing in this chapter shall create a private cause of action in favor of any person aggrieved by a violation of this chapter.</u>\n   </p>\n   <effective_clause>\n    <p class=\"indent\">\n     <b>2. That the provisions of this act shall become effective on July 1, 2026.</b>\n    </p>\n   </effective_clause>\n   <p class=\"indent\">\n    <b>3. That the provisions of this act shall apply only to a violation committed or a cause of action accruing on or after July 1, 2026.</b>\n   </p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2024 VA H 2094 | | Author: | Maldonado  \n---|---  \nVersion: | Introduced  \nVersion Date: | 01/08/2025  \n  \n**HOUSE BILL NO. 2094**\n\nOffered January 8, 2025\n\nPrefiled January 7, 2025\n\n_A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-613,\nrelating to high-risk artificial intelligence; development, deployment, and\nuse; civil penalties._\n\nPatron--Maldonado\n\nCommittee Referral Pending\n\n**Be it enacted by the General Assembly of Virginia:**\n\n**1\\. That the Code of Virginia is amended by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-613, as\nfollows:**\n\n_CHAPTER 58._\n\n_HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT._\n\n**_§ 59.1-607. Definitions._ **\n\n_As used in this chapter, unless the context requires a different meaning:_\n\n_\" Algorithmic discrimination\" means the use of an artificial intelligence\nsystem that results in an unlawful differential treatment or impact that\ndisfavors an individual or group of individuals on the basis of their actual\nor perceived age, color, disability, ethnicity, genetic information, limited\nproficiency in the English language, national origin, race, religion,\nreproductive health, sex, sexual orientation, veteran status, or other\nclassification protected under state or federal law. \"Algorithmic\ndiscrimination\" does not include (i) the offer, license, or use of a high-risk\nartificial intelligence system by a developer or deployer for the sole purpose\nof the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; (ii) the expansion of an applicant, customer, or participant pool to\nincrease diversity or redress historical discrimination; or (iii) an act or\nomission by or on behalf of a private club or other establishment not in fact\nopen to the public, as set forth in Title II of the Civil Rights Act of 1964,\n42 U.S.C. § 2000a(e), as amended from time to time._\n\n_\" Artificial intelligence system\" means any machine learning-based system\nthat, for any explicit or implicit objective, infers from the inputs such\nsystem receives how to generate outputs, including content, decisions,\npredictions, and recommendations, that can influence physical or virtual\nenvironments._\n\n_\" Consequential decision\" means any decision that has a material legal, or\nsimilarly significant, effect on the provision or denial to any consumer of,\nor the cost or terms of, (i) parole, probation, a pardon, or any other release\nfrom incarceration or supervision, (ii) education enrollment or an education\nopportunity, (iii) employment, (iv) a financial or lending service, (v) health\ncare services, (vi) housing, (vii) insurance, or (viii) a legal service._\n\n_\" Consumer\" means a natural person who is a resident of the Commonwealth and\nis acting only in an individual or household context. \"Consumer\" does not\ninclude a natural person acting in a commercial or employment context._\n\n_\" Deployer\" means any person doing business in the Commonwealth that deploys\nor uses a high-risk artificial intelligence system to make a consequential\ndecision in the Commonwealth._\n\n_\" Developer\" means any person doing business in the Commonwealth that\ndevelops or intentionally and substantially modifies a high-risk artificial\nintelligence system that is offered, sold, leased, given, or otherwise\nprovided to consumers in the Commonwealth._\n\n_\" Distributor\" means a person doing business in the Commonwealth, other than\na developer, that makes an artificial intelligence system available in the\nmarket._\n\n_\" Foundation model\" means a machine learning model that (i) is trained on\nbroad data at scale, (ii) is designed for generality of output, and (iii) can\nbe adapted to a wide range of distinctive tasks._\n\n_\" General-purpose artificial intelligence model\" means any form of artificial\nintelligence system that (i) displays significant generality, (ii) is capable\nof competently performing a wide range of distinct tasks, and (iii) can be\nintegrated into a variety of downstream applications or systems. \"General-\npurpose artificial intelligence model\" does not include any artificial\nintelligence model that is used for development, prototyping, and research\nactivities before such artificial intelligence model is released on the\nmarket._\n\n_\" Generative artificial intelligence\" means artificial intelligence capable\nof emulating the structure and characteristics of input data in order to\ngenerate derived synthetic content, including audio, images, text, and\nvideos._\n\n_\" Generative artificial intelligence system\" means any artificial\nintelligence system or service that incorporates generative artificial\nintelligence._\n\n_\" High-risk artificial intelligence system\" means any artificial intelligence\nsystem that is specifically intended to autonomously make, or be a substantial\nfactor in making, a consequential decision. A system or service is not a\n\"high-risk artificial intelligence system\" if it is intended to (i) perform a\nnarrow procedural task, (ii) improve the result of a previously completed\nhuman activity, (iii) detect any decision-making patterns or any deviations\nfrom pre-existing decision-making patterns, or (iv) perform a preparatory task\nto an assessment relevant to a consequential decision. \"High-risk artificial\nintelligence system\" does not include any of the following technologies:_\n\n_1\\. Anti-fraud technology that does not use facial recognition technology;_\n\n_2\\. Anti-malware technology;_\n\n_3\\. Anti-virus technology;_\n\n_4\\. Artificial intelligence-enabled video games;_\n\n_5\\. Calculators;_\n\n_6\\. Cybersecurity technology;_\n\n_7\\. Databases;_\n\n_8\\. Data storage;_\n\n_9\\. Firewall technology;_\n\n_10\\. Internet domain registration;_\n\n_11\\. Internet website loading;_\n\n_12\\. Networking;_\n\n_13\\. Spam and robocall filtering;_\n\n_14\\. Spell-checking technology;_\n\n_15\\. Spreadsheets;_\n\n_16\\. Web caching;_\n\n_17\\. Web hosting or any similar technology; or_\n\n_18\\. Technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an accepted use\npolicy that prohibits generating content that is discriminatory or harmful._\n\n_\" Integrator\" means a person that knowingly integrates an artificial\nintelligence system into a software application and places such software\napplication on the market. An \"integrator\" does not include a person offering\ninformation technology infrastructure._\n\n_\" Intentional and substantial modification\" means any deliberate change made\nto (i) an artificial intelligence system that results in any new reasonably\nforeseeable risk of algorithmic discrimination or (ii) a general-purpose\nartificial intelligence model that affects compliance of the general-purpose\nartificial intelligence model, materially changes the purpose of the general-\npurpose artificial intelligence model, or results in any new reasonably\nforeseeable risk of algorithmic discrimination. \"Intentional and substantial\nmodification\" does not include any change made to a high-risk artificial\nintelligence system, or the performance of a high-risk artificial intelligence\nsystem, if (a) the high-risk artificial intelligence system continues to learn\nafter such high-risk artificial intelligence system is offered, sold, leased,\nlicensed, given, or otherwise made available to a deployer, or deployed, and\n(b) such change (1) is made to such high-risk artificial intelligence system\nas a result of any learning described in clause (a), and (2) was predetermined\nby the deployer or the third party contracted by the deployer and concluded\nand included within the initial impact assessment of such high-risk artificial\nintelligence system as required in § 59.1-609._\n\n_\" Machine learning\" means the development of algorithms to build data-derived\nstatistical models that are capable of drawing inferences from previously\nunseen data without explicit human instruction._\n\n_\" Person\" includes any individual, corporation, partnership, association,\ncooperative, limited liability company, trust, joint venture, or any other\nlegal or commercial entity and any successor, representative, agent, agency,\nor instrumentality thereof. \"Person\" does not include any government or\npolitical subdivision._\n\n_\" Principal basis\" means the use of an output of a high-risk artificial\nintelligence system to make a decision without (i) human review, oversight,\ninvolvement, or intervention or (ii) meaningful consideration by a human._\n\n_\" Red-teaming\" means an exercise that is conducted to identify the potential\nadverse behaviors or outcomes of an artificial intelligence system, identify\nhow such behaviors or outcomes occur, and stress test the safeguards against\nsuch behaviors or outcomes._\n\n_\" Significant update\" means any new version, new release, or other update to\na high-risk artificial intelligence system that results in significant changes\nto such high-risk artificial intelligence system's use case or key\nfunctionality and that results in any new or reasonably foreseeable risk of\nalgorithmic discrimination._\n\n_\" Social media platform\" means an electronic medium or service where users\nmay create, share, or view user-generated content, including videos,\nphotographs, blogs, podcasts, messages, emails, or website profiles or\nlocations, and create a personal account._\n\n_\" Substantial factor\" means a factor that is (i) the principal basis for\nmaking a consequential decision, (ii) capable of altering the outcome of a\nconsequential decision, and (iii) generated by an artificial intelligence\nsystem. \"Substantial factor\" includes any use of an artificial intelligence\nsystem to generate any content, decision, prediction, or recommendation\nconcerning a consumer that is used as the principal basis to make a\nconsequential decision concerning the consumer._\n\n_\" Synthetic digital content\" means any digital content, including any audio,\nimage, text, or video, that is produced or manipulated by a generative\nartificial intelligence system, including a general-purpose artificial\nintelligence model._\n\n_\" Trade secret\" means information, including a formula, pattern, compilation,\nprogram, device, method, technique, or process, that (i) derives independent\neconomic value, actual or potential, from not being generally known to, and\nnot being readily ascertainable by proper means by, other persons who can\nobtain economic value from its disclosure or use and (ii) is the subject of\nefforts that are reasonable under the circumstances to maintain its secrecy._\n\n**_§ 59.1-608. Operating standards for developers of high-risk artificial\nintelligence systems._ **\n\n_A. No developer of a high-risk artificial intelligence system shall offer,\nsell, lease, give, or otherwise provide to a deployer, or other developer, a\nhigh-risk artificial intelligence system unless the developer makes available\nto the deployer or other developer:_\n\n_1\\. A statement disclosing the intended uses of such high-risk artificial\nintelligence system;_\n\n_2\\. Documentation disclosing the following:_\n\n_a. The known or reasonably known limitations of such high-risk artificial\nintelligence system, including any and all known or reasonably foreseeable\nrisks of algorithmic discrimination arising from the intended uses of such\nhigh-risk artificial intelligence system;_\n\n_b. The purpose of such high-risk artificial intelligence system and the\nintended benefits and uses of such high-risk artificial intelligence system;_\n\n_c. A summary describing how such high-risk artificial intelligence system was\nevaluated for performance before such high-risk artificial intelligence system\nwas licensed, sold, leased, given, or otherwise made available to a deployer;_\n\n_d. The measures the developer has taken to mitigate reasonable foreseeable\nrisks of algorithmic discrimination that the developer knows arises from\ndeployment or use of such high-risk artificial intelligence system; and_\n\n_e. How an individual can use such high-risk artificial intelligence system\nand monitor the performance of such high-risk artificial intelligence system\nfor any risk of algorithmic discrimination;_\n\n_3\\. Documentation describing (i) how the high-risk artificial intelligence\nsystem was evaluated for performance and for mitigation of algorithmic\ndiscrimination before such system was made available to the deployer; (ii) the\ndata governance measures used to cover the training data sets and the measures\nused to examine the suitability of data sources, possible biases of data\nsources, and appropriate mitigation; (iii) the intended outputs of the high-\nrisk artificial intelligence system; (iv) the measures the developer has taken\nto mitigate known or reasonably foreseeable risks of algorithmic\ndiscrimination that may arise from the reasonably foreseeable deployment of\nthe high-risk artificial intelligence system; and (v) how the high-risk\nartificial intelligence system should be used, not be used, and be monitored\nby an individual when such system is used to make, or is a substantial factor\nin making, a consequential decision; and_\n\n_4\\. Any additional documentation that is reasonably necessary to assist the\ndeployer in understanding the outputs and monitoring performance of the high-\nrisk artificial intelligence system for risks of algorithmic discrimination._\n\n_B. Each developer that offers, sells, leases, gives, or otherwise makes\navailable to a deployer a high-risk artificial intelligence system shall make\navailable to the deployer, to the extent feasible and necessary, information\nand documentation through artifacts such as model cards or impact assessments,\nand such documentation and information shall enable the deployer or a third\nparty contracted by the deployer to complete an impact assessment as required\nin § 59.1-609._\n\n_C. A developer that also serves as a deployer for any high-risk artificial\nintelligence system shall not be required to generate the documentation\nrequired by this section unless such high-risk artificial intelligence system\nis provided to an unaffiliated entity acting as a deployer or as otherwise\nrequired by law._\n\n_D. Nothing in this section shall be construed to require a developer to\ndisclose any trade secret._\n\n_E. High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_F. For any disclosure required pursuant to this section, each developer\nshall, no later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_G. 1. Each developer of a high-risk artificial intelligence system, including\na general purpose artificial intelligence model, that generates or manipulates\nsynthetic digital content shall ensure that the outputs of such high-risk\nartificial intelligence system are marked and detectable, in a manner that is\ndetectable by consumers and complies with any applicable accessibility\nrequirements, as synthetic digital content no later than the time that\nconsumers who did not create such outputs first interact with or are exposed\nto such output;_\n\n_2\\. If such synthetic digital content is an audio, image, or video format\nthat forms part of an evidently artistic, creative, satirical, fictional\nanalogous work or program, such requirement for marking outputs of high-risk\nartificial intelligence systems pursuant to subdivision 1 shall be limited to\na manner that does not hinder the display or enjoyment of such work or\nprogram._\n\n_3\\. The marking of outputs required by subdivision 1 shall not apply to (i)\nsynthetic digital content that consists exclusively of text, is published to\ninform the public on any matter of public interest, or is unlikely to mislead\na reasonable person consuming such synthetic digital content or (ii) the\noutputs of a high-risk artificial intelligence system that performs an\nassistive function for standard editing, does not substantially alter the\ninput data provided by the developer, or is used to detect, prevent,\ninvestigate, or prosecute any crime as authorized by law._\n\n**_§ 59.1-609. Operating standards for deployers of high-risk artificial\nintelligence systems._ **\n\n_A. Each deployer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought on or after such date by the Attorney General pursuant to § 59.1-613,\nthere shall be a rebuttable presumption that a deployer of a high-risk\nartificial intelligence system used a reasonable duty of care as required by\nthis subsection if the deployer complied with the requirements of this\nsection._\n\n_B. No deployer shall deploy or use a high-risk artificial intelligence system\nto make a consequential decision unless the deployer has designed and\nimplemented a risk management policy and program for such high-risk artificial\nintelligence system. The risk management policy shall specify the principles,\nprocesses, and personnel that the deployer shall use in maintaining the risk\nmanagement program to identify, mitigate, and document any risk of algorithmic\ndiscrimination that is a reasonably foreseeable consequence of deploying or\nusing such high-risk artificial intelligence system to make a consequential\ndecision. Each risk management policy and program designed, implemented, and\nmaintained pursuant to this subsection shall be (i) at least as stringent as\nthe latest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems and (ii) reasonable considering (a) the size\nand complexity of the deployer; (b) the nature and scope of the high-risk\nartificial intelligence systems deployed and used by the deployer, including\nthe intended uses of such high-risk artificial intelligence systems; (c) the\nsensitivity and volume of data processed in connection with the high-risk\nartificial intelligence systems deployed and used by the deployer; and (d) the\ncost to the deployer to implement and maintain such risk management program._\n\n_C. Except as provided in this subsection, no deployer shall deploy or use a\nhigh-risk artificial intelligence system to make a consequential decision\nunless the deployer has completed an impact assessment for such high-risk\nartificial intelligence system. The deployer shall complete an impact\nassessment for a high-risk artificial intelligence system (i) before the\ndeployer initially deploys such high-risk artificial intelligence system and\n(ii) not later than 90 days after each significant update to such high-risk\nartificial intelligence system is made available._\n\n_Each impact assessment completed pursuant to this subsection shall include,\nat a minimum:_\n\n_1\\. A statement by the deployer disclosing (i) the purpose, intended use\ncases and deployment context of, and benefits afforded by the high-risk\nartificial intelligence system and (ii) whether the deployment or use of the\nhigh-risk artificial intelligence system poses any known or reasonably\nforeseeable risk of algorithmic discrimination and, if so, (a) the nature of\nsuch algorithmic discrimination and (b) the steps that have been taken, to the\nextent feasible, to mitigate such risk;_\n\n_2\\. For each post-deployment impact assessment completed pursuant to this\nsubsection, whether the intended use cases of the high-risk artificial\nintelligence system as updated were consistent with, or varied from, the\ndeveloper 's intended uses of such high-risk artificial intelligence system;_\n\n_3\\. A description of (i) the categories of data the high-risk artificial\nintelligence system processes as inputs and (ii) the outputs such high-risk\nartificial intelligence system produces;_\n\n_4\\. If the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize such high-risk artificial intelligence system;_\n\n_5\\. A list of any metrics used to evaluate the performance and known\nlimitations of the high-risk artificial intelligence system;_\n\n_6\\. A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that such high-risk artificial intelligence system is in use when\nsuch high-risk artificial intelligence system is in use; and_\n\n_7\\. A description of any post-deployment monitoring performed and user\nsafeguards provided concerning such high-risk artificial intelligence system,\nincluding any oversight process established by the deployer to address issues\narising from deployment or use of such high-risk artificial intelligence\nsystem as such issues arise._\n\n_A single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed or used by a deployer. High-risk\nartificial intelligence systems that are in conformity with the latest version\nof the Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology, Standard ISO/IEC 42001 of the\nInternational Organization for Standardization, or another nationally or\ninternationally recognized risk management framework for artificial\nintelligence systems, or parts thereof, shall be presumed to be in conformity\nwith related requirements set out in this section and in associated\nregulations. If a deployer completes an impact assessment for the purpose of\ncomplying with another applicable law or regulation, such impact assessment\nshall be deemed to satisfy the requirements established in this subsection if\nsuch impact assessment is reasonably similar in scope and effect to the impact\nassessment that would otherwise be completed pursuant to this subsection. A\ndeployer that completes an impact assessment pursuant to this subsection shall\nmaintain such impact assessment and all records concerning such impact\nassessment for three years._\n\n_D. Not later than the time that a deployer uses a high-risk artificial\nintelligence system to interact with a consumer, the deployer shall disclose\nto the consumer that the deployer is interacting with an artificial\nintelligence system disclosing (i) the purpose of such high-risk artificial\nintelligence system, (ii) the nature of such system, (iii) the nature of the\nconsequential decision, (iv) the contact information for the deployer, and (v)\na description of the artificial intelligence system in plain language of such\nsystem._\n\n_If such consequential decision is adverse to such consumer, the deployer\nshall provide to the consumer (a) a statement disclosing the principal reason\nor reasons for the consequential decision, including (1) the degree to which\nand manner in which the high-risk artificial intelligence system contributed\nto the consequential decision, (2) the type of data that was processed by such\nsystem in making the consequential decision, and (3) the sources of such data;\n(b) an opportunity to correct any incorrect personal data that the high-risk\nartificial intelligence system processed in making, or as a substantial factor\nin making, the consequential decision; and (c) an opportunity to appeal such\nadverse consequential decision concerning the consumer arising from the\ndeployment of such system. Any such appeal shall allow for human review, if\ntechnically feasible, unless providing the opportunity for appeal is not in\nthe best interest of the consumer, including instances in which any delay\nmight pose a risk to the life or safety of such consumer._\n\n_E. Each deployer shall make available, in a manner that is clear and readily\navailable, a statement summarizing how such deployer manages any reasonably\nforeseeable risk of algorithmic discrimination that may arise from the use or\ndeployment of the high-risk artificial intelligence system._\n\n_F. For any disclosure required pursuant to this section, each deployer shall,\nno later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_G. Any deployer who performs an intentional and substantial modification to\nany high-risk artificial system shall comply with the documentation and\ndisclosure requirements for developers pursuant to subsections A through F of\n§ 59.1-608._\n\n**_§ 59.1-610. Operating standards for integrators of high-risk artificial\nintelligence systems._ **\n\n_Each integrator of a high-risk artificial intelligence system shall develop\nand adopt an acceptable use policy, which shall limit the use of the high-risk\nartificial intelligence system to mitigate known risks of algorithmic\ndiscrimination._\n\n_Each integrator of a high-risk artificial intelligence system shall provide\nto the deployer clear, conspicuous notice of (i) the name or other identifier\nof the high-risk artificial intelligence system integrated into a software\napplication provided to the deployer; (ii) the name and contact information of\nthe developer of the high-risk artificial intelligence system integrated into\na software application provided to the deployer; (iii) whether the integrator\nhas adjusted the model weights of the high-risk artificial intelligence system\nintegrated into the software application by exposing it to additional data, a\nsummary of the adjustment process, and how such process and the resulting\nsystem were evaluated for risk of algorithmic discrimination; (iv) a summary\nof any other non-substantial modifications made by the integrator; and (v) the\nintegrator 's acceptable use policy._\n\n**_§ 59.1-611. Operating standards for distributors of high-risk artificial\nintelligence systems._ **\n\n_Each distributor of a high-risk artificial intelligence system shall use\nreasonable care to protect consumers from any known or reasonably foreseeable\nrisks of algorithmic discrimination. If a distributor of a high-risk\nartificial intelligence system considers or has reason to consider that a\nhigh-risk artificial intelligence system is not in compliance with any\nrequirement of this chapter, it shall immediately withdraw, disable, or\nrecall, as appropriate, the high-risk artificial intelligence system from the\nmarket until such system has been brought into compliance with the\nrequirements of this chapter. The distributor shall inform the developers of\nthe high-risk artificial intelligence system concerned and, where applicable,\nthe deployer of any such system 's noncompliance with this chapter and the\nwithdrawal, disablement, or recall of such system. _\n\n**_§ 59.1-612. Exemptions._ **\n\n_A. Nothing in this chapter shall be construed to restrict a developer 's,\nintegrator's, distributor's or deployer's ability to (i) comply with federal,\nstate, or municipal ordinances or regulations; (ii) comply with a civil,\ncriminal, or regulatory inquiry, investigation, subpoena, or summons by\nfederal, state, local, or other governmental authorities; (iii) cooperate with\nlaw-enforcement agencies concerning conduct or activity that the developer,\nintegrator, distributor, or deployer reasonably and in good faith believes may\nviolate federal, state, or local law, ordinances, or regulations; (iv)\ninvestigate, establish, exercise, prepare for, or defend legal claims; (v)\nprovide a product or service specifically requested by a consumer; (vi)\nperform under a contract to which a consumer is a party, including fulfilling\nthe terms of a written warranty; (vii) take steps at the request of a consumer\nprior to entering into a contract; (viii) take immediate steps to protect an\ninterest that is essential for the life or physical safety of the consumer or\nanother individual; (ix) prevent, detect, protect against, or respond to\nsecurity incidents, identity theft, fraud, harassment, or malicious or\ndeceptive activities; (x) take actions to prevent, detect, protect against,\nreport, or respond to the production, generation, incorporation, or\nsynthesization of child sex abuse material, or any illegal activity, preserve\nthe integrity or security of systems, or investigate, report, or prosecute\nthose responsible for any such action; (xi) engage in public or peer-reviewed\nscientific or statistical research in the public interest that adheres to all\nother applicable ethics and privacy laws and is approved, monitored, and\ngoverned by an institutional review board that determines, or similar\nindependent oversight entities that determine, (a) that the expected benefits\nof the research outweigh the risks associated with such research and (b)\nwhether the developer, integrator, distributor, or deployer has implemented\nreasonable safeguards to mitigate the risks associated with such research;\n(xii) assist another developer, integrator, distributor, or deployer with any\nof the obligations imposed by this chapter; or (xiii) take any action that is\nin the public interest in the areas of public health, community health, or\npopulation health, but solely to the extent that such action is subject to\nsuitable and specific measures to safeguard the public._\n\n_B. The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall not restrict a developer 's or deployer's\nability to (i) conduct internal research to develop, improve, or repair\nproducts, services, or technologies; (ii) effectuate a product recall; (iii)\nidentify and repair technical errors that impair existing or intended\nfunctionality; or (iv) perform internal operations that are reasonably aligned\nwith the expectations of the consumer or reasonably anticipated based on the\nconsumer's existing relationship with the developer, integrator, or deployer._\n\n_C. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper, integrator, distributor, or deployer to disclose trade secrets or\ninformation protected from disclosure by state or federal law._\n\n_D. The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall not apply where compliance by the developer,\nintegrator, distributor, or deployer with such obligations would violate an\nevidentiary privilege under federal law or the laws of the Commonwealth._\n\n_E. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper, integrator, distributor, or deployer that adversely affects the\nlegally protected rights or freedoms of any person, including the rights of\nany person to freedom of speech or freedom of the press guaranteed in the\nFirst Amendment to the Constitution of the United States or under the Virginia\nHuman Rights Act ( § 2.2-3900 et seq.)._\n\n_F. The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall not apply to any artificial intelligence\nsystem that is acquired by or for the federal government or any federal agency\nor department, including the U.S. Department of Commerce, the U.S. Department\nof Defense, and the National Aeronautics and Space Administration, unless such\nartificial intelligence system is a high-risk artificial intelligence system\nthat is used to make, or is a substantial factor in making, a decision\nconcerning employment or housing._\n\n_G. For the purposes of this subsection:_\n\n_\" Affiliate\" means the same as that term is defined in § 6.2-1800._\n\n_\" Bank\" means the same as that term is defined in § 6.2-800._\n\n_\" Credit union\" means the same as that term is defined in § 6.2-1300._\n\n_\" Federal credit union\" means a credit union duly organized under federal\nlaw._\n\n_\" Out-of-state bank\" means the same as that term is defined in § 6.2-836._\n\n_\" Out-of-state credit union\" means a credit union organized and doing\nbusiness in another state._\n\n_\" Subsidiary\" means the same as that term is defined in § 6.2-700._\n\n_The obligations imposed on developers, integrators, distributors, or\ndeployers by this chapter shall be deemed satisfied for any bank, out-of-state\nbank, credit union, federal credit union, out-of-state credit union, or any\naffiliate or subsidiary thereof if such bank, out-of-state bank, credit union,\nfederal credit union, out-of-state credit union, or affiliate or subsidiary is\nsubject to examination by any state or federal prudential regulator under any\npublished guidance or regulations that apply to the use of high-risk\nartificial intelligence systems and such guidance or regulations (i) impose\nrequirements that are substantially equivalent to, and at least as stringent\nas, the requirements set forth in this chapter, and (ii) at a minimum, require\nsuch bank, out-of-state bank, credit union, federal credit union, out-of-state\ncredit union, or affiliate or subsidiary to (a) regularly audit such bank 's,\nout-of-state bank's, credit union's, federal credit union's, out-of-state\ncredit union's, or affiliate's or subsidiary's use of high-risk artificial\nintelligence systems for compliance with state and federal anti-discrimination\nlaws and regulations applicable to such bank, out-of-state bank, credit union,\nfederal credit union, out-of-state credit union, or affiliate or subsidiary\nand (b) mitigate any algorithmic discrimination caused by the use of a high-\nrisk artificial intelligence system or any risk of algorithmic discrimination\nthat is reasonably foreseeable as a result of the use of a high-risk\nartificial intelligence system._\n\n_H. For purposes of this subsection, \"insurer\" means the same as that term is\ndefined in § 38.2-100._\n\n_The provisions of this chapter shall not apply to any insurer, or any high-\nrisk artificial intelligence system developed or deployed by an insurer for\nuse in the business of insurance, if such insurer is regulated and supervised\nby the State Corporation Commission or a comparable federal regulating body\nand subject to examination by such entity under any existing statutes, rules,\nor regulations pertaining to unfair trade practices and unfair discrimination\nprohibited under Chapter 5 ( § 38.2-500 et seq.) of Title 38.2, or published\nguidance or regulations that apply to the use of high-risk artificial\nintelligence systems and such guidance or regulations aid in the prevention\nand mitigation of algorithmic discrimination caused by the use of a high-risk\nartificial intelligence system or any risk of algorithmic discrimination that\nis reasonably foreseeable as a result of the use of a high-risk artificial\nintelligence system. Nothing in this chapter shall be construed to delegate\nexisting regulatory oversight of the business of insurance to any department\nor agency other than the Bureau of Insurance of the Virginia State Corporation\nCommission._\n\n_I. The provisions of this chapter shall not apply to the development of an\nartificial intelligence system that is used exclusively for research,\ntraining, testing, or other pre-deployment activities performed by active\nparticipants of any sandbox software or sandbox environment established and\nsubject to oversight by a designated agency or other government entity and\nthat is in compliance with the provisions of this chapter._\n\n_J. The provisions of this chapter shall not apply to a developer, integrator,\ndistributor, or deployer, or other person who develops, deploys, puts into\nservice, or intentionally modifies, as applicable, a high-risk artificial\nintelligence system that (i) has been approved, authorized, certified,\ncleared, developed, or granted by a federal agency acting within the scope of\nthe federal agency 's authority, or by a regulated entity subject to the\nsupervision and regulation of the Federal Housing Finance Agency or (ii) is in\ncompliance with standards established by a federal agency or by a regulated\nentity subject to the supervision and regulation of the Federal Housing\nFinance Agency, if the standards are substantially equivalent or more\nstringent than the requirements of this chapter._\n\n_K. The provisions of this chapter shall not apply to a developer, integrator,\ndistributor, deployer, or other person that is a covered entity within the\nmeaning of the federal Health Insurance Portability and Accountability Act of\n1996 (42 U.S.C. § 1320d et seq.) and the regulations promulgated under such\nfederal act, as both may be amended from time to time, and is providing (i)\nhealth care recommendations that (a) are generated by an artificial\nintelligence system and (b) require a health care provider to take action to\nimplement the recommendations or (ii) services utilizing an artificial\nintelligence system for an administrative, financial, quality measurement,\nsecurity, or performance improvement function._\n\n_L. If a developer, integrator, distributor, or deployer engages in any action\nauthorized by an exemption set forth in this section, the developer,\nintegrator, distributor, or deployer bears the burden of demonstrating that\nsuch action qualifies for such exemption._\n\n**_§ 59.1-613. Enforcement; civil penalty._ **\n\n_A. The Attorney General shall have exclusive authority to enforce the\nprovisions of this chapter._\n\n_B. Whenever the Attorney General has reasonable cause to believe that any\nperson has engaged in or is engaging in any violation of this chapter, the\nAttorney General is empowered to issue a civil investigative demand. The\nprovisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative\ndemands issued pursuant to this section. In rendering and furnishing any\ninformation requested pursuant to a civil investigative demand issued pursuant\nto this section, a developer, integrator, distributor, or deployer may redact\nor omit any trade secrets or information protected from disclosure by state or\nfederal law. To the extent that any information requested pursuant to a civil\ninvestigative demand issued pursuant to this section is subject to attorney-\nclient privilege or work-product protection, disclosure of such information\npursuant to the civil investigative demand shall not constitute a waiver of\nsuch privilege or protection. Any information, statement, or documentation\nprovided to the Attorney General pursuant to this section shall be exempt from\ndisclosure under the Virginia Freedom of Information Act (§ 2.2-3700 et\nseq.)._\n\n_C. Notwithstanding any contrary provision of law, the Attorney General may\ncause an action to be brought in the appropriate circuit court in the name of\nthe Commonwealth to enjoin any violation of this chapter. The circuit court\nhaving jurisdiction may enjoin such violation notwithstanding the existence of\nan adequate alternative remedy at law. In any action brought pursuant to this\nchapter, it shall not be necessary that damages be proved._\n\n_D. Any person who violates the provisions of this chapter shall be subject to\na civil penalty in an amount not to exceed $1,000 plus reasonable attorney\nfees, expenses, and costs, as determined by the court. Any person who\nwillfully violates the provisions of this chapter shall be subject to a civil\npenalty in an amount not less than $1,000 and not more than $10,000 plus\nreasonable attorney fees, expenses, and costs, as determined by the court.\nSuch civil penalties shall be paid into the Literary Fund._\n\n_E. Each violation of this chapter shall constitute a separate violation and\nshall be subject to any civil penalties imposed under this section._\n\n_F. The Attorney General may require that a developer disclose to the Attorney\nGeneral any statement or documentation described in this chapter if such\nstatement or documentation is relevant to an investigation conducted by the\nAttorney General. The Attorney General may also require that a deployer\ndisclose to the Attorney General any risk management policy designed and\nimplemented, impact assessment completed, or record maintained pursuant to\nthis chapter if such risk management policy, impact assessment, or record is\nrelevant to an investigation conducted by the Attorney General._\n\n_G. In an action brought by the Attorney General pursuant to this section, it\nshall be an affirmative defense that the developer, integrator, distributor,\nor deployer (i) discovers a violation of any provision of this chapter through\nred-teaming; (ii) no later than 45 days after discovering such violation (a)\ncures such violation and (b) provides notice to the Attorney General in a form\nand manner as prescribed by the Attorney General that such violation has been\ncured and evidence that any harm caused by such violation has been mitigated;\nand (iii) is otherwise in compliance with the requirements of this chapter._\n\n_H. Prior to causing an action against a developer, integrator, distributor,\nor deployer for a violation of this chapter pursuant to subsection C, the\nAttorney General shall determine, in consultation with the developer,\nintegrator, distributor, or deployer, if it is possible to cure the violation.\nIf it is possible to cure such violation, the Attorney General may issue a\nnotice of violation to the developer, integrator, distributor, or deployer and\nafford the developer, integrator, distributor, or deployer the opportunity to\ncure such violation within 45 days of the receipt of such notice of violation.\nIn determining whether to grant such opportunity to cure such violation, the\nAttorney General shall consider (i) the number of violations, (ii) the size\nand complexity of the developer, integrator, distributor, or deployer; (iii)\nthe nature and extent of the developer 's, integrator's, distributor's, or\ndeployer's business; (iv) the substantial likelihood of injury to the public;\n(v) the safety of persons or property; (vi) whether such violation was likely\ncaused by human or technical error. If the developer, integrator, distributor,\nor deployer fails to cure such violation within 45 days of the receipt of such\nnotice of violation, the Attorney General may proceed with such action._\n\n_I. Nothing in this chapter shall create a private cause of action in favor of\nany person aggrieved by a violation of this chapter._\n\n**2\\. That the provisions of this act shall become effective on July 1,\n2026.**\n\n**3\\. That the provisions of this act shall apply only to a violation\ncommitted or a cause of action accruing on or after July 1, 2026.**\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "01/27/2025",
      "label": "Recommended as Substituted from Committee",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:VA2024000H2094&verid=VA2024000H2094_20250127_0_RS&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2024 VA H 2094</td> <td><table><tr><td class=\"label\">Author:</td> <td>Maldonado</td></tr> <tr><td class=\"label\">Version:</td> <td>Recommended as Substituted from Committee</td></tr> <tr><td class=\"label\">Version Date:</td> <td>01/27/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">HOUSE BILL NO. 2094</p>\n   <p class=\"center\">AMENDMENT IN THE NATURE OF A SUBSTITUTE</p>\n   <p class=\"center\">(Proposed by the House Committee on Communications, Technology and Innovation</p>\n   <p class=\"center\">on January 27, 2025)</p>\n   <p class=\"center\">(Patron Prior to Substitute Delegate Maldonado)</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"indent\">\n    <i>A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-613, relating to high-risk artificial intelligence; development, deployment, and use; civil penalties.</i>\n   </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">Be it enacted by the General Assembly of Virginia:</p>\n   </span>\n   <p class=\"indent\">1. That the Code of Virginia is amended by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-613, as follows:</p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">CHAPTER 58.</u>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-607. Definitions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">As used in this chapter, unless the context requires a different meaning:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Algorithmic discrimination&quot; means the use of an artificial intelligence system that results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, sexual orientation, veteran status, or other classification protected under state or federal law. &quot;Algorithmic discrimination&quot; does not include (i) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; (ii) the expansion of an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or (iii) an act or omission by or on behalf of a private club or other establishment not in fact open to the public, as set forth in Title II of the Civil Rights Act of 1964, 42 U.S.C. &sect; 2000a(e), as amended from time to time.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Artificial intelligence system&quot; means any machine learning-based system that, for any explicit or implicit objective, infers from the inputs such system receives how to generate outputs, including content, decisions, predictions, and recommendations, that can influence physical or virtual environments. &quot;Artificial intelligence system&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is released on the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consequential decision&quot; means any decision that has a material legal, or similarly significant, effect on the provision or denial to any consumer of (i) parole, probation, a pardon, or any other release from incarceration or court supervision; (ii) education enrollment or an education opportunity; (iii) access to employment; (iv) a financial or lending service; (v) access to health care services; (vi) housing; (vii) insurance; (viii) marital status; or (ix) a legal service.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consumer&quot; means a natural person who is a resident of the Commonwealth and is acting only in an individual or household context. &quot;Consumer&quot; does not include a natural person acting in a commercial or employment context.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Deployer&quot; means any person doing business in the Commonwealth that deploys or uses a high-risk artificial intelligence system to make a consequential decision in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Developer&quot; means any person doing business in the Commonwealth that develops or intentionally and substantially modifies a high-risk artificial intelligence system that is offered, sold, leased, given, or otherwise provided to consumers in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;General-purpose artificial intelligence model&quot; means a model used by an artificial intelligence system or other system that (i) displays significant generality, (ii) is capable of competently performing a wide range of distinct tasks, and (iii) can be integrated into a variety of downstream applications or systems. &quot;General-purpose artificial intelligence model&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is released on the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence&quot; means an artificial intelligence system that is capable of producing and used to produce synthetic content, including audio, images, text, and videos.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence system&quot; means any artificial intelligence system or service that incorporates generative artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that is specifically intended to autonomously make, or be a substantial factor in making, a consequential decision. A system or service is not a &quot;high-risk artificial intelligence system&quot; if it is intended to (i) perform a narrow procedural task, (ii) improve the result of a previously completed human activity, (iii) detect any decision-making patterns or any deviations from pre-existing decision-making patterns, or (iv) perform a preparatory task to an assessment relevant to a consequential decision. &quot;High-risk artificial intelligence system&quot; does not include any of the following technologies:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. Anti-fraud technology that does not use facial recognition technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Anti-malware technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Anti-virus technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Artificial intelligence-enabled video games;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. Calculators;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. Cybersecurity technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. Databases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. Data storage;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. Firewall technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. Internet domain registration;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. Internet website loading;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. Networking;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. Spam and robocall filtering;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. Spell-checking technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. Spreadsheets;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">16. Web caching;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">17. Web hosting or any similar technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">18. Autonomous vehicle technology; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">19. Technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an acceptable use policy that prohibits generating content that is discriminatory or unlawful.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Integrator&quot; means a natural or legal person that knowingly integrates a high-risk artificial intelligence system into a software application and places said integration on the market under that person&#39;s mark, whether for payment or free of charge. An integrator is neither a developer nor a deployer, except as otherwise provided in &sect; 59.1-610, nor will any person be deemed an integrator as a result of offering information technology infrastructure.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Intentional and substantial modification&quot; means any deliberate change made to (i) an artificial intelligence system that results, at the time when the change is implemented and any time thereafter, in any new material risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects compliance of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. &quot;Intentional and substantial modification&quot; does not include (a) any customization made by deployers based on legitimate nondiscriminatory business justifications and within the scope and purpose of the artificial intelligence tool; (b) any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (1) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (2) such change (A) is made to such high-risk artificial intelligence system as a result of any learning described in clause (1) and (B) was predetermined by the deployer or the third party contracted by the deployer and included within the initial impact assessment of such high-risk artificial intelligence system as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Machine learning&quot; means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Person&quot; includes any individual, corporation, partnership, association, cooperative, limited liability company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. &quot;Person&quot; does not include any government or political subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Principal basis&quot; means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Red-teaming&quot; means adversarial testing to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Significant update&quot; means any new version, new release, or other update to a high-risk artificial intelligence system that results in significant changes to such high-risk artificial intelligence system&#39;s use case or key functionality and that results in any new or reasonably foreseeable risk of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Substantial factor&quot; means a factor that is (i) the principal basis for making a consequential decision, (ii) capable of altering the outcome of a consequential decision, and (iii) generated by an artificial intelligence system. &quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as the principal basis to make a consequential decision concerning the consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Synthetic content&quot; means information, such as images, video, audio clips, and text, that has been significantly modified or generated by algorithms, including by artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique, or process, that (i) derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by, other persons who can obtain economic value from its disclosure or use and (ii) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-608. Operating standards for developers of high-risk artificial intelligence systems.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each developer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-612, there shall be a rebuttable presumption that a developer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the developer complied with the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No developer of a high-risk artificial intelligence system shall offer, sell, lease, give, or otherwise provide to a deployer, integrator, or other developer a high-risk artificial intelligence system unless the developer makes available to the deployer, integrator, or other developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement disclosing the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Documentation disclosing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">a. The known or reasonably known limitations of such high-risk artificial intelligence system, including any and all known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">b. The purpose of such high-risk artificial intelligence system and the intended benefits and uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">c. A summary describing how such high-risk artificial intelligence system was evaluated for performance before such high-risk artificial intelligence system was licensed, sold, leased, given, or otherwise made available to a deployer, integrator, or other developer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">d. The measures the developer has taken to mitigate reasonable foreseeable risks of algorithmic discrimination that the developer knows arises from deployment or use of such high-risk artificial intelligence system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">e. How an individual can use such high-risk artificial intelligence system and monitor the performance of such high-risk artificial intelligence system for any risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Documentation including a (i) description of how the high-risk artificial intelligence system was evaluated for performance and for mitigation of algorithmic discrimination before such system was made available to the deployer, integrator, or other developer; (ii) high level summary about how data sources were evaluated for potential bias and appropriate mitigations were applied; (iii) description of the intended outputs of the high-risk artificial intelligence system; (iv) description of the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and (v) description of how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when such system is used to make, or is a substantial factor in making, a consequential decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any additional documentation that is reasonably necessary to assist the deployer, integrator, or other developer in understanding the outputs and monitoring performance of the high-risk artificial intelligence system for risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Each developer that offers, sells, leases, gives, or otherwise makes available to a deployer, integrator, or other developer a high-risk artificial intelligence system shall make available to the deployer, integrator, or other developer to the extent feasible and necessary, information and documentation through artifacts such as model cards or impact assessments, including any risk management policy designed and implemented and any relevant impact assessment completed, and such documentation and information shall enable the deployer, integrator, other developer, or a third party contracted by the deployer to complete an impact assessment as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. A developer that also serves as a deployer for any high-risk artificial intelligence system shall not be required to generate the documentation required by this section unless such high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer or as otherwise required by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this section shall be construed to require a developer to disclose any trade secret, as defined in &sect; 59.1-336, information that could create a security risk, or other confidential or proprietary information.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For any disclosure required pursuant to this section, each developer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. 1. Each developer of a generative artificial intelligence system that generates or modifies synthetic content shall ensure that the outputs of such high-risk artificial intelligence system (i) are marked and detectable in a manner that is detectable by consumers; (ii) comply with any applicable accessibility requirements, as synthetic content; and (iii) such marking must be applied at the time the output is generated;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. If such synthetic content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for marking outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The marking of outputs required by subdivision 1 shall not apply to (i) synthetic content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Where multiple developers contribute to the development of a high-risk artificial intelligence system, each developer shall be subject to the obligations and operating standards applicable to developers pursuant to this section solely with respect to its activities contributing to the development of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-609. Operating standards for deployers of high-risk artificial intelligence systems.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each deployer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-612, there shall be a rebuttable presumption that a deployer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if (i) the deployer complied with the provisions of this section and (ii) an independent third party identified by the Attorney General and retained by the developer completed bias and governance audits for the high-risk artificial intelligence system. Not later than July 1, 2026, and at least annually thereafter, the Attorney General shall (a) identify the independent third parties the Attorney General deems qualified to complete bias and governance audits for the purposes of clause (ii) and (b) make a list of such independent third parties available on the Attorney General&#39;s website for the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has designed and implemented a risk management policy and program for such high-risk artificial intelligence system. The risk management policy shall specify the principles, processes, and personnel that the deployer shall use in maintaining the risk management program to identify, mitigate, and document any risk of algorithmic discrimination that is a reasonably foreseeable consequence of deploying or using such high-risk artificial intelligence system to make a consequential decision. Each risk management policy and program designed, implemented, and maintained pursuant to this subsection shall be reasonable considering the guidance and standards set forth in the latest version of:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. The Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Standard ISO/IEC 42001 of the International Organization for Standardization;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A nationally or internationally recognized risk management framework for artificial intelligence systems with requirements that are substantially equivalent to, and at least as stringent as, the requirements set forth in this section; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any risk management framework for artificial intelligence systems that the Attorney General may designate and is substantially equivalent to, and at least as stringent as, the guidance and standards described in subdivision 1.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Except as provided in this subsection, no deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has completed an impact assessment for such high-risk artificial intelligence system. The deployer shall complete an impact assessment for a high-risk artificial intelligence system (i) before the deployer initially deploys such high-risk artificial intelligence system and (ii) before a significant update to such high-risk artificial intelligence system is made available.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each impact assessment completed pursuant to this subsection shall include, at a minimum:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement by the deployer disclosing (i) the purpose, intended use cases and deployment context of, and benefits afforded by the high-risk artificial intelligence system and (ii) whether the deployment or use of the high-risk artificial intelligence system poses any known or reasonably foreseeable risk of algorithmic discrimination and, if so, (a) the nature of such algorithmic discrimination and (b) the steps that have been taken, to the extent feasible, to mitigate such risk;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. For each post-deployment impact assessment completed pursuant to this subsection, whether the intended use cases of the high-risk artificial intelligence system as updated were consistent with, or varied from, the developer&#39;s intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A description of (i) the categories of data the high-risk artificial intelligence system processes as inputs and (ii) the outputs such high-risk artificial intelligence system produces;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. If the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. A list of any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. A description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that such high-risk artificial intelligence system is in use when such high-risk artificial intelligence system is in use;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. A description of any post-deployment monitoring performed and user safeguards provided concerning such high-risk artificial intelligence system, including any oversight process established by the deployer to address issues arising from deployment or use of such high-risk artificial intelligence system as such issues arise; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. An analysis of such high-risk artificial intelligence system&#39;s validity and reliability in accordance with standard industry practices and a description of any metrics used to evaluate the performance and known limitations of such high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed or used by a deployer. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations. If a deployer completes an impact assessment for the purpose of complying with another applicable law or regulation, such impact assessment shall be deemed to satisfy the requirements established in this subsection if such impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection. A deployer that completes an impact assessment pursuant to this subsection shall maintain such impact assessment and all records concerning such impact assessment for three years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Throughout the period of time that a high-risk artificial intelligence system is deployed and for a period of at least three years following the final deployment of such high-risk artificial intelligence system, the deployer shall retain all records concerning each impact assessment conducted on the high-risk artificial intelligence system, including all raw data used to conduct disparate impact testing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Not later than the time that a deployer uses a high-risk artificial intelligence system to interact with a consumer, the deployer shall disclose to the consumer that the deployer is interacting with an artificial intelligence system disclosing (i) the purpose of such high-risk artificial intelligence system, (ii) the nature of such system, (iii) the nature of the consequential decision, (iv) the contact information for the deployer, and (v) a description of the artificial intelligence system in plain language of such system, which shall include (a) a description of the personal characteristics or attributes that such system will measure or assess; (b) the method by which the system measures or assesses such attributes or characteristics; (c) how such attributes or characteristics are relevant to the consequential decisions for which the system should be used; (d) any human components of such system; (e) how any automated components of such system are used to inform such consequential decisions; and (f) a direct link to a publicly accessible page on the deployer&#39;s website that contains a plain language description of the logic used in the system, including the key parameters that affect the output of the system; the system&#39;s outputs; the type and source of data collected from natural persons and processed by the system when it is used to make, or assists in making, a consequential decision; and the results of the most recent impact assessment, or an active link to a webpage where a consumer can review those results.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A deployer that has deployed a high-risk artificial intelligence system to make a consequential decision concerning a consumer shall transmit to the consumer the consequential decision within three business days of the consequential decision. If such consequential decision is adverse to such consumer, the deployer shall provide to the consumer (a) a statement disclosing the principal reason or reasons for the consequential decision, including (1) the degree to which and manner in which the high-risk artificial intelligence system contributed to the consequential decision, (2) the type of data that was processed by such system in making the consequential decision, and (3) the sources of such data; (b) pursuant to subdivision A 2 of &sect; 59.1-577, an opportunity to correct any inaccuracies in the consumer&#39;s personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and (c) an opportunity to appeal such adverse consequential decision concerning the consumer arising from the deployment of such system. Any such appeal shall allow for human review, if technically feasible, unless providing the opportunity for appeal is not in the best interest of the consumer, including instances in which any delay might pose a risk to the life or safety of such consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each deployer shall make available, in a manner that is clear and readily available, a statement summarizing how such deployer manages any reasonably foreseeable risk of algorithmic discrimination that may arise from the use or deployment of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each deployer shall, no later than 30 days after the deployer is notified by the developer that the developer has performed an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. Any deployer who performs an intentional and substantial modification to any high-risk artificial intelligence system shall comply with the documentation and disclosure requirements for developers pursuant to subsections B through G of &sect; 59.1-608.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Nothing in this section shall be construed to require a deployer to disclose any trade secret, as defined in &sect; 59.1-336, information that could create a security risk, or other confidential or proprietary information.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-610. Operating standards for integrators of high-risk artificial intelligence systems.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. On and after September 1, 2026, an integrator shall be considered a developer of a high-risk artificial intelligence system and shall be subject to any obligations and operating standards of a developer pursuant to this chapter with respect to any substantial modifications made by the integrator to such high-risk artificial intelligence system. In such event, the developer that initially placed the high-risk artificial intelligence system on the market shall no longer be considered a developer under this chapter with respect to such substantial modifications made by the integrator.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. On and after September 1, 2026, an integrator shall provide clear and conspicuous notice to the deployer of:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. The name or other identifier of the high-risk artificial intelligence system integrated into a software application provided to the deployer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. The name and contact information of the developer of the high-risk artificial intelligence system integrated into a software application provided to the deployer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. If the integrator has adjusted the model weights of the high-risk artificial intelligence system integrated into the software application by exposing it to additional data, a high-level summary of the adjustment process and how that process and the resultant system were evaluated for risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. A high-level summary of any other nonsubstantial modifications made by the integrator; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. The integrator&#39;s applicable policies regarding usage, including any intended limitations or recommended mitigations associated with the intended use of a high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. On and after September 1, 2026, an integrator shall provide to a deployer any information or documentation the developer provides to the integrator pursuant to the developer&#39;s obligations under subsections B and C of &sect; 59.1-608, provided that under no circumstance shall the integrator be responsible under this section for any failure to provide documentation to a deployer if that failure was caused by a developer&#39;s failure to provide the applicable documentation to the integrator in the first instance.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-611. Exemptions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Nothing in this chapter shall be construed to restrict a developer&#39;s or deployer&#39;s ability to (i) comply with federal, state, or municipal ordinances or regulations; (ii) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by federal, state, local, or other governmental authorities; (iii) cooperate with law-enforcement agencies concerning conduct or activity that the developer or deployer reasonably and in good faith believes may violate federal, state, or local law, ordinances, or regulations; (iv) investigate, establish, exercise, prepare for, or defend legal claims; (v) provide a product or service specifically requested by a consumer; (vi) perform under a contract to which a consumer is a party, including fulfilling the terms of a written warranty; (vii) take steps at the request of a consumer prior to entering into a contract; (viii) take immediate steps to protect an interest that is essential for the life or physical safety of the consumer or another individual; (ix) prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, or malicious or deceptive activities; (x) take actions to prevent, detect, protect against, report, or respond to the production, generation, incorporation, or synthesization of child sex abuse material, or any illegal activity, preserve the integrity or security of systems, or investigate, report, or prosecute those responsible for any such action; (xi) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is approved, monitored, and governed by an institutional review board that determines, or similar independent oversight entities that determine, (a) that the expected benefits of the research outweigh the risks associated with such research and (b) whether the developer or deployer has implemented reasonable safeguards to mitigate the risks associated with such research; (xii) assist another developer or deployer with any of the obligations imposed by this chapter; or (xiii) take any action that is in the public interest in the areas of public health, community health, or population health, but solely to the extent that such action is subject to suitable and specific measures to safeguard the public.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. The obligations imposed on developers or deployers by this chapter shall not restrict a developer&#39;s or deployer&#39;s ability to (i) conduct internal research to develop, improve, or repair products, services, or technologies; (ii) effectuate a product recall; (iii) identify and repair technical errors that impair existing or intended functionality; or (iv) perform internal operations that are reasonably aligned with the expectations of the consumer or reasonably anticipated based on the consumer&#39;s existing relationship with the developer or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer to disclose trade secrets or information protected from disclosure by state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. The obligations imposed on developers or deployers by this chapter shall not apply where compliance by the developer or deployer with such obligations would violate an evidentiary privilege under federal law or the laws of the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer that adversely affects the legally protected rights or freedoms of any person, including the rights of any person to freedom of speech or freedom of the press guaranteed in the First Amendment to the Constitution of the United States or under the Virginia Human Rights Act (&sect; 2.2-3900 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The obligations imposed on developers or deployers by this chapter shall not apply to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the U.S. Department of Commerce, the U.S. Department of Defense, and the National Aeronautics and Space Administration, unless such artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For the purposes of this subsection:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Affiliate&quot; means the same as that term is defined in &sect; 6.2-899.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Bank&quot; means the same as that term is defined in &sect; 6.2-800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Credit union&quot; means the same as that term is defined in &sect; 6.2-1300.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Federal credit union&quot; means a credit union duly organized under federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Mortgage lender&quot; means the same as that term is defined in &sect; 6.2-1600.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state bank&quot; means the same as that term is defined in &sect; 6.2-836.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state credit union&quot; means a credit union organized and doing business in another state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Savings institution&quot; means the same as that term is defined in &sect; 6.2-1100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Subsidiary&quot; means the same as that term is defined in &sect; 6.2-700.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The obligations imposed on developers or deployers by this chapter shall be deemed satisfied for any bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or any affiliate or subsidiary thereof if such bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or affiliate or subsidiary is subject to the jurisdiction of any state or federal regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. For purposes of this subsection, &quot;insurer&quot; means the same as that term is defined in &sect; 38.2-100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The provisions of this chapter shall not apply to any insurer, or any high-risk artificial intelligence system developed or deployed by an insurer for use in the business of insurance, if such insurer is regulated and supervised by the State Corporation Commission or a comparable federal regulating body and subject to examination by such entity under any existing statutes, rules, or regulations pertaining to unfair trade practices and unfair discrimination prohibited under Chapter 5 (&sect; 38.2-500 et seq.) of Title 38.2, or published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations aid in the prevention and mitigation of algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. Nothing in this chapter shall be construed to delegate existing regulatory oversight of the business of insurance to any department or agency other than the Bureau of Insurance of the Virginia State Corporation Commission.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. The provisions of this chapter shall not apply to the development of an artificial intelligence system that is used exclusively for research, training, testing, or other pre-deployment activities performed by active participants of any sandbox software or sandbox environment established and subject to oversight by a designated agency or other government entity and that is in compliance with the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">J. The provisions of this chapter shall not apply to a developer or deployer, or other person who develops, deploys, puts into service, or intentionally modifies, as applicable, a high-risk artificial intelligence system that (i) has been approved, authorized, certified, cleared, developed, or granted by a federal agency acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency or (ii) is in compliance with standards established by a federal agency or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">K. The provisions of this chapter shall not apply to a developer or deployer, or other person that (i) facilitates or engages in the provision of telehealth services, as defined in &sect; 32.1-122.03:1, or (ii) is a covered entity within the meaning of the federal Health Insurance Portability and Accountability Act of 1996 (42 U.S.C. &sect; 1320d et seq.) and the regulations promulgated under such federal act, as both may be amended from time to time, and is providing (a) health care recommendations that (1) are generated by an artificial intelligence system and (2) require a health care provider, as defined in &sect; 8.01-581.1, to take action to implement the recommendations or (b) services utilizing an artificial intelligence system for an administrative, quality measurement, security, or internal cost or performance improvement function.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">L. If a developer or deployer engages in any action authorized by an exemption set forth in this section, the developer or deployer bears the burden of demonstrating that such action qualifies for such exemption.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">M. If a developer or deployer withholds information pursuant to an exemption set forth in this chapter for which disclosure would otherwise be required by this chapter, including the exemption from disclosure of trade secrets, the developer or deployer shall notify the subject of disclosure and provide a basis for withholding the information. If a developer or deployer redacts any information pursuant to an exemption from disclosure, the developer or deployer shall notify the subject of disclosure that the developer or deployer is redacting such information and provide the basis for such decision to redact.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-612. Enforcement; civil penalties.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. The Attorney General shall have exclusive authority to enforce the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any violation of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of &sect; 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer, integrator, or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. If a developer, integrator, or deployer refuses to disclose, redacts, or omits information based on the exemption from disclosure of trade secrets, such developer, integrator, or deployer shall affirmatively state to the Attorney General that the basis for nondisclosure, redaction, or omission is because such information is a trade secret. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand shall not constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be exempt from disclosure under the Virginia Freedom of Information Act (&sect; 2.2-3700 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any violation of this chapter. The circuit court having jurisdiction may enjoin such violation notwithstanding the existence of an adequate alternative remedy at law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Any person who violates the provisions of this chapter shall be subject to a civil penalty in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil penalty in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil penalties shall be paid into the Literary Fund.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each violation of this chapter shall constitute a separate violation and shall be subject to any civil penalties imposed under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The Attorney General may require that a developer disclose to the Attorney General any statement or documentation described in this chapter if such statement or documentation is relevant to an investigation conducted by the Attorney General. The Attorney General may also require that a deployer disclose to the Attorney General any risk management policy designed and implemented, impact assessment completed, or record maintained pursuant to this chapter if such risk management policy, impact assessment, or record is relevant to an investigation conducted by the Attorney General.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. In an action brought by the Attorney General pursuant to this section, it shall be an affirmative defense that the developer, integrator, or deployer (i) discovers a violation of any provision of this chapter through red-teaming; (ii) no later than 45 days after discovering such violation (a) cures such violation and (b) provides notice to the Attorney General in a form and manner as prescribed by the Attorney General that such violation has been cured and evidence that any harm caused by such violation has been mitigated; and (iii) is otherwise in compliance with the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Prior to causing an action against a developer, integrator, or deployer for a violation of this chapter pursuant to subsection C, the Attorney General shall determine, in consultation with the developer, integrator, or deployer, if it is possible to cure the violation. If it is possible to cure such violation, the Attorney General may issue a notice of violation to the developer, integrator, or deployer and afford the developer, integrator, or deployer the opportunity to cure such violation within 45 days of the receipt of such notice of violation. In determining whether to grant such opportunity to cure such violation, the Attorney General shall consider (i) the number of violations; (ii) the size and complexity of the developer, integrator, or deployer; (iii) the nature and extent of the developer&#39;s, integrator&#39;s, or deployer&#39;s business; (iv) the substantial likelihood of injury to the public; (v) the safety of persons or property; and (vi) whether such violation was likely caused by human or technical error. If the developer, integrator, or deployer fails to cure such violation within 45 days of the receipt of such notice of violation, the Attorney General may proceed with such action.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Nothing in this chapter shall create a private cause of action in favor of any person aggrieved by a violation of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-613. Construction of chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. This chapter is declared to be remedial, with the purposes of protecting consumers and ensuring consumers receive information about consequential decisions affecting them. The provisions of this chapter granting rights or protections to consumers shall be construed broadly and exemptions construed narrowly.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. If any provision of this chapter or its application to any person or circumstance is held invalid, the invalidity shall not affect other provisions or applications of this chapter that can be given effect without the invalid provision or application, and to this end all the provisions of this chapter are hereby expressly declared to be severable.</u>\n   </p>\n   <effective_clause>\n    <p class=\"indent\">2. That the provisions of this act shall become effective on July 1, 2026.</p>\n   </effective_clause>\n   <p class=\"indent\">3. That compliance with the provisions of Chapter 58 (&sect; 59.1-607 et seq.) of Title 59.1 of the Code of Virginia, as created by this act, shall not (i) relieve a person from liability for any causes of action that existed at common law or by statute prior to July 1, 2026, or (ii) be construed to modify or otherwise affect, preempt, limit, or displace any causes of action that existed at common law or by statute prior to July 1, 2026.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2024 VA H 2094 | | Author: | Maldonado  \n---|---  \nVersion: | Recommended as Substituted from Committee  \nVersion Date: | 01/27/2025  \n  \nHOUSE BILL NO. 2094\n\nAMENDMENT IN THE NATURE OF A SUBSTITUTE\n\n(Proposed by the House Committee on Communications, Technology and Innovation\n\non January 27, 2025)\n\n(Patron Prior to Substitute Delegate Maldonado)\n\n_A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-613,\nrelating to high-risk artificial intelligence; development, deployment, and\nuse; civil penalties._\n\nBe it enacted by the General Assembly of Virginia:\n\n1\\. That the Code of Virginia is amended by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-613, as\nfollows:\n\n_CHAPTER 58._\n\n_HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT._\n\n_§ 59.1-607. Definitions._\n\n_As used in this chapter, unless the context requires a different meaning:_\n\n_\" Algorithmic discrimination\" means the use of an artificial intelligence\nsystem that results in an unlawful differential treatment or impact that\ndisfavors an individual or group of individuals on the basis of their actual\nor perceived age, color, disability, ethnicity, genetic information, limited\nproficiency in the English language, national origin, race, religion,\nreproductive health, sex, sexual orientation, veteran status, or other\nclassification protected under state or federal law. \"Algorithmic\ndiscrimination\" does not include (i) the offer, license, or use of a high-risk\nartificial intelligence system by a developer or deployer for the sole purpose\nof the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; (ii) the expansion of an applicant, customer, or participant pool to\nincrease diversity or redress historical discrimination; or (iii) an act or\nomission by or on behalf of a private club or other establishment not in fact\nopen to the public, as set forth in Title II of the Civil Rights Act of 1964,\n42 U.S.C. § 2000a(e), as amended from time to time._\n\n_\" Artificial intelligence system\" means any machine learning-based system\nthat, for any explicit or implicit objective, infers from the inputs such\nsystem receives how to generate outputs, including content, decisions,\npredictions, and recommendations, that can influence physical or virtual\nenvironments. \"Artificial intelligence system\" does not include any artificial\nintelligence model that is used for development, prototyping, and research\nactivities before such artificial intelligence model is released on the\nmarket._\n\n_\" Consequential decision\" means any decision that has a material legal, or\nsimilarly significant, effect on the provision or denial to any consumer of\n(i) parole, probation, a pardon, or any other release from incarceration or\ncourt supervision; (ii) education enrollment or an education opportunity;\n(iii) access to employment; (iv) a financial or lending service; (v) access to\nhealth care services; (vi) housing; (vii) insurance; (viii) marital status; or\n(ix) a legal service._\n\n_\" Consumer\" means a natural person who is a resident of the Commonwealth and\nis acting only in an individual or household context. \"Consumer\" does not\ninclude a natural person acting in a commercial or employment context._\n\n_\" Deployer\" means any person doing business in the Commonwealth that deploys\nor uses a high-risk artificial intelligence system to make a consequential\ndecision in the Commonwealth._\n\n_\" Developer\" means any person doing business in the Commonwealth that\ndevelops or intentionally and substantially modifies a high-risk artificial\nintelligence system that is offered, sold, leased, given, or otherwise\nprovided to consumers in the Commonwealth._\n\n_\" General-purpose artificial intelligence model\" means a model used by an\nartificial intelligence system or other system that (i) displays significant\ngenerality, (ii) is capable of competently performing a wide range of distinct\ntasks, and (iii) can be integrated into a variety of downstream applications\nor systems. \"General-purpose artificial intelligence model\" does not include\nany artificial intelligence model that is used for development, prototyping,\nand research activities before such artificial intelligence model is released\non the market._\n\n_\" Generative artificial intelligence\" means an artificial intelligence system\nthat is capable of producing and used to produce synthetic content, including\naudio, images, text, and videos._\n\n_\" Generative artificial intelligence system\" means any artificial\nintelligence system or service that incorporates generative artificial\nintelligence._\n\n_\" High-risk artificial intelligence system\" means any artificial intelligence\nsystem that is specifically intended to autonomously make, or be a substantial\nfactor in making, a consequential decision. A system or service is not a\n\"high-risk artificial intelligence system\" if it is intended to (i) perform a\nnarrow procedural task, (ii) improve the result of a previously completed\nhuman activity, (iii) detect any decision-making patterns or any deviations\nfrom pre-existing decision-making patterns, or (iv) perform a preparatory task\nto an assessment relevant to a consequential decision. \"High-risk artificial\nintelligence system\" does not include any of the following technologies:_\n\n_1\\. Anti-fraud technology that does not use facial recognition technology;_\n\n_2\\. Anti-malware technology;_\n\n_3\\. Anti-virus technology;_\n\n_4\\. Artificial intelligence-enabled video games;_\n\n_5\\. Calculators;_\n\n_6\\. Cybersecurity technology;_\n\n_7\\. Databases;_\n\n_8\\. Data storage;_\n\n_9\\. Firewall technology;_\n\n_10\\. Internet domain registration;_\n\n_11\\. Internet website loading;_\n\n_12\\. Networking;_\n\n_13\\. Spam and robocall filtering;_\n\n_14\\. Spell-checking technology;_\n\n_15\\. Spreadsheets;_\n\n_16\\. Web caching;_\n\n_17\\. Web hosting or any similar technology;_\n\n_18\\. Autonomous vehicle technology; or_\n\n_19\\. Technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an acceptable use\npolicy that prohibits generating content that is discriminatory or unlawful._\n\n_\" Integrator\" means a natural or legal person that knowingly integrates a\nhigh-risk artificial intelligence system into a software application and\nplaces said integration on the market under that person's mark, whether for\npayment or free of charge. An integrator is neither a developer nor a\ndeployer, except as otherwise provided in § 59.1-610, nor will any person be\ndeemed an integrator as a result of offering information technology\ninfrastructure._\n\n_\" Intentional and substantial modification\" means any deliberate change made\nto (i) an artificial intelligence system that results, at the time when the\nchange is implemented and any time thereafter, in any new material risk of\nalgorithmic discrimination or (ii) a general-purpose artificial intelligence\nmodel that affects compliance of the general-purpose artificial intelligence\nmodel, materially changes the purpose of the general-purpose artificial\nintelligence model, or results in any new reasonably foreseeable risk of\nalgorithmic discrimination. \"Intentional and substantial modification\" does\nnot include (a) any customization made by deployers based on legitimate\nnondiscriminatory business justifications and within the scope and purpose of\nthe artificial intelligence tool; (b) any change made to a high-risk\nartificial intelligence system, or the performance of a high-risk artificial\nintelligence system, if (1) the high-risk artificial intelligence system\ncontinues to learn after such high-risk artificial intelligence system is\noffered, sold, leased, licensed, given, or otherwise made available to a\ndeployer, or deployed, and (2) such change (A) is made to such high-risk\nartificial intelligence system as a result of any learning described in clause\n(1) and (B) was predetermined by the deployer or the third party contracted by\nthe deployer and included within the initial impact assessment of such high-\nrisk artificial intelligence system as required in § 59.1-609._\n\n_\" Machine learning\" means the development of algorithms to build data-derived\nstatistical models that are capable of drawing inferences from previously\nunseen data without explicit human instruction._\n\n_\" Person\" includes any individual, corporation, partnership, association,\ncooperative, limited liability company, trust, joint venture, or any other\nlegal or commercial entity and any successor, representative, agent, agency,\nor instrumentality thereof. \"Person\" does not include any government or\npolitical subdivision._\n\n_\" Principal basis\" means the use of an output of a high-risk artificial\nintelligence system to make a decision without (i) human review, oversight,\ninvolvement, or intervention or (ii) meaningful consideration by a human._\n\n_\" Red-teaming\" means adversarial testing to identify the potential adverse\nbehaviors or outcomes of an artificial intelligence system, identify how such\nbehaviors or outcomes occur, and stress test the safeguards against such\nbehaviors or outcomes._\n\n_\" Significant update\" means any new version, new release, or other update to\na high-risk artificial intelligence system that results in significant changes\nto such high-risk artificial intelligence system's use case or key\nfunctionality and that results in any new or reasonably foreseeable risk of\nalgorithmic discrimination._\n\n_\" Substantial factor\" means a factor that is (i) the principal basis for\nmaking a consequential decision, (ii) capable of altering the outcome of a\nconsequential decision, and (iii) generated by an artificial intelligence\nsystem. \"Substantial factor\" includes any use of an artificial intelligence\nsystem to generate any content, decision, prediction, or recommendation\nconcerning a consumer that is used as the principal basis to make a\nconsequential decision concerning the consumer._\n\n_\" Synthetic content\" means information, such as images, video, audio clips,\nand text, that has been significantly modified or generated by algorithms,\nincluding by artificial intelligence._\n\n_\" Trade secret\" means information, including a formula, pattern, compilation,\nprogram, device, method, technique, or process, that (i) derives independent\neconomic value, actual or potential, from not being generally known to, and\nnot being readily ascertainable by proper means by, other persons who can\nobtain economic value from its disclosure or use and (ii) is the subject of\nefforts that are reasonable under the circumstances to maintain its secrecy._\n\n_§ 59.1-608. Operating standards for developers of high-risk artificial\nintelligence systems._\n\n_A. Each developer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-612, there shall be a\nrebuttable presumption that a developer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if the\ndeveloper complied with the requirements of this section._\n\n_B. No developer of a high-risk artificial intelligence system shall offer,\nsell, lease, give, or otherwise provide to a deployer, integrator, or other\ndeveloper a high-risk artificial intelligence system unless the developer\nmakes available to the deployer, integrator, or other developer:_\n\n_1\\. A statement disclosing the intended uses of such high-risk artificial\nintelligence system;_\n\n_2\\. Documentation disclosing the following:_\n\n_a. The known or reasonably known limitations of such high-risk artificial\nintelligence system, including any and all known or reasonably foreseeable\nrisks of algorithmic discrimination arising from the intended uses of such\nhigh-risk artificial intelligence system;_\n\n_b. The purpose of such high-risk artificial intelligence system and the\nintended benefits and uses of such high-risk artificial intelligence system;_\n\n_c. A summary describing how such high-risk artificial intelligence system was\nevaluated for performance before such high-risk artificial intelligence system\nwas licensed, sold, leased, given, or otherwise made available to a deployer,\nintegrator, or other developer;_\n\n_d. The measures the developer has taken to mitigate reasonable foreseeable\nrisks of algorithmic discrimination that the developer knows arises from\ndeployment or use of such high-risk artificial intelligence system; and_\n\n_e. How an individual can use such high-risk artificial intelligence system\nand monitor the performance of such high-risk artificial intelligence system\nfor any risk of algorithmic discrimination;_\n\n_3\\. Documentation including a (i) description of how the high-risk artificial\nintelligence system was evaluated for performance and for mitigation of\nalgorithmic discrimination before such system was made available to the\ndeployer, integrator, or other developer; (ii) high level summary about how\ndata sources were evaluated for potential bias and appropriate mitigations\nwere applied; (iii) description of the intended outputs of the high-risk\nartificial intelligence system; (iv) description of the measures the developer\nhas taken to mitigate known or reasonably foreseeable risks of algorithmic\ndiscrimination that may arise from the reasonably foreseeable deployment of\nthe high-risk artificial intelligence system; and (v) description of how the\nhigh-risk artificial intelligence system should be used, not be used, and be\nmonitored by an individual when such system is used to make, or is a\nsubstantial factor in making, a consequential decision; and_\n\n_4\\. Any additional documentation that is reasonably necessary to assist the\ndeployer, integrator, or other developer in understanding the outputs and\nmonitoring performance of the high-risk artificial intelligence system for\nrisks of algorithmic discrimination._\n\n_C. Each developer that offers, sells, leases, gives, or otherwise makes\navailable to a deployer, integrator, or other developer a high-risk artificial\nintelligence system shall make available to the deployer, integrator, or other\ndeveloper to the extent feasible and necessary, information and documentation\nthrough artifacts such as model cards or impact assessments, including any\nrisk management policy designed and implemented and any relevant impact\nassessment completed, and such documentation and information shall enable the\ndeployer, integrator, other developer, or a third party contracted by the\ndeployer to complete an impact assessment as required in § 59.1-609._\n\n_D. A developer that also serves as a deployer for any high-risk artificial\nintelligence system shall not be required to generate the documentation\nrequired by this section unless such high-risk artificial intelligence system\nis provided to an unaffiliated entity acting as a deployer or as otherwise\nrequired by law._\n\n_E. Nothing in this section shall be construed to require a developer to\ndisclose any trade secret, as defined in § 59.1-336, information that could\ncreate a security risk, or other confidential or proprietary information._\n\n_F. High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_G. For any disclosure required pursuant to this section, each developer\nshall, no later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_H. 1. Each developer of a generative artificial intelligence system that\ngenerates or modifies synthetic content shall ensure that the outputs of such\nhigh-risk artificial intelligence system (i) are marked and detectable in a\nmanner that is detectable by consumers; (ii) comply with any applicable\naccessibility requirements, as synthetic content; and (iii) such marking must\nbe applied at the time the output is generated;_\n\n_2\\. If such synthetic content is an audio, image, or video format that forms\npart of an evidently artistic, creative, satirical, fictional analogous work\nor program, such requirement for marking outputs of high-risk artificial\nintelligence systems pursuant to subdivision 1 shall be limited to a manner\nthat does not hinder the display or enjoyment of such work or program._\n\n_3\\. The marking of outputs required by subdivision 1 shall not apply to (i)\nsynthetic content that consists exclusively of text, is published to inform\nthe public on any matter of public interest, or is unlikely to mislead a\nreasonable person consuming such synthetic content or (ii) the outputs of a\nhigh-risk artificial intelligence system that performs an assistive function\nfor standard editing, does not substantially alter the input data provided by\nthe developer, or is used to detect, prevent, investigate, or prosecute any\ncrime as authorized by law._\n\n_I. Where multiple developers contribute to the development of a high-risk\nartificial intelligence system, each developer shall be subject to the\nobligations and operating standards applicable to developers pursuant to this\nsection solely with respect to its activities contributing to the development\nof the high-risk artificial intelligence system._\n\n_§ 59.1-609. Operating standards for deployers of high-risk artificial\nintelligence systems._\n\n_A. Each deployer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-612, there shall be a\nrebuttable presumption that a deployer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if (i)\nthe deployer complied with the provisions of this section and (ii) an\nindependent third party identified by the Attorney General and retained by the\ndeveloper completed bias and governance audits for the high-risk artificial\nintelligence system. Not later than July 1, 2026, and at least annually\nthereafter, the Attorney General shall (a) identify the independent third\nparties the Attorney General deems qualified to complete bias and governance\naudits for the purposes of clause (ii) and (b) make a list of such independent\nthird parties available on the Attorney General's website for the high-risk\nartificial intelligence system._\n\n_B. No deployer shall deploy or use a high-risk artificial intelligence system\nto make a consequential decision unless the deployer has designed and\nimplemented a risk management policy and program for such high-risk artificial\nintelligence system. The risk management policy shall specify the principles,\nprocesses, and personnel that the deployer shall use in maintaining the risk\nmanagement program to identify, mitigate, and document any risk of algorithmic\ndiscrimination that is a reasonably foreseeable consequence of deploying or\nusing such high-risk artificial intelligence system to make a consequential\ndecision. Each risk management policy and program designed, implemented, and\nmaintained pursuant to this subsection shall be reasonable considering the\nguidance and standards set forth in the latest version of:_\n\n_1\\. The Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology;_\n\n_2\\. Standard ISO/IEC 42001 of the International Organization for\nStandardization;_\n\n_3\\. A nationally or internationally recognized risk management framework for\nartificial intelligence systems with requirements that are substantially\nequivalent to, and at least as stringent as, the requirements set forth in\nthis section; or_\n\n_4\\. Any risk management framework for artificial intelligence systems that\nthe Attorney General may designate and is substantially equivalent to, and at\nleast as stringent as, the guidance and standards described in subdivision 1._\n\n_High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_C. Except as provided in this subsection, no deployer shall deploy or use a\nhigh-risk artificial intelligence system to make a consequential decision\nunless the deployer has completed an impact assessment for such high-risk\nartificial intelligence system. The deployer shall complete an impact\nassessment for a high-risk artificial intelligence system (i) before the\ndeployer initially deploys such high-risk artificial intelligence system and\n(ii) before a significant update to such high-risk artificial intelligence\nsystem is made available._\n\n_Each impact assessment completed pursuant to this subsection shall include,\nat a minimum:_\n\n_1\\. A statement by the deployer disclosing (i) the purpose, intended use\ncases and deployment context of, and benefits afforded by the high-risk\nartificial intelligence system and (ii) whether the deployment or use of the\nhigh-risk artificial intelligence system poses any known or reasonably\nforeseeable risk of algorithmic discrimination and, if so, (a) the nature of\nsuch algorithmic discrimination and (b) the steps that have been taken, to the\nextent feasible, to mitigate such risk;_\n\n_2\\. For each post-deployment impact assessment completed pursuant to this\nsubsection, whether the intended use cases of the high-risk artificial\nintelligence system as updated were consistent with, or varied from, the\ndeveloper 's intended uses of such high-risk artificial intelligence system;_\n\n_3\\. A description of (i) the categories of data the high-risk artificial\nintelligence system processes as inputs and (ii) the outputs such high-risk\nartificial intelligence system produces;_\n\n_4\\. If the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize such high-risk artificial intelligence system;_\n\n_5\\. A list of any metrics used to evaluate the performance and known\nlimitations of the high-risk artificial intelligence system;_\n\n_6\\. A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that such high-risk artificial intelligence system is in use when\nsuch high-risk artificial intelligence system is in use;_\n\n_7\\. A description of any post-deployment monitoring performed and user\nsafeguards provided concerning such high-risk artificial intelligence system,\nincluding any oversight process established by the deployer to address issues\narising from deployment or use of such high-risk artificial intelligence\nsystem as such issues arise; and_\n\n_8\\. An analysis of such high-risk artificial intelligence system 's validity\nand reliability in accordance with standard industry practices and a\ndescription of any metrics used to evaluate the performance and known\nlimitations of such high-risk artificial intelligence system._\n\n_A single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed or used by a deployer. High-risk\nartificial intelligence systems that are in conformity with the latest version\nof the Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology, Standard ISO/IEC 42001 of the\nInternational Organization for Standardization, or another nationally or\ninternationally recognized risk management framework for artificial\nintelligence systems, or parts thereof, shall be presumed to be in conformity\nwith related requirements set out in this section and in associated\nregulations. If a deployer completes an impact assessment for the purpose of\ncomplying with another applicable law or regulation, such impact assessment\nshall be deemed to satisfy the requirements established in this subsection if\nsuch impact assessment is reasonably similar in scope and effect to the impact\nassessment that would otherwise be completed pursuant to this subsection. A\ndeployer that completes an impact assessment pursuant to this subsection shall\nmaintain such impact assessment and all records concerning such impact\nassessment for three years._\n\n_Throughout the period of time that a high-risk artificial intelligence system\nis deployed and for a period of at least three years following the final\ndeployment of such high-risk artificial intelligence system, the deployer\nshall retain all records concerning each impact assessment conducted on the\nhigh-risk artificial intelligence system, including all raw data used to\nconduct disparate impact testing._\n\n_D. Not later than the time that a deployer uses a high-risk artificial\nintelligence system to interact with a consumer, the deployer shall disclose\nto the consumer that the deployer is interacting with an artificial\nintelligence system disclosing (i) the purpose of such high-risk artificial\nintelligence system, (ii) the nature of such system, (iii) the nature of the\nconsequential decision, (iv) the contact information for the deployer, and (v)\na description of the artificial intelligence system in plain language of such\nsystem, which shall include (a) a description of the personal characteristics\nor attributes that such system will measure or assess; (b) the method by which\nthe system measures or assesses such attributes or characteristics; (c) how\nsuch attributes or characteristics are relevant to the consequential decisions\nfor which the system should be used; (d) any human components of such system;\n(e) how any automated components of such system are used to inform such\nconsequential decisions; and (f) a direct link to a publicly accessible page\non the deployer 's website that contains a plain language description of the\nlogic used in the system, including the key parameters that affect the output\nof the system; the system's outputs; the type and source of data collected\nfrom natural persons and processed by the system when it is used to make, or\nassists in making, a consequential decision; and the results of the most\nrecent impact assessment, or an active link to a webpage where a consumer can\nreview those results._\n\n_A deployer that has deployed a high-risk artificial intelligence system to\nmake a consequential decision concerning a consumer shall transmit to the\nconsumer the consequential decision within three business days of the\nconsequential decision. If such consequential decision is adverse to such\nconsumer, the deployer shall provide to the consumer (a) a statement\ndisclosing the principal reason or reasons for the consequential decision,\nincluding (1) the degree to which and manner in which the high-risk artificial\nintelligence system contributed to the consequential decision, (2) the type of\ndata that was processed by such system in making the consequential decision,\nand (3) the sources of such data; (b) pursuant to subdivision A 2 of §\n59.1-577, an opportunity to correct any inaccuracies in the consumer's\npersonal data that the high-risk artificial intelligence system processed in\nmaking, or as a substantial factor in making, the consequential decision; and\n(c) an opportunity to appeal such adverse consequential decision concerning\nthe consumer arising from the deployment of such system. Any such appeal shall\nallow for human review, if technically feasible, unless providing the\nopportunity for appeal is not in the best interest of the consumer, including\ninstances in which any delay might pose a risk to the life or safety of such\nconsumer._\n\n_E. Each deployer shall make available, in a manner that is clear and readily\navailable, a statement summarizing how such deployer manages any reasonably\nforeseeable risk of algorithmic discrimination that may arise from the use or\ndeployment of the high-risk artificial intelligence system._\n\n_F. For any disclosure required pursuant to this section, each deployer shall,\nno later than 30 days after the deployer is notified by the developer that the\ndeveloper has performed an intentional and substantial modification to any\nhigh-risk artificial intelligence system, update such disclosure as necessary\nto ensure that such disclosure remains accurate._\n\n_G. Any deployer who performs an intentional and substantial modification to\nany high-risk artificial intelligence system shall comply with the\ndocumentation and disclosure requirements for developers pursuant to\nsubsections B through G of § 59.1-608._\n\n_H. Nothing in this section shall be construed to require a deployer to\ndisclose any trade secret, as defined in § 59.1-336, information that could\ncreate a security risk, or other confidential or proprietary information._\n\n_§ 59.1-610. Operating standards for integrators of high-risk artificial\nintelligence systems._\n\n_A. On and after September 1, 2026, an integrator shall be considered a\ndeveloper of a high-risk artificial intelligence system and shall be subject\nto any obligations and operating standards of a developer pursuant to this\nchapter with respect to any substantial modifications made by the integrator\nto such high-risk artificial intelligence system. In such event, the developer\nthat initially placed the high-risk artificial intelligence system on the\nmarket shall no longer be considered a developer under this chapter with\nrespect to such substantial modifications made by the integrator._\n\n_B. On and after September 1, 2026, an integrator shall provide clear and\nconspicuous notice to the deployer of:_\n\n_1\\. The name or other identifier of the high-risk artificial intelligence\nsystem integrated into a software application provided to the deployer;_\n\n_2\\. The name and contact information of the developer of the high-risk\nartificial intelligence system integrated into a software application provided\nto the deployer;_\n\n_3\\. If the integrator has adjusted the model weights of the high-risk\nartificial intelligence system integrated into the software application by\nexposing it to additional data, a high-level summary of the adjustment process\nand how that process and the resultant system were evaluated for risk of\nalgorithmic discrimination;_\n\n_4\\. A high-level summary of any other nonsubstantial modifications made by\nthe integrator; and_\n\n_5\\. The integrator 's applicable policies regarding usage, including any\nintended limitations or recommended mitigations associated with the intended\nuse of a high-risk artificial intelligence system._\n\n_C. On and after September 1, 2026, an integrator shall provide to a deployer\nany information or documentation the developer provides to the integrator\npursuant to the developer 's obligations under subsections B and C of §\n59.1-608, provided that under no circumstance shall the integrator be\nresponsible under this section for any failure to provide documentation to a\ndeployer if that failure was caused by a developer's failure to provide the\napplicable documentation to the integrator in the first instance._\n\n_§ 59.1-611. Exemptions._\n\n_A. Nothing in this chapter shall be construed to restrict a developer 's or\ndeployer's ability to (i) comply with federal, state, or municipal ordinances\nor regulations; (ii) comply with a civil, criminal, or regulatory inquiry,\ninvestigation, subpoena, or summons by federal, state, local, or other\ngovernmental authorities; (iii) cooperate with law-enforcement agencies\nconcerning conduct or activity that the developer or deployer reasonably and\nin good faith believes may violate federal, state, or local law, ordinances,\nor regulations; (iv) investigate, establish, exercise, prepare for, or defend\nlegal claims; (v) provide a product or service specifically requested by a\nconsumer; (vi) perform under a contract to which a consumer is a party,\nincluding fulfilling the terms of a written warranty; (vii) take steps at the\nrequest of a consumer prior to entering into a contract; (viii) take immediate\nsteps to protect an interest that is essential for the life or physical safety\nof the consumer or another individual; (ix) prevent, detect, protect against,\nor respond to security incidents, identity theft, fraud, harassment, or\nmalicious or deceptive activities; (x) take actions to prevent, detect,\nprotect against, report, or respond to the production, generation,\nincorporation, or synthesization of child sex abuse material, or any illegal\nactivity, preserve the integrity or security of systems, or investigate,\nreport, or prosecute those responsible for any such action; (xi) engage in\npublic or peer-reviewed scientific or statistical research in the public\ninterest that adheres to all other applicable ethics and privacy laws and is\napproved, monitored, and governed by an institutional review board that\ndetermines, or similar independent oversight entities that determine, (a) that\nthe expected benefits of the research outweigh the risks associated with such\nresearch and (b) whether the developer or deployer has implemented reasonable\nsafeguards to mitigate the risks associated with such research; (xii) assist\nanother developer or deployer with any of the obligations imposed by this\nchapter; or (xiii) take any action that is in the public interest in the areas\nof public health, community health, or population health, but solely to the\nextent that such action is subject to suitable and specific measures to\nsafeguard the public._\n\n_B. The obligations imposed on developers or deployers by this chapter shall\nnot restrict a developer 's or deployer's ability to (i) conduct internal\nresearch to develop, improve, or repair products, services, or technologies;\n(ii) effectuate a product recall; (iii) identify and repair technical errors\nthat impair existing or intended functionality; or (iv) perform internal\noperations that are reasonably aligned with the expectations of the consumer\nor reasonably anticipated based on the consumer's existing relationship with\nthe developer or deployer._\n\n_C. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer to disclose trade secrets or information protected from\ndisclosure by state or federal law._\n\n_D. The obligations imposed on developers or deployers by this chapter shall\nnot apply where compliance by the developer or deployer with such obligations\nwould violate an evidentiary privilege under federal law or the laws of the\nCommonwealth._\n\n_E. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer that adversely affects the legally protected rights or\nfreedoms of any person, including the rights of any person to freedom of\nspeech or freedom of the press guaranteed in the First Amendment to the\nConstitution of the United States or under the Virginia Human Rights Act ( §\n2.2-3900 et seq.)._\n\n_F. The obligations imposed on developers or deployers by this chapter shall\nnot apply to any artificial intelligence system that is acquired by or for the\nfederal government or any federal agency or department, including the U.S.\nDepartment of Commerce, the U.S. Department of Defense, and the National\nAeronautics and Space Administration, unless such artificial intelligence\nsystem is a high-risk artificial intelligence system that is used to make, or\nis a substantial factor in making, a decision concerning employment or\nhousing._\n\n_G. For the purposes of this subsection:_\n\n_\" Affiliate\" means the same as that term is defined in § 6.2-899._\n\n_\" Bank\" means the same as that term is defined in § 6.2-800._\n\n_\" Credit union\" means the same as that term is defined in § 6.2-1300._\n\n_\" Federal credit union\" means a credit union duly organized under federal\nlaw._\n\n_\" Mortgage lender\" means the same as that term is defined in § 6.2-1600._\n\n_\" Out-of-state bank\" means the same as that term is defined in § 6.2-836._\n\n_\" Out-of-state credit union\" means a credit union organized and doing\nbusiness in another state._\n\n_\" Savings institution\" means the same as that term is defined in § 6.2-1100._\n\n_\" Subsidiary\" means the same as that term is defined in § 6.2-700._\n\n_The obligations imposed on developers or deployers by this chapter shall be\ndeemed satisfied for any bank, out-of-state bank, credit union, federal credit\nunion, mortgage lender, out-of-state credit union, savings institution, or any\naffiliate or subsidiary thereof if such bank, out-of-state bank, credit union,\nfederal credit union, mortgage lender, out-of-state credit union, savings\ninstitution, or affiliate or subsidiary is subject to the jurisdiction of any\nstate or federal regulator under any published guidance or regulations that\napply to the use of high-risk artificial intelligence systems and such\nguidance or regulations._\n\n_H. For purposes of this subsection, \"insurer\" means the same as that term is\ndefined in § 38.2-100._\n\n_The provisions of this chapter shall not apply to any insurer, or any high-\nrisk artificial intelligence system developed or deployed by an insurer for\nuse in the business of insurance, if such insurer is regulated and supervised\nby the State Corporation Commission or a comparable federal regulating body\nand subject to examination by such entity under any existing statutes, rules,\nor regulations pertaining to unfair trade practices and unfair discrimination\nprohibited under Chapter 5 ( § 38.2-500 et seq.) of Title 38.2, or published\nguidance or regulations that apply to the use of high-risk artificial\nintelligence systems and such guidance or regulations aid in the prevention\nand mitigation of algorithmic discrimination caused by the use of a high-risk\nartificial intelligence system or any risk of algorithmic discrimination that\nis reasonably foreseeable as a result of the use of a high-risk artificial\nintelligence system. Nothing in this chapter shall be construed to delegate\nexisting regulatory oversight of the business of insurance to any department\nor agency other than the Bureau of Insurance of the Virginia State Corporation\nCommission._\n\n_I. The provisions of this chapter shall not apply to the development of an\nartificial intelligence system that is used exclusively for research,\ntraining, testing, or other pre-deployment activities performed by active\nparticipants of any sandbox software or sandbox environment established and\nsubject to oversight by a designated agency or other government entity and\nthat is in compliance with the provisions of this chapter._\n\n_J. The provisions of this chapter shall not apply to a developer or deployer,\nor other person who develops, deploys, puts into service, or intentionally\nmodifies, as applicable, a high-risk artificial intelligence system that (i)\nhas been approved, authorized, certified, cleared, developed, or granted by a\nfederal agency acting within the scope of the federal agency 's authority, or\nby a regulated entity subject to the supervision and regulation of the Federal\nHousing Finance Agency or (ii) is in compliance with standards established by\na federal agency or by a regulated entity subject to the supervision and\nregulation of the Federal Housing Finance Agency, if the standards are\nsubstantially equivalent or more stringent than the requirements of this\nchapter._\n\n_K. The provisions of this chapter shall not apply to a developer or deployer,\nor other person that (i) facilitates or engages in the provision of telehealth\nservices, as defined in § 32.1-122.03:1, or (ii) is a covered entity within\nthe meaning of the federal Health Insurance Portability and Accountability Act\nof 1996 (42 U.S.C. § 1320d et seq.) and the regulations promulgated under such\nfederal act, as both may be amended from time to time, and is providing (a)\nhealth care recommendations that (1) are generated by an artificial\nintelligence system and (2) require a health care provider, as defined in §\n8.01-581.1, to take action to implement the recommendations or (b) services\nutilizing an artificial intelligence system for an administrative, quality\nmeasurement, security, or internal cost or performance improvement function._\n\n_L. If a developer or deployer engages in any action authorized by an\nexemption set forth in this section, the developer or deployer bears the\nburden of demonstrating that such action qualifies for such exemption._\n\n_M. If a developer or deployer withholds information pursuant to an exemption\nset forth in this chapter for which disclosure would otherwise be required by\nthis chapter, including the exemption from disclosure of trade secrets, the\ndeveloper or deployer shall notify the subject of disclosure and provide a\nbasis for withholding the information. If a developer or deployer redacts any\ninformation pursuant to an exemption from disclosure, the developer or\ndeployer shall notify the subject of disclosure that the developer or deployer\nis redacting such information and provide the basis for such decision to\nredact._\n\n_§ 59.1-612. Enforcement; civil penalties._\n\n_A. The Attorney General shall have exclusive authority to enforce the\nprovisions of this chapter._\n\n_B. Whenever the Attorney General has reasonable cause to believe that any\nperson has engaged in or is engaging in any violation of this chapter, the\nAttorney General is empowered to issue a civil investigative demand. The\nprovisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative\ndemands issued pursuant to this section. In rendering and furnishing any\ninformation requested pursuant to a civil investigative demand issued pursuant\nto this section, a developer, integrator, or deployer may redact or omit any\ntrade secrets or information protected from disclosure by state or federal\nlaw. If a developer, integrator, or deployer refuses to disclose, redacts, or\nomits information based on the exemption from disclosure of trade secrets,\nsuch developer, integrator, or deployer shall affirmatively state to the\nAttorney General that the basis for nondisclosure, redaction, or omission is\nbecause such information is a trade secret. To the extent that any information\nrequested pursuant to a civil investigative demand issued pursuant to this\nsection is subject to attorney-client privilege or work-product protection,\ndisclosure of such information pursuant to the civil investigative demand\nshall not constitute a waiver of such privilege or protection. Any\ninformation, statement, or documentation provided to the Attorney General\npursuant to this section shall be exempt from disclosure under the Virginia\nFreedom of Information Act (§ 2.2-3700 et seq.)._\n\n_C. Notwithstanding any contrary provision of law, the Attorney General may\ncause an action to be brought in the appropriate circuit court in the name of\nthe Commonwealth to enjoin any violation of this chapter. The circuit court\nhaving jurisdiction may enjoin such violation notwithstanding the existence of\nan adequate alternative remedy at law._\n\n_D. Any person who violates the provisions of this chapter shall be subject to\na civil penalty in an amount not to exceed $1,000 plus reasonable attorney\nfees, expenses, and costs, as determined by the court. Any person who\nwillfully violates the provisions of this chapter shall be subject to a civil\npenalty in an amount not less than $1,000 and not more than $10,000 plus\nreasonable attorney fees, expenses, and costs, as determined by the court.\nSuch civil penalties shall be paid into the Literary Fund._\n\n_E. Each violation of this chapter shall constitute a separate violation and\nshall be subject to any civil penalties imposed under this section._\n\n_F. The Attorney General may require that a developer disclose to the Attorney\nGeneral any statement or documentation described in this chapter if such\nstatement or documentation is relevant to an investigation conducted by the\nAttorney General. The Attorney General may also require that a deployer\ndisclose to the Attorney General any risk management policy designed and\nimplemented, impact assessment completed, or record maintained pursuant to\nthis chapter if such risk management policy, impact assessment, or record is\nrelevant to an investigation conducted by the Attorney General._\n\n_G. In an action brought by the Attorney General pursuant to this section, it\nshall be an affirmative defense that the developer, integrator, or deployer\n(i) discovers a violation of any provision of this chapter through red-\nteaming; (ii) no later than 45 days after discovering such violation (a) cures\nsuch violation and (b) provides notice to the Attorney General in a form and\nmanner as prescribed by the Attorney General that such violation has been\ncured and evidence that any harm caused by such violation has been mitigated;\nand (iii) is otherwise in compliance with the requirements of this chapter._\n\n_H. Prior to causing an action against a developer, integrator, or deployer\nfor a violation of this chapter pursuant to subsection C, the Attorney General\nshall determine, in consultation with the developer, integrator, or deployer,\nif it is possible to cure the violation. If it is possible to cure such\nviolation, the Attorney General may issue a notice of violation to the\ndeveloper, integrator, or deployer and afford the developer, integrator, or\ndeployer the opportunity to cure such violation within 45 days of the receipt\nof such notice of violation. In determining whether to grant such opportunity\nto cure such violation, the Attorney General shall consider (i) the number of\nviolations; (ii) the size and complexity of the developer, integrator, or\ndeployer; (iii) the nature and extent of the developer 's, integrator's, or\ndeployer's business; (iv) the substantial likelihood of injury to the public;\n(v) the safety of persons or property; and (vi) whether such violation was\nlikely caused by human or technical error. If the developer, integrator, or\ndeployer fails to cure such violation within 45 days of the receipt of such\nnotice of violation, the Attorney General may proceed with such action._\n\n_I. Nothing in this chapter shall create a private cause of action in favor of\nany person aggrieved by a violation of this chapter._\n\n_§ 59.1-613. Construction of chapter._\n\n_A. This chapter is declared to be remedial, with the purposes of protecting\nconsumers and ensuring consumers receive information about consequential\ndecisions affecting them. The provisions of this chapter granting rights or\nprotections to consumers shall be construed broadly and exemptions construed\nnarrowly._\n\n_B. If any provision of this chapter or its application to any person or\ncircumstance is held invalid, the invalidity shall not affect other provisions\nor applications of this chapter that can be given effect without the invalid\nprovision or application, and to this end all the provisions of this chapter\nare hereby expressly declared to be severable._\n\n2\\. That the provisions of this act shall become effective on July 1, 2026.\n\n3\\. That compliance with the provisions of Chapter 58 (§ 59.1-607 et seq.) of\nTitle 59.1 of the Code of Virginia, as created by this act, shall not (i)\nrelieve a person from liability for any causes of action that existed at\ncommon law or by statute prior to July 1, 2026, or (ii) be construed to modify\nor otherwise affect, preempt, limit, or displace any causes of action that\nexisted at common law or by statute prior to July 1, 2026.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "01/31/2025",
      "label": "Recommended as Substituted from Committee",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:VA2024000H2094&verid=VA2024000H2094_20250131_0_RS&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2024 VA H 2094</td> <td><table><tr><td class=\"label\">Author:</td> <td>Maldonado</td></tr> <tr><td class=\"label\">Version:</td> <td>Recommended as Substituted from Committee</td></tr> <tr><td class=\"label\">Version Date:</td> <td>01/31/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">HOUSE BILL NO. 2094</p>\n   <p class=\"center\">AMENDMENT IN THE NATURE OF A SUBSTITUTE</p>\n   <p class=\"center\">(Proposed by the House Committee on Appropriations</p>\n   <p class=\"center\">on January 31, 2025)</p>\n   <p class=\"center\">(Patron Prior to Substitute--Delegate Maldonado)</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"indent\">\n    <i>A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, relating to high-risk artificial intelligence; development, deployment, and use; civil penalties.</i>\n   </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">\n     <b>Be it enacted by the General Assembly of Virginia:</b>\n    </p>\n   </span>\n   <p class=\"indent\">\n    <b>1. That the Code of Virginia is amended by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as follows:</b>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">CHAPTER 58.</u>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-607. Definitions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">As used in this chapter, unless the context requires a different meaning:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Algorithmic discrimination&quot; means the use of an artificial intelligence system that results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, sexual orientation, veteran status, or other classification protected under state or federal law. &quot;Algorithmic discrimination&quot; does not include (i) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; (ii) the expansion of an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or (iii) an act or omission by or on behalf of a private club or other establishment not in fact open to the public, as set forth in Title II of the Civil Rights Act of 1964, 42 U.S.C. &sect; 2000a(e), as amended from time to time.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Artificial intelligence system&quot; means any machine learning-based system that, for any explicit or implicit objective, infers from the inputs such system receives how to generate outputs, including content, decisions, predictions, and recommendations, that can influence physical or virtual environments. &quot;Artificial intelligence system&quot; does not include any artificial intelligence system or general purpose artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence system or general purpose artificial intelligence model is released on the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consequential decision&quot; means any decision that has a material legal, or similarly significant, effect on the provision or denial to any consumer of (i) parole, probation, a pardon, or any other release from incarceration or court supervision; (ii) education enrollment or an education opportunity; (iii) access to employment; (iv) a financial or lending service; (v) access to health care services; (vi) housing; (vii) insurance; (viii) marital status; or (ix) a legal service.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consumer&quot; means a natural person who is a resident of the Commonwealth and is acting only in an individual or household context. &quot;Consumer&quot; does not include a natural person acting in a commercial or employment context.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Deployer&quot; means any person doing business in the Commonwealth that deploys or uses a high-risk artificial intelligence system to make a consequential decision in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Developer&quot; means any person doing business in the Commonwealth that develops or intentionally and substantially modifies a high-risk artificial intelligence system that is offered, sold, leased, given, or otherwise provided to consumers in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;General-purpose artificial intelligence model&quot; means a model used by an artificial intelligence system or other system that (i) displays significant generality, (ii) is capable of competently performing a wide range of distinct tasks, and (iii) can be integrated into a variety of downstream applications or systems. &quot;General-purpose artificial intelligence model&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is released on the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence&quot; means an artificial intelligence system that is capable of producing and used to produce synthetic content, including audio, images, text, and videos.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence system&quot; means any artificial intelligence system or service that incorporates generative artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that is specifically intended to autonomously make, or be a substantial factor in making, a consequential decision. A system or service is not a &quot;high-risk artificial intelligence system&quot; if it is intended to (i) perform a narrow procedural task, (ii) improve the result of a previously completed human activity, (iii) detect any decision-making patterns or any deviations from pre-existing decision-making patterns, or (iv) perform a preparatory task to an assessment relevant to a consequential decision. &quot;High-risk artificial intelligence system&quot; does not include any of the following technologies:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. Anti-fraud technology that does not use facial recognition technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Anti-malware technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Anti-virus technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Artificial intelligence-enabled video games;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. Calculators;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. Cybersecurity technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. Databases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. Data storage;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. Firewall technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. Internet domain registration;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. Internet website loading;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. Networking;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. Spam and robocall filtering;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. Spell-checking technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. Spreadsheets;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">16. Web caching;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">17. Web hosting or any similar technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">18. Autonomous vehicle technology; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">19. Technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an acceptable use policy that prohibits generating content that is discriminatory or unlawful.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Intentional and substantial modification&quot; means any deliberate change made to (i) an artificial intelligence system that results, at the time when the change is implemented and any time thereafter, in any new material risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects compliance of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. &quot;Intentional and substantial modification&quot; does not include (a) any customization made by deployers based on legitimate nondiscriminatory business justifications and within the scope and purpose of the artificial intelligence tool; (b) any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (1) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (2) such change (A) is made to such high-risk artificial intelligence system as a result of any learning described in clause (1) and (B) was predetermined by the deployer or the third party contracted by the deployer and included within the initial impact assessment of such high-risk artificial intelligence system as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Machine learning&quot; means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Person&quot; includes any individual, corporation, partnership, association, cooperative, limited liability company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. &quot;Person&quot; does not include any government or political subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Principal basis&quot; means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Red-teaming&quot; means adversarial testing to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Substantial factor&quot; means a factor that is (i) the principal basis for making a consequential decision, (ii) capable of altering the outcome of a consequential decision, and (iii) generated by an artificial intelligence system. &quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as the principal basis to make a consequential decision concerning the consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Synthetic content&quot; means information, such as images, video, audio clips, and text, that has been significantly modified or generated by algorithms, including by artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique, or process, that (i) derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by, other persons who can obtain economic value from its disclosure or use and (ii) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-608. Operating standards for developers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each developer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a developer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the developer complied with the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No developer of a high-risk artificial intelligence system shall offer, sell, lease, give, or otherwise provide to a deployer or other developer a high-risk artificial intelligence system unless the developer makes available to the deployer or other developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement disclosing the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Documentation disclosing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">a. The known or reasonably known limitations of such high-risk artificial intelligence system, including any and all known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">b. The purpose of such high-risk artificial intelligence system and the intended benefits and uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">c. A summary describing how such high-risk artificial intelligence system was evaluated for performance before such high-risk artificial intelligence system was licensed, sold, leased, given, or otherwise made available to a deployer or other developer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">d. The measures the developer has taken to mitigate reasonable foreseeable risks of algorithmic discrimination that the developer knows arises from deployment or use of such high-risk artificial intelligence system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">e. How an individual can use such high-risk artificial intelligence system and monitor the performance of such high-risk artificial intelligence system for any risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Documentation including a (i) description of how the high-risk artificial intelligence system was evaluated for performance and for mitigation of algorithmic discrimination before such system was made available to the deployer or other developer; (ii) description of the intended outputs of the high-risk artificial intelligence system; (iii) description of the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and (iv) description of how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when such system is used to make, or is a substantial factor in making, a consequential decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any additional documentation that is reasonably necessary to assist the deployer or other developer in understanding the outputs and monitoring performance of the high-risk artificial intelligence system for risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Each developer that offers, sells, leases, gives, or otherwise makes available to a deployer or other developer a high-risk artificial intelligence system shall make available to the deployer or other developer to the extent feasible and necessary, information and documentation through artifacts such as system cards or predeployment impact assessments, including any risk management policy designed and implemented and any relevant impact assessment completed, and such documentation and information shall enable the deployer, other developer, or a third party contracted by the deployer to complete an impact assessment as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. A developer that also serves as a deployer for any high-risk artificial intelligence system shall not be required to generate the documentation required by this section unless such high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer or as otherwise required by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this section shall be construed to require a developer to disclose any trade secret, as defined in &sect; 59.1-336, information that could create a security risk, or other confidential or proprietary information.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For any disclosure required pursuant to this section, each developer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. 1. Each developer of a high-risk generative artificial intelligence system that generates or modifies synthetic content shall ensure that the outputs of such high-risk artificial intelligence system (i) are identifiable and detectable in a manner that is accessible by consumers; (ii) comply with any applicable accessibility requirements, as synthetic content; and (iii) such marking must be applied at the time the output is generated;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. If such synthetic content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for marking outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The marking of outputs required by subdivision 1 shall not apply to (i) synthetic content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Where multiple developers directly contribute to the development of a high-risk artificial intelligence system, each developer shall be subject to the obligations and operating standards applicable to developers pursuant to this section solely with respect to its activities contributing to the development of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-609. Operating standards for deployers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each deployer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a deployer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the deployer complied with the provisions of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has designed and implemented a risk management policy and program for such high-risk artificial intelligence system. The risk management policy shall specify the principles, processes, and personnel that the deployer shall use in maintaining the risk management program to identify, mitigate, and document any risk of algorithmic discrimination that is a reasonably foreseeable consequence of deploying or using such high-risk artificial intelligence system to make a consequential decision. Each risk management policy and program designed, implemented, and maintained pursuant to this subsection shall be reasonable considering the guidance and standards set forth in the latest version of:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. The Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Standard ISO/IEC 42001 of the International Organization for Standardization;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A nationally or internationally recognized risk management framework for artificial intelligence systems with requirements that are substantially equivalent to, and at least as stringent as, the requirements set forth in this section; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any risk management framework for artificial intelligence systems that the Attorney General may designate and is substantially equivalent to, and at least as stringent as, the guidance and standards described in subdivision 1.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Except as provided in this subsection, no deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has completed an impact assessment for such high-risk artificial intelligence system. The deployer shall complete an impact assessment for a high-risk artificial intelligence system (i) before the deployer initially deploys such high-risk artificial intelligence system and (ii) before a significant update to such high-risk artificial intelligence system is used to make a consequential decision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each impact assessment completed pursuant to this subsection shall include, at a minimum:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement by the deployer disclosing (i) the purpose, intended use cases and deployment context of, and benefits afforded by the high-risk artificial intelligence system and (ii) whether the deployment or use of the high-risk artificial intelligence system poses any known or reasonably foreseeable risk of algorithmic discrimination and, if so, (a) the nature of such algorithmic discrimination and (b) the steps that have been taken, to the extent feasible, to mitigate such risk;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. For each post-deployment impact assessment completed pursuant to this subsection, whether the intended use cases of the high-risk artificial intelligence system as updated were consistent with, or varied from, the developer&#39;s intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A description of (i) the categories of data the high-risk artificial intelligence system processes as inputs and (ii) the outputs such high-risk artificial intelligence system produces;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. If the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. A list of any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. A description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that such high-risk artificial intelligence system is in use when such high-risk artificial intelligence system is in use;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. A description of any post-deployment monitoring performed and user safeguards provided concerning such high-risk artificial intelligence system, including any oversight process established by the deployer to address issues arising from deployment or use of such high-risk artificial intelligence system as such issues arise; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. An analysis of such high-risk artificial intelligence system&#39;s validity and reliability in accordance with standard industry practices and a description of any metrics used to evaluate the performance and known limitations of such high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed or used by a deployer. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations. If a deployer completes an impact assessment for the purpose of complying with another applicable law or regulation, such impact assessment shall be deemed to satisfy the requirements established in this subsection if such impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection. A deployer that completes an impact assessment pursuant to this subsection shall maintain such impact assessment and all records concerning such impact assessment for three years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Throughout the period of time that a high-risk artificial intelligence system is deployed and for a period of at least three years following the final deployment of such high-risk artificial intelligence system, the deployer shall retain all records concerning each impact assessment conducted on the high-risk artificial intelligence system, including all raw data used to evaluate the performance and known limitations of such system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Not later than the time that a deployer uses a high-risk artificial intelligence system to interact with a consumer, the deployer shall disclose to the consumer that the deployer is interacting with an artificial intelligence system disclosing (i) the purpose of such high-risk artificial intelligence system, (ii) the nature of such system, (iii) the nature of the consequential decision, (iv) the contact information for the deployer, and (v) a description of the artificial intelligence system in plain language of such system, which shall include (a) a description of the personal characteristics or attributes that such system will measure or assess, (b) the method by which the system measures or assesses such attributes or characteristics, (c) how such attributes or characteristics are relevant to the consequential decisions for which the system should be used, (d) any human components of such system, and (e) how any automated components of such system are used to inform such consequential decisions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A deployer that has deployed a high-risk artificial intelligence system to make a consequential decision concerning a consumer shall transmit to the consumer the consequential decision without undue delay. If such consequential decision is adverse to such consumer and based on personal data beyond information that the consumer provided directly to the deployer, the deployer shall provide to the consumer (a) a statement disclosing the principal reason or reasons for the consequential decision, including (1) the degree to which and manner in which the high-risk artificial intelligence system contributed to the consequential decision, (2) the type of data that was processed by such system in making the consequential decision, and (3) the sources of such data; (b) pursuant to subdivision A 2 of &sect; 59.1-577, an opportunity to correct any inaccuracies in the consumer&#39;s personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and (c) an opportunity to appeal such adverse consequential decision concerning the consumer arising from the deployment of such system. Any such appeal shall allow for human review, if technically reasonable and practicable, unless providing the opportunity for appeal is not in the best interest of the consumer, including instances in which any delay might pose a risk to the life or safety of such consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each deployer shall make available, in a manner that is clear and readily available, a statement summarizing how such deployer manages any reasonably foreseeable risk of algorithmic discrimination that may arise from the use or deployment of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each deployer shall, no later than 30 days after the deployer is notified by the developer that the developer has performed an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. Any deployer who performs an intentional and substantial modification to any high-risk artificial intelligence system shall comply with the documentation and disclosure requirements for developers pursuant to subsections B through G of &sect; 59.1-608.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Nothing in this section shall be construed to require a deployer to disclose any trade secret, as defined in &sect; 59.1-336, information that could create a security risk, or other confidential or proprietary information.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-610. Exemptions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Nothing in this chapter shall be construed to restrict a developer&#39;s or deployer&#39;s ability to (i) comply with federal, state, or municipal ordinances or regulations; (ii) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by federal, state, local, or other governmental authorities; (iii) cooperate with law-enforcement agencies concerning conduct or activity that the developer or deployer reasonably and in good faith believes may violate federal, state, or local law, ordinances, or regulations; (iv) investigate, establish, exercise, prepare for, or defend legal claims; (v) provide a product or service specifically requested by a consumer; (vi) perform under a contract to which a consumer is a party, including fulfilling the terms of a written warranty; (vii) take steps at the request of a consumer prior to entering into a contract; (viii) take immediate steps to protect an interest that is essential for the life or physical safety of the consumer or another individual; (ix) prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, or malicious or deceptive activities; (x) take actions to prevent, detect, protect against, report, or respond to the production, generation, incorporation, or synthesization of child sex abuse material, or any illegal activity, preserve the integrity or security of systems, or investigate, report, or prosecute those responsible for any such action; (xi) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is approved, monitored, and governed by an institutional review board that determines, or similar independent oversight entities that determine, (a) that the expected benefits of the research outweigh the risks associated with such research and (b) whether the developer or deployer has implemented reasonable safeguards to mitigate the risks associated with such research; (xii) assist another developer or deployer with any of the obligations imposed by this chapter; or (xiii) take any action that is in the public interest in the areas of public health, community health, or population health, but solely to the extent that such action is subject to suitable and specific measures to safeguard the public.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. The obligations imposed on developers or deployers by this chapter shall not restrict a developer&#39;s or deployer&#39;s ability to (i) conduct internal research to develop, improve, or repair products, services, or technologies; (ii) effectuate a product recall; (iii) identify and repair technical errors that impair existing or intended functionality; or (iv) perform internal operations that are reasonably aligned with the expectations of the consumer or reasonably anticipated based on the consumer&#39;s existing relationship with the developer or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer to disclose trade secrets or information protected from disclosure by state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. The obligations imposed on developers or deployers by this chapter shall not apply where compliance by the developer or deployer with such obligations would violate an evidentiary privilege under federal law or the laws of the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer that adversely affects the legally protected rights or freedoms of any person, including the rights of any person to freedom of speech or freedom of the press guaranteed in the First Amendment to the Constitution of the United States or under the Virginia Human Rights Act (&sect; 2.2-3900 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The obligations imposed on developers or deployers by this chapter shall not apply to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the U.S. Department of Commerce, the U.S. Department of Defense, and the National Aeronautics and Space Administration, unless such artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For the purposes of this subsection:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Affiliate&quot; means the same as that term is defined in &sect; 6.2-899.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Bank&quot; means the same as that term is defined in &sect; 6.2-800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Credit union&quot; means the same as that term is defined in &sect; 6.2-1300.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Federal credit union&quot; means a credit union duly organized under federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Mortgage lender&quot; means the same as that term is defined in &sect; 6.2-1600.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state bank&quot; means the same as that term is defined in &sect; 6.2-836.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state credit union&quot; means a credit union organized and doing business in another state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Savings institution&quot; means the same as that term is defined in &sect; 6.2-1100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Subsidiary&quot; means the same as that term is defined in &sect; 6.2-700.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The obligations imposed on developers or deployers by this chapter shall be deemed satisfied for any bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or any affiliate or subsidiary thereof if such bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or affiliate or subsidiary is subject to the jurisdiction of any state or federal regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. For purposes of this subsection, &quot;insurer&quot; means the same as that term is defined in &sect; 38.2-100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The provisions of this chapter shall not apply to any insurer, or any high-risk artificial intelligence system developed by or for or deployed by an insurer for use in the business of insurance, if such insurer is regulated and supervised by the State Corporation Commission or a comparable federal regulating body and subject to examination by such entity under any existing statutes, rules, or regulations pertaining to unfair trade practices and unfair discrimination prohibited under Chapter 5 (&sect; 38.2-500 et seq.) of Title 38.2, or published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations aid in the prevention and mitigation of algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. Nothing in this chapter shall be construed to delegate existing regulatory oversight of the business of insurance to any department or agency other than the Bureau of Insurance of the Virginia State Corporation Commission.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. The provisions of this chapter shall not apply to the development of an artificial intelligence system that is used exclusively for research, training, testing, or other pre-deployment activities performed by active participants of any sandbox software or sandbox environment established and subject to oversight by a designated agency or other government entity and that is in compliance with the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">J. The provisions of this chapter shall not apply to a developer or deployer, or other person who develops, deploys, puts into service, or intentionally modifies, as applicable, a high-risk artificial intelligence system that (i) has been approved, authorized, certified, cleared, developed, or granted by a federal agency acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency or (ii) is in compliance with standards established by a federal agency or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">K. The provisions of this chapter shall not apply to a developer or deployer, or other person that (i) facilitates or engages in the provision of telehealth services, as defined in &sect; 32.1-122.03:1, or (ii) is a covered entity within the meaning of the federal Health Insurance Portability and Accountability Act of 1996 (42 U.S.C. &sect; 1320d et seq.) and the regulations promulgated under such federal act, as both may be amended from time to time, and is providing (a) health care recommendations that (1) are generated by an artificial intelligence system and (2) require a health care provider, as defined in &sect; 8.01-581.1, to take action to implement the recommendations or (b) services utilizing an artificial intelligence system for an administrative, quality measurement, security, or internal cost or performance improvement function.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">L. If a developer or deployer engages in any action authorized by an exemption set forth in this section, the developer or deployer bears the burden of demonstrating that such action qualifies for such exemption.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">M. If a developer or deployer withholds information pursuant to an exemption set forth in this chapter for which disclosure would otherwise be required by this chapter, including the exemption from disclosure of trade secrets, the developer or deployer shall notify the subject of disclosure and provide a basis for withholding the information. If a developer or deployer redacts any information pursuant to an exemption from disclosure, the developer or deployer shall notify the subject of disclosure that the developer or deployer is redacting such information and provide the basis for such decision to redact.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-611. Enforcement; civil penalties.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. The Attorney General shall have exclusive authority to enforce the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any violation of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of &sect; 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. If a developer or deployer refuses to disclose, redacts, or omits information based on the exemption from disclosure of trade secrets, such developer or deployer shall affirmatively state to the Attorney General that the basis for nondisclosure, redaction, or omission is because such information is a trade secret. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand shall not constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be exempt from disclosure under the Virginia Freedom of Information Act (&sect; 2.2-3700 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any violation of this chapter. The circuit court having jurisdiction may enjoin such violation notwithstanding the existence of an adequate alternative remedy at law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Any person who violates the provisions of this chapter shall be subject to a civil penalty in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil penalty in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil penalties shall be paid into the Literary Fund.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each violation of this chapter shall constitute a separate violation and shall be subject to any civil penalties imposed under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The Attorney General may require that a developer disclose to the Attorney General any statement or documentation described in this chapter if such statement or documentation is relevant to an investigation conducted by the Attorney General. The Attorney General may also require that a deployer disclose to the Attorney General any risk management policy designed and implemented, impact assessment completed, or record maintained pursuant to this chapter if such risk management policy, impact assessment, or record is relevant to an investigation conducted by the Attorney General.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. In an action brought by the Attorney General pursuant to this section, it shall be an affirmative defense that the developer or deployer (i) discovers a violation of any provision of this chapter through red-teaming or other method; (ii) no later than 45 days after discovering such violation (a) cures such violation and (b) provides notice to the Attorney General in a form and manner as prescribed by the Attorney General that such violation has been cured and evidence that any harm caused by such violation has been mitigated; and (iii) is otherwise in compliance with the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Prior to causing an action against a developer or deployer for a violation of this chapter pursuant to subsection C, the Attorney General shall determine, in consultation with the developer or deployer, if it is possible to cure the violation. If it is possible to cure such violation, the Attorney General may issue a notice of violation to the developer or deployer and afford the developer or deployer the opportunity to cure such violation within 45 days of the receipt of such notice of violation. In determining whether to grant such opportunity to cure such violation, the Attorney General shall consider (i) the number of violations; (ii) the size and complexity of the developer or deployer; (iii) the nature and extent of the developer&#39;s or deployer&#39;s business; (iv) the substantial likelihood of injury to the public; (v) the safety of persons or property; and (vi) whether such violation was likely caused by human or technical error. If the developer or deployer fails to cure such violation within 45 days of the receipt of such notice of violation, the Attorney General may proceed with such action.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Nothing in this chapter shall create a private cause of action in favor of any person aggrieved by a violation of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-612. Construction of chapter.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. This chapter is declared to be remedial, with the purposes of protecting consumers and ensuring consumers receive information about consequential decisions affecting them. The provisions of this chapter granting rights or protections to consumers shall be construed broadly and exemptions construed narrowly.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. If any provision of this chapter or its application to any person or circumstance is held invalid, the invalidity shall not affect other provisions or applications of this chapter that can be given effect without the invalid provision or application, and to this end all the provisions of this chapter are hereby expressly declared to be severable.</u>\n   </p>\n   <effective_clause>\n    <p class=\"indent\">\n     <b>2. That the provisions of this act shall become effective on July 1, 2026.</b>\n    </p>\n   </effective_clause>\n   <p class=\"indent\">\n    <b>3. That compliance with the provisions of Chapter 58 (&sect; 59.1-607 et seq.) of Title 59.1 of the Code of Virginia, as created by this act, shall not (i) relieve a person from liability for any causes of action that existed at common law or by statute prior to July 1, 2026, or (ii) be construed to modify or otherwise affect, preempt, limit, or displace any causes of action that existed at common law or by statute prior to July 1, 2026.</b>\n   </p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2024 VA H 2094 | | Author: | Maldonado  \n---|---  \nVersion: | Recommended as Substituted from Committee  \nVersion Date: | 01/31/2025  \n  \nHOUSE BILL NO. 2094\n\nAMENDMENT IN THE NATURE OF A SUBSTITUTE\n\n(Proposed by the House Committee on Appropriations\n\non January 31, 2025)\n\n(Patron Prior to Substitute--Delegate Maldonado)\n\n_A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612,\nrelating to high-risk artificial intelligence; development, deployment, and\nuse; civil penalties._\n\n**Be it enacted by the General Assembly of Virginia:**\n\n**1\\. That the Code of Virginia is amended by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as\nfollows:**\n\n_CHAPTER 58._\n\n_HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT._\n\n**_§ 59.1-607. Definitions._ **\n\n_As used in this chapter, unless the context requires a different meaning:_\n\n_\" Algorithmic discrimination\" means the use of an artificial intelligence\nsystem that results in an unlawful differential treatment or impact that\ndisfavors an individual or group of individuals on the basis of their actual\nor perceived age, color, disability, ethnicity, genetic information, limited\nproficiency in the English language, national origin, race, religion,\nreproductive health, sex, sexual orientation, veteran status, or other\nclassification protected under state or federal law. \"Algorithmic\ndiscrimination\" does not include (i) the offer, license, or use of a high-risk\nartificial intelligence system by a developer or deployer for the sole purpose\nof the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; (ii) the expansion of an applicant, customer, or participant pool to\nincrease diversity or redress historical discrimination; or (iii) an act or\nomission by or on behalf of a private club or other establishment not in fact\nopen to the public, as set forth in Title II of the Civil Rights Act of 1964,\n42 U.S.C. § 2000a(e), as amended from time to time._\n\n_\" Artificial intelligence system\" means any machine learning-based system\nthat, for any explicit or implicit objective, infers from the inputs such\nsystem receives how to generate outputs, including content, decisions,\npredictions, and recommendations, that can influence physical or virtual\nenvironments. \"Artificial intelligence system\" does not include any artificial\nintelligence system or general purpose artificial intelligence model that is\nused for development, prototyping, and research activities before such\nartificial intelligence system or general purpose artificial intelligence\nmodel is released on the market._\n\n_\" Consequential decision\" means any decision that has a material legal, or\nsimilarly significant, effect on the provision or denial to any consumer of\n(i) parole, probation, a pardon, or any other release from incarceration or\ncourt supervision; (ii) education enrollment or an education opportunity;\n(iii) access to employment; (iv) a financial or lending service; (v) access to\nhealth care services; (vi) housing; (vii) insurance; (viii) marital status; or\n(ix) a legal service._\n\n_\" Consumer\" means a natural person who is a resident of the Commonwealth and\nis acting only in an individual or household context. \"Consumer\" does not\ninclude a natural person acting in a commercial or employment context._\n\n_\" Deployer\" means any person doing business in the Commonwealth that deploys\nor uses a high-risk artificial intelligence system to make a consequential\ndecision in the Commonwealth._\n\n_\" Developer\" means any person doing business in the Commonwealth that\ndevelops or intentionally and substantially modifies a high-risk artificial\nintelligence system that is offered, sold, leased, given, or otherwise\nprovided to consumers in the Commonwealth._\n\n_\" General-purpose artificial intelligence model\" means a model used by an\nartificial intelligence system or other system that (i) displays significant\ngenerality, (ii) is capable of competently performing a wide range of distinct\ntasks, and (iii) can be integrated into a variety of downstream applications\nor systems. \"General-purpose artificial intelligence model\" does not include\nany artificial intelligence model that is used for development, prototyping,\nand research activities before such artificial intelligence model is released\non the market._\n\n_\" Generative artificial intelligence\" means an artificial intelligence system\nthat is capable of producing and used to produce synthetic content, including\naudio, images, text, and videos._\n\n_\" Generative artificial intelligence system\" means any artificial\nintelligence system or service that incorporates generative artificial\nintelligence._\n\n_\" High-risk artificial intelligence system\" means any artificial intelligence\nsystem that is specifically intended to autonomously make, or be a substantial\nfactor in making, a consequential decision. A system or service is not a\n\"high-risk artificial intelligence system\" if it is intended to (i) perform a\nnarrow procedural task, (ii) improve the result of a previously completed\nhuman activity, (iii) detect any decision-making patterns or any deviations\nfrom pre-existing decision-making patterns, or (iv) perform a preparatory task\nto an assessment relevant to a consequential decision. \"High-risk artificial\nintelligence system\" does not include any of the following technologies:_\n\n_1\\. Anti-fraud technology that does not use facial recognition technology;_\n\n_2\\. Anti-malware technology;_\n\n_3\\. Anti-virus technology;_\n\n_4\\. Artificial intelligence-enabled video games;_\n\n_5\\. Calculators;_\n\n_6\\. Cybersecurity technology;_\n\n_7\\. Databases;_\n\n_8\\. Data storage;_\n\n_9\\. Firewall technology;_\n\n_10\\. Internet domain registration;_\n\n_11\\. Internet website loading;_\n\n_12\\. Networking;_\n\n_13\\. Spam and robocall filtering;_\n\n_14\\. Spell-checking technology;_\n\n_15\\. Spreadsheets;_\n\n_16\\. Web caching;_\n\n_17\\. Web hosting or any similar technology;_\n\n_18\\. Autonomous vehicle technology; or_\n\n_19\\. Technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an acceptable use\npolicy that prohibits generating content that is discriminatory or unlawful._\n\n_\" Intentional and substantial modification\" means any deliberate change made\nto (i) an artificial intelligence system that results, at the time when the\nchange is implemented and any time thereafter, in any new material risk of\nalgorithmic discrimination or (ii) a general-purpose artificial intelligence\nmodel that affects compliance of the general-purpose artificial intelligence\nmodel, materially changes the purpose of the general-purpose artificial\nintelligence model, or results in any new reasonably foreseeable risk of\nalgorithmic discrimination. \"Intentional and substantial modification\" does\nnot include (a) any customization made by deployers based on legitimate\nnondiscriminatory business justifications and within the scope and purpose of\nthe artificial intelligence tool; (b) any change made to a high-risk\nartificial intelligence system, or the performance of a high-risk artificial\nintelligence system, if (1) the high-risk artificial intelligence system\ncontinues to learn after such high-risk artificial intelligence system is\noffered, sold, leased, licensed, given, or otherwise made available to a\ndeployer, or deployed, and (2) such change (A) is made to such high-risk\nartificial intelligence system as a result of any learning described in clause\n(1) and (B) was predetermined by the deployer or the third party contracted by\nthe deployer and included within the initial impact assessment of such high-\nrisk artificial intelligence system as required in § 59.1-609._\n\n_\" Machine learning\" means the development of algorithms to build data-derived\nstatistical models that are capable of drawing inferences from previously\nunseen data without explicit human instruction._\n\n_\" Person\" includes any individual, corporation, partnership, association,\ncooperative, limited liability company, trust, joint venture, or any other\nlegal or commercial entity and any successor, representative, agent, agency,\nor instrumentality thereof. \"Person\" does not include any government or\npolitical subdivision._\n\n_\" Principal basis\" means the use of an output of a high-risk artificial\nintelligence system to make a decision without (i) human review, oversight,\ninvolvement, or intervention or (ii) meaningful consideration by a human._\n\n_\" Red-teaming\" means adversarial testing to identify the potential adverse\nbehaviors or outcomes of an artificial intelligence system, identify how such\nbehaviors or outcomes occur, and stress test the safeguards against such\nbehaviors or outcomes._\n\n_\" Substantial factor\" means a factor that is (i) the principal basis for\nmaking a consequential decision, (ii) capable of altering the outcome of a\nconsequential decision, and (iii) generated by an artificial intelligence\nsystem. \"Substantial factor\" includes any use of an artificial intelligence\nsystem to generate any content, decision, prediction, or recommendation\nconcerning a consumer that is used as the principal basis to make a\nconsequential decision concerning the consumer._\n\n_\" Synthetic content\" means information, such as images, video, audio clips,\nand text, that has been significantly modified or generated by algorithms,\nincluding by artificial intelligence._\n\n_\" Trade secret\" means information, including a formula, pattern, compilation,\nprogram, device, method, technique, or process, that (i) derives independent\neconomic value, actual or potential, from not being generally known to, and\nnot being readily ascertainable by proper means by, other persons who can\nobtain economic value from its disclosure or use and (ii) is the subject of\nefforts that are reasonable under the circumstances to maintain its secrecy._\n\n**_§ 59.1-608. Operating standards for developers of high-risk artificial\nintelligence systems._ **\n\n_A. Each developer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-611, there shall be a\nrebuttable presumption that a developer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if the\ndeveloper complied with the requirements of this section._\n\n_B. No developer of a high-risk artificial intelligence system shall offer,\nsell, lease, give, or otherwise provide to a deployer or other developer a\nhigh-risk artificial intelligence system unless the developer makes available\nto the deployer or other developer:_\n\n_1\\. A statement disclosing the intended uses of such high-risk artificial\nintelligence system;_\n\n_2\\. Documentation disclosing the following:_\n\n_a. The known or reasonably known limitations of such high-risk artificial\nintelligence system, including any and all known or reasonably foreseeable\nrisks of algorithmic discrimination arising from the intended uses of such\nhigh-risk artificial intelligence system;_\n\n_b. The purpose of such high-risk artificial intelligence system and the\nintended benefits and uses of such high-risk artificial intelligence system;_\n\n_c. A summary describing how such high-risk artificial intelligence system was\nevaluated for performance before such high-risk artificial intelligence system\nwas licensed, sold, leased, given, or otherwise made available to a deployer\nor other developer;_\n\n_d. The measures the developer has taken to mitigate reasonable foreseeable\nrisks of algorithmic discrimination that the developer knows arises from\ndeployment or use of such high-risk artificial intelligence system; and_\n\n_e. How an individual can use such high-risk artificial intelligence system\nand monitor the performance of such high-risk artificial intelligence system\nfor any risk of algorithmic discrimination;_\n\n_3\\. Documentation including a (i) description of how the high-risk artificial\nintelligence system was evaluated for performance and for mitigation of\nalgorithmic discrimination before such system was made available to the\ndeployer or other developer; (ii) description of the intended outputs of the\nhigh-risk artificial intelligence system; (iii) description of the measures\nthe developer has taken to mitigate known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the reasonably foreseeable\ndeployment of the high-risk artificial intelligence system; and (iv)\ndescription of how the high-risk artificial intelligence system should be\nused, not be used, and be monitored by an individual when such system is used\nto make, or is a substantial factor in making, a consequential decision; and_\n\n_4\\. Any additional documentation that is reasonably necessary to assist the\ndeployer or other developer in understanding the outputs and monitoring\nperformance of the high-risk artificial intelligence system for risks of\nalgorithmic discrimination._\n\n_C. Each developer that offers, sells, leases, gives, or otherwise makes\navailable to a deployer or other developer a high-risk artificial intelligence\nsystem shall make available to the deployer or other developer to the extent\nfeasible and necessary, information and documentation through artifacts such\nas system cards or predeployment impact assessments, including any risk\nmanagement policy designed and implemented and any relevant impact assessment\ncompleted, and such documentation and information shall enable the deployer,\nother developer, or a third party contracted by the deployer to complete an\nimpact assessment as required in § 59.1-609._\n\n_D. A developer that also serves as a deployer for any high-risk artificial\nintelligence system shall not be required to generate the documentation\nrequired by this section unless such high-risk artificial intelligence system\nis provided to an unaffiliated entity acting as a deployer or as otherwise\nrequired by law._\n\n_E. Nothing in this section shall be construed to require a developer to\ndisclose any trade secret, as defined in § 59.1-336, information that could\ncreate a security risk, or other confidential or proprietary information._\n\n_F. High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_G. For any disclosure required pursuant to this section, each developer\nshall, no later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_H. 1. Each developer of a high-risk generative artificial intelligence system\nthat generates or modifies synthetic content shall ensure that the outputs of\nsuch high-risk artificial intelligence system (i) are identifiable and\ndetectable in a manner that is accessible by consumers; (ii) comply with any\napplicable accessibility requirements, as synthetic content; and (iii) such\nmarking must be applied at the time the output is generated;_\n\n_2\\. If such synthetic content is an audio, image, or video format that forms\npart of an evidently artistic, creative, satirical, fictional analogous work\nor program, such requirement for marking outputs of high-risk artificial\nintelligence systems pursuant to subdivision 1 shall be limited to a manner\nthat does not hinder the display or enjoyment of such work or program._\n\n_3\\. The marking of outputs required by subdivision 1 shall not apply to (i)\nsynthetic content that consists exclusively of text, is published to inform\nthe public on any matter of public interest, or is unlikely to mislead a\nreasonable person consuming such synthetic content or (ii) the outputs of a\nhigh-risk artificial intelligence system that performs an assistive function\nfor standard editing, does not substantially alter the input data provided by\nthe developer, or is used to detect, prevent, investigate, or prosecute any\ncrime as authorized by law._\n\n_I. Where multiple developers directly contribute to the development of a\nhigh-risk artificial intelligence system, each developer shall be subject to\nthe obligations and operating standards applicable to developers pursuant to\nthis section solely with respect to its activities contributing to the\ndevelopment of the high-risk artificial intelligence system._\n\n**_§ 59.1-609. Operating standards for deployers of high-risk artificial\nintelligence systems._ **\n\n_A. Each deployer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-611, there shall be a\nrebuttable presumption that a deployer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if the\ndeployer complied with the provisions of this section._\n\n_B. No deployer shall deploy or use a high-risk artificial intelligence system\nto make a consequential decision unless the deployer has designed and\nimplemented a risk management policy and program for such high-risk artificial\nintelligence system. The risk management policy shall specify the principles,\nprocesses, and personnel that the deployer shall use in maintaining the risk\nmanagement program to identify, mitigate, and document any risk of algorithmic\ndiscrimination that is a reasonably foreseeable consequence of deploying or\nusing such high-risk artificial intelligence system to make a consequential\ndecision. Each risk management policy and program designed, implemented, and\nmaintained pursuant to this subsection shall be reasonable considering the\nguidance and standards set forth in the latest version of:_\n\n_1\\. The Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology;_\n\n_2\\. Standard ISO/IEC 42001 of the International Organization for\nStandardization;_\n\n_3\\. A nationally or internationally recognized risk management framework for\nartificial intelligence systems with requirements that are substantially\nequivalent to, and at least as stringent as, the requirements set forth in\nthis section; or_\n\n_4\\. Any risk management framework for artificial intelligence systems that\nthe Attorney General may designate and is substantially equivalent to, and at\nleast as stringent as, the guidance and standards described in subdivision 1._\n\n_High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_C. Except as provided in this subsection, no deployer shall deploy or use a\nhigh-risk artificial intelligence system to make a consequential decision\nunless the deployer has completed an impact assessment for such high-risk\nartificial intelligence system. The deployer shall complete an impact\nassessment for a high-risk artificial intelligence system (i) before the\ndeployer initially deploys such high-risk artificial intelligence system and\n(ii) before a significant update to such high-risk artificial intelligence\nsystem is used to make a consequential decision._\n\n_Each impact assessment completed pursuant to this subsection shall include,\nat a minimum:_\n\n_1\\. A statement by the deployer disclosing (i) the purpose, intended use\ncases and deployment context of, and benefits afforded by the high-risk\nartificial intelligence system and (ii) whether the deployment or use of the\nhigh-risk artificial intelligence system poses any known or reasonably\nforeseeable risk of algorithmic discrimination and, if so, (a) the nature of\nsuch algorithmic discrimination and (b) the steps that have been taken, to the\nextent feasible, to mitigate such risk;_\n\n_2\\. For each post-deployment impact assessment completed pursuant to this\nsubsection, whether the intended use cases of the high-risk artificial\nintelligence system as updated were consistent with, or varied from, the\ndeveloper 's intended uses of such high-risk artificial intelligence system;_\n\n_3\\. A description of (i) the categories of data the high-risk artificial\nintelligence system processes as inputs and (ii) the outputs such high-risk\nartificial intelligence system produces;_\n\n_4\\. If the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize such high-risk artificial intelligence system;_\n\n_5\\. A list of any metrics used to evaluate the performance and known\nlimitations of the high-risk artificial intelligence system;_\n\n_6\\. A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that such high-risk artificial intelligence system is in use when\nsuch high-risk artificial intelligence system is in use;_\n\n_7\\. A description of any post-deployment monitoring performed and user\nsafeguards provided concerning such high-risk artificial intelligence system,\nincluding any oversight process established by the deployer to address issues\narising from deployment or use of such high-risk artificial intelligence\nsystem as such issues arise; and_\n\n_8\\. An analysis of such high-risk artificial intelligence system 's validity\nand reliability in accordance with standard industry practices and a\ndescription of any metrics used to evaluate the performance and known\nlimitations of such high-risk artificial intelligence system._\n\n_A single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed or used by a deployer. High-risk\nartificial intelligence systems that are in conformity with the latest version\nof the Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology, Standard ISO/IEC 42001 of the\nInternational Organization for Standardization, or another nationally or\ninternationally recognized risk management framework for artificial\nintelligence systems, or parts thereof, shall be presumed to be in conformity\nwith related requirements set out in this section and in associated\nregulations. If a deployer completes an impact assessment for the purpose of\ncomplying with another applicable law or regulation, such impact assessment\nshall be deemed to satisfy the requirements established in this subsection if\nsuch impact assessment is reasonably similar in scope and effect to the impact\nassessment that would otherwise be completed pursuant to this subsection. A\ndeployer that completes an impact assessment pursuant to this subsection shall\nmaintain such impact assessment and all records concerning such impact\nassessment for three years._\n\n_Throughout the period of time that a high-risk artificial intelligence system\nis deployed and for a period of at least three years following the final\ndeployment of such high-risk artificial intelligence system, the deployer\nshall retain all records concerning each impact assessment conducted on the\nhigh-risk artificial intelligence system, including all raw data used to\nevaluate the performance and known limitations of such system._\n\n_D. Not later than the time that a deployer uses a high-risk artificial\nintelligence system to interact with a consumer, the deployer shall disclose\nto the consumer that the deployer is interacting with an artificial\nintelligence system disclosing (i) the purpose of such high-risk artificial\nintelligence system, (ii) the nature of such system, (iii) the nature of the\nconsequential decision, (iv) the contact information for the deployer, and (v)\na description of the artificial intelligence system in plain language of such\nsystem, which shall include (a) a description of the personal characteristics\nor attributes that such system will measure or assess, (b) the method by which\nthe system measures or assesses such attributes or characteristics, (c) how\nsuch attributes or characteristics are relevant to the consequential decisions\nfor which the system should be used, (d) any human components of such system,\nand (e) how any automated components of such system are used to inform such\nconsequential decisions._\n\n_A deployer that has deployed a high-risk artificial intelligence system to\nmake a consequential decision concerning a consumer shall transmit to the\nconsumer the consequential decision without undue delay. If such consequential\ndecision is adverse to such consumer and based on personal data beyond\ninformation that the consumer provided directly to the deployer, the deployer\nshall provide to the consumer (a) a statement disclosing the principal reason\nor reasons for the consequential decision, including (1) the degree to which\nand manner in which the high-risk artificial intelligence system contributed\nto the consequential decision, (2) the type of data that was processed by such\nsystem in making the consequential decision, and (3) the sources of such data;\n(b) pursuant to subdivision A 2 of § 59.1-577, an opportunity to correct any\ninaccuracies in the consumer's personal data that the high-risk artificial\nintelligence system processed in making, or as a substantial factor in making,\nthe consequential decision; and (c) an opportunity to appeal such adverse\nconsequential decision concerning the consumer arising from the deployment of\nsuch system. Any such appeal shall allow for human review, if technically\nreasonable and practicable, unless providing the opportunity for appeal is not\nin the best interest of the consumer, including instances in which any delay\nmight pose a risk to the life or safety of such consumer._\n\n_E. Each deployer shall make available, in a manner that is clear and readily\navailable, a statement summarizing how such deployer manages any reasonably\nforeseeable risk of algorithmic discrimination that may arise from the use or\ndeployment of the high-risk artificial intelligence system._\n\n_F. For any disclosure required pursuant to this section, each deployer shall,\nno later than 30 days after the deployer is notified by the developer that the\ndeveloper has performed an intentional and substantial modification to any\nhigh-risk artificial intelligence system, update such disclosure as necessary\nto ensure that such disclosure remains accurate._\n\n_G. Any deployer who performs an intentional and substantial modification to\nany high-risk artificial intelligence system shall comply with the\ndocumentation and disclosure requirements for developers pursuant to\nsubsections B through G of § 59.1-608._\n\n_H. Nothing in this section shall be construed to require a deployer to\ndisclose any trade secret, as defined in § 59.1-336, information that could\ncreate a security risk, or other confidential or proprietary information._\n\n**_§ 59.1-610. Exemptions._ **\n\n_A. Nothing in this chapter shall be construed to restrict a developer 's or\ndeployer's ability to (i) comply with federal, state, or municipal ordinances\nor regulations; (ii) comply with a civil, criminal, or regulatory inquiry,\ninvestigation, subpoena, or summons by federal, state, local, or other\ngovernmental authorities; (iii) cooperate with law-enforcement agencies\nconcerning conduct or activity that the developer or deployer reasonably and\nin good faith believes may violate federal, state, or local law, ordinances,\nor regulations; (iv) investigate, establish, exercise, prepare for, or defend\nlegal claims; (v) provide a product or service specifically requested by a\nconsumer; (vi) perform under a contract to which a consumer is a party,\nincluding fulfilling the terms of a written warranty; (vii) take steps at the\nrequest of a consumer prior to entering into a contract; (viii) take immediate\nsteps to protect an interest that is essential for the life or physical safety\nof the consumer or another individual; (ix) prevent, detect, protect against,\nor respond to security incidents, identity theft, fraud, harassment, or\nmalicious or deceptive activities; (x) take actions to prevent, detect,\nprotect against, report, or respond to the production, generation,\nincorporation, or synthesization of child sex abuse material, or any illegal\nactivity, preserve the integrity or security of systems, or investigate,\nreport, or prosecute those responsible for any such action; (xi) engage in\npublic or peer-reviewed scientific or statistical research in the public\ninterest that adheres to all other applicable ethics and privacy laws and is\napproved, monitored, and governed by an institutional review board that\ndetermines, or similar independent oversight entities that determine, (a) that\nthe expected benefits of the research outweigh the risks associated with such\nresearch and (b) whether the developer or deployer has implemented reasonable\nsafeguards to mitigate the risks associated with such research; (xii) assist\nanother developer or deployer with any of the obligations imposed by this\nchapter; or (xiii) take any action that is in the public interest in the areas\nof public health, community health, or population health, but solely to the\nextent that such action is subject to suitable and specific measures to\nsafeguard the public._\n\n_B. The obligations imposed on developers or deployers by this chapter shall\nnot restrict a developer 's or deployer's ability to (i) conduct internal\nresearch to develop, improve, or repair products, services, or technologies;\n(ii) effectuate a product recall; (iii) identify and repair technical errors\nthat impair existing or intended functionality; or (iv) perform internal\noperations that are reasonably aligned with the expectations of the consumer\nor reasonably anticipated based on the consumer's existing relationship with\nthe developer or deployer._\n\n_C. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer to disclose trade secrets or information protected from\ndisclosure by state or federal law._\n\n_D. The obligations imposed on developers or deployers by this chapter shall\nnot apply where compliance by the developer or deployer with such obligations\nwould violate an evidentiary privilege under federal law or the laws of the\nCommonwealth._\n\n_E. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer that adversely affects the legally protected rights or\nfreedoms of any person, including the rights of any person to freedom of\nspeech or freedom of the press guaranteed in the First Amendment to the\nConstitution of the United States or under the Virginia Human Rights Act ( §\n2.2-3900 et seq.)._\n\n_F. The obligations imposed on developers or deployers by this chapter shall\nnot apply to any artificial intelligence system that is acquired by or for the\nfederal government or any federal agency or department, including the U.S.\nDepartment of Commerce, the U.S. Department of Defense, and the National\nAeronautics and Space Administration, unless such artificial intelligence\nsystem is a high-risk artificial intelligence system that is used to make, or\nis a substantial factor in making, a decision concerning employment or\nhousing._\n\n_G. For the purposes of this subsection:_\n\n_\" Affiliate\" means the same as that term is defined in § 6.2-899._\n\n_\" Bank\" means the same as that term is defined in § 6.2-800._\n\n_\" Credit union\" means the same as that term is defined in § 6.2-1300._\n\n_\" Federal credit union\" means a credit union duly organized under federal\nlaw._\n\n_\" Mortgage lender\" means the same as that term is defined in § 6.2-1600._\n\n_\" Out-of-state bank\" means the same as that term is defined in § 6.2-836._\n\n_\" Out-of-state credit union\" means a credit union organized and doing\nbusiness in another state._\n\n_\" Savings institution\" means the same as that term is defined in § 6.2-1100._\n\n_\" Subsidiary\" means the same as that term is defined in § 6.2-700._\n\n_The obligations imposed on developers or deployers by this chapter shall be\ndeemed satisfied for any bank, out-of-state bank, credit union, federal credit\nunion, mortgage lender, out-of-state credit union, savings institution, or any\naffiliate or subsidiary thereof if such bank, out-of-state bank, credit union,\nfederal credit union, mortgage lender, out-of-state credit union, savings\ninstitution, or affiliate or subsidiary is subject to the jurisdiction of any\nstate or federal regulator under any published guidance or regulations that\napply to the use of high-risk artificial intelligence systems and such\nguidance or regulations._\n\n_H. For purposes of this subsection, \"insurer\" means the same as that term is\ndefined in § 38.2-100._\n\n_The provisions of this chapter shall not apply to any insurer, or any high-\nrisk artificial intelligence system developed by or for or deployed by an\ninsurer for use in the business of insurance, if such insurer is regulated and\nsupervised by the State Corporation Commission or a comparable federal\nregulating body and subject to examination by such entity under any existing\nstatutes, rules, or regulations pertaining to unfair trade practices and\nunfair discrimination prohibited under Chapter 5 ( § 38.2-500 et seq.) of\nTitle 38.2, or published guidance or regulations that apply to the use of\nhigh-risk artificial intelligence systems and such guidance or regulations aid\nin the prevention and mitigation of algorithmic discrimination caused by the\nuse of a high-risk artificial intelligence system or any risk of algorithmic\ndiscrimination that is reasonably foreseeable as a result of the use of a\nhigh-risk artificial intelligence system. Nothing in this chapter shall be\nconstrued to delegate existing regulatory oversight of the business of\ninsurance to any department or agency other than the Bureau of Insurance of\nthe Virginia State Corporation Commission._\n\n_I. The provisions of this chapter shall not apply to the development of an\nartificial intelligence system that is used exclusively for research,\ntraining, testing, or other pre-deployment activities performed by active\nparticipants of any sandbox software or sandbox environment established and\nsubject to oversight by a designated agency or other government entity and\nthat is in compliance with the provisions of this chapter._\n\n_J. The provisions of this chapter shall not apply to a developer or deployer,\nor other person who develops, deploys, puts into service, or intentionally\nmodifies, as applicable, a high-risk artificial intelligence system that (i)\nhas been approved, authorized, certified, cleared, developed, or granted by a\nfederal agency acting within the scope of the federal agency 's authority, or\nby a regulated entity subject to the supervision and regulation of the Federal\nHousing Finance Agency or (ii) is in compliance with standards established by\na federal agency or by a regulated entity subject to the supervision and\nregulation of the Federal Housing Finance Agency, if the standards are\nsubstantially equivalent or more stringent than the requirements of this\nchapter._\n\n_K. The provisions of this chapter shall not apply to a developer or deployer,\nor other person that (i) facilitates or engages in the provision of telehealth\nservices, as defined in § 32.1-122.03:1, or (ii) is a covered entity within\nthe meaning of the federal Health Insurance Portability and Accountability Act\nof 1996 (42 U.S.C. § 1320d et seq.) and the regulations promulgated under such\nfederal act, as both may be amended from time to time, and is providing (a)\nhealth care recommendations that (1) are generated by an artificial\nintelligence system and (2) require a health care provider, as defined in §\n8.01-581.1, to take action to implement the recommendations or (b) services\nutilizing an artificial intelligence system for an administrative, quality\nmeasurement, security, or internal cost or performance improvement function._\n\n_L. If a developer or deployer engages in any action authorized by an\nexemption set forth in this section, the developer or deployer bears the\nburden of demonstrating that such action qualifies for such exemption._\n\n_M. If a developer or deployer withholds information pursuant to an exemption\nset forth in this chapter for which disclosure would otherwise be required by\nthis chapter, including the exemption from disclosure of trade secrets, the\ndeveloper or deployer shall notify the subject of disclosure and provide a\nbasis for withholding the information. If a developer or deployer redacts any\ninformation pursuant to an exemption from disclosure, the developer or\ndeployer shall notify the subject of disclosure that the developer or deployer\nis redacting such information and provide the basis for such decision to\nredact._\n\n**_§ 59.1-611. Enforcement; civil penalties._ **\n\n_A. The Attorney General shall have exclusive authority to enforce the\nprovisions of this chapter._\n\n_B. Whenever the Attorney General has reasonable cause to believe that any\nperson has engaged in or is engaging in any violation of this chapter, the\nAttorney General is empowered to issue a civil investigative demand. The\nprovisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative\ndemands issued pursuant to this section. In rendering and furnishing any\ninformation requested pursuant to a civil investigative demand issued pursuant\nto this section, a developer or deployer may redact or omit any trade secrets\nor information protected from disclosure by state or federal law. If a\ndeveloper or deployer refuses to disclose, redacts, or omits information based\non the exemption from disclosure of trade secrets, such developer or deployer\nshall affirmatively state to the Attorney General that the basis for\nnondisclosure, redaction, or omission is because such information is a trade\nsecret. To the extent that any information requested pursuant to a civil\ninvestigative demand issued pursuant to this section is subject to attorney-\nclient privilege or work-product protection, disclosure of such information\npursuant to the civil investigative demand shall not constitute a waiver of\nsuch privilege or protection. Any information, statement, or documentation\nprovided to the Attorney General pursuant to this section shall be exempt from\ndisclosure under the Virginia Freedom of Information Act (§ 2.2-3700 et\nseq.)._\n\n_C. Notwithstanding any contrary provision of law, the Attorney General may\ncause an action to be brought in the appropriate circuit court in the name of\nthe Commonwealth to enjoin any violation of this chapter. The circuit court\nhaving jurisdiction may enjoin such violation notwithstanding the existence of\nan adequate alternative remedy at law._\n\n_D. Any person who violates the provisions of this chapter shall be subject to\na civil penalty in an amount not to exceed $1,000 plus reasonable attorney\nfees, expenses, and costs, as determined by the court. Any person who\nwillfully violates the provisions of this chapter shall be subject to a civil\npenalty in an amount not less than $1,000 and not more than $10,000 plus\nreasonable attorney fees, expenses, and costs, as determined by the court.\nSuch civil penalties shall be paid into the Literary Fund._\n\n_E. Each violation of this chapter shall constitute a separate violation and\nshall be subject to any civil penalties imposed under this section._\n\n_F. The Attorney General may require that a developer disclose to the Attorney\nGeneral any statement or documentation described in this chapter if such\nstatement or documentation is relevant to an investigation conducted by the\nAttorney General. The Attorney General may also require that a deployer\ndisclose to the Attorney General any risk management policy designed and\nimplemented, impact assessment completed, or record maintained pursuant to\nthis chapter if such risk management policy, impact assessment, or record is\nrelevant to an investigation conducted by the Attorney General._\n\n_G. In an action brought by the Attorney General pursuant to this section, it\nshall be an affirmative defense that the developer or deployer (i) discovers a\nviolation of any provision of this chapter through red-teaming or other\nmethod; (ii) no later than 45 days after discovering such violation (a) cures\nsuch violation and (b) provides notice to the Attorney General in a form and\nmanner as prescribed by the Attorney General that such violation has been\ncured and evidence that any harm caused by such violation has been mitigated;\nand (iii) is otherwise in compliance with the requirements of this chapter._\n\n_H. Prior to causing an action against a developer or deployer for a violation\nof this chapter pursuant to subsection C, the Attorney General shall\ndetermine, in consultation with the developer or deployer, if it is possible\nto cure the violation. If it is possible to cure such violation, the Attorney\nGeneral may issue a notice of violation to the developer or deployer and\nafford the developer or deployer the opportunity to cure such violation within\n45 days of the receipt of such notice of violation. In determining whether to\ngrant such opportunity to cure such violation, the Attorney General shall\nconsider (i) the number of violations; (ii) the size and complexity of the\ndeveloper or deployer; (iii) the nature and extent of the developer 's or\ndeployer's business; (iv) the substantial likelihood of injury to the public;\n(v) the safety of persons or property; and (vi) whether such violation was\nlikely caused by human or technical error. If the developer or deployer fails\nto cure such violation within 45 days of the receipt of such notice of\nviolation, the Attorney General may proceed with such action._\n\n_I. Nothing in this chapter shall create a private cause of action in favor of\nany person aggrieved by a violation of this chapter._\n\n**_§ 59.1-612. Construction of chapter._ **\n\n_A. This chapter is declared to be remedial, with the purposes of protecting\nconsumers and ensuring consumers receive information about consequential\ndecisions affecting them. The provisions of this chapter granting rights or\nprotections to consumers shall be construed broadly and exemptions construed\nnarrowly._\n\n_B. If any provision of this chapter or its application to any person or\ncircumstance is held invalid, the invalidity shall not affect other provisions\nor applications of this chapter that can be given effect without the invalid\nprovision or application, and to this end all the provisions of this chapter\nare hereby expressly declared to be severable._\n\n**2\\. That the provisions of this act shall become effective on July 1,\n2026.**\n\n**3\\. That compliance with the provisions of Chapter 58 ( § 59.1-607 et seq.)\nof Title 59.1 of the Code of Virginia, as created by this act, shall not (i)\nrelieve a person from liability for any causes of action that existed at\ncommon law or by statute prior to July 1, 2026, or (ii) be construed to modify\nor otherwise affect, preempt, limit, or displace any causes of action that\nexisted at common law or by statute prior to July 1, 2026.**\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "02/03/2025",
      "label": "Engrossed",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:VA2024000H2094&verid=VA2024000H2094_20250203_0_N&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2024 VA H 2094</td> <td><table><tr><td class=\"label\">Author:</td> <td>Maldonado</td></tr> <tr><td class=\"label\">Version:</td> <td>Engrossed</td></tr> <tr><td class=\"label\">Version Date:</td> <td>02/03/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">HOUSE BILL NO. 2094</p>\n   <p class=\"center\">AMENDMENT IN THE NATURE OF A SUBSTITUTE</p>\n   <p class=\"center\">(Proposed by the House Committee on Appropriations)</p>\n   <p class=\"center\">(Patron Prior to Substitute Delegate Maldonado)</p>\n   <p class=\"center\">House Amendments in [ ] - February 3, 2025</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"indent\">A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, relating to high-risk artificial intelligence; development, deployment, and use; civil penalties.</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">Be it enacted by the General Assembly of Virginia:</p>\n   </span>\n   <p class=\"indent\">1. That the Code of Virginia is amended by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as follows:</p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">CHAPTER 58.</u>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-607. Definitions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">As used in this chapter, unless the context requires a different meaning:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Algorithmic discrimination&quot; means the use of an artificial intelligence system that results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, sexual orientation, veteran status, or other classification protected under state or federal law. &quot;Algorithmic discrimination&quot; does not include (i) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; (ii) the expansion of an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or (iii) an act or omission by or on behalf of a private club or other establishment not in fact open to the public, as set forth in Title II of the Civil Rights Act of 1964, 42 U.S.C. &sect; 2000a(e), as amended from time to time.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">quot;Artificial intelligence system&quot; means any machine learning-based system that, for any explicit or implicit objective, infers from the inputs such system receives how to generate outputs, including content, decisions, predictions, and recommendations, that can influence physical or virtual environments. &quot;Artificial intelligence system&quot; does not include any artificial intelligence system or general purpose artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence system or general purpose artificial intelligence model is [</u>\n    <strike class=\"amendmentDeletedText\">released on the market</strike>\n    <u class=\"amendmentInsertedText\">made available to consumers ] .</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consequential decision&quot; means any decision that has a material legal, or similarly significant, effect on the provision or denial to any consumer of (i) parole, probation, a pardon, or any other release from incarceration or court supervision; (ii) education enrollment or an education opportunity; (iii) access to employment; (iv) a financial or lending service; (v) access to health care services; (vi) housing; (vii) insurance; (viii) marital status; or (ix) a legal service.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consumer&quot; means a natural person who is a resident of the Commonwealth and is acting only in an individual or household context. &quot;Consumer&quot; does not include a natural person acting in a commercial or employment context.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Deployer&quot; means any person doing business in the Commonwealth that deploys or uses a high-risk artificial intelligence system to make a consequential decision in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Developer&quot; means any person doing business in the Commonwealth that develops or intentionally and substantially modifies a high-risk artificial intelligence system that is offered, sold, leased, given, or otherwise provided to consumers in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;General-purpose artificial intelligence model&quot; means a model used by an artificial intelligence system or other system that (i) displays significant generality, (ii) is capable of competently performing a wide range of distinct tasks, and (iii) can be integrated into a variety of downstream applications or systems. &quot;General-purpose artificial intelligence model&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is released on the market.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence&quot; means an artificial intelligence system that is capable of producing and used to produce synthetic content, including audio, images, text, and videos.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence system&quot; means any artificial intelligence system or service that incorporates generative artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that is specifically intended to autonomously make, or be a substantial factor in making, a consequential decision. A system or service is not a &quot;high-risk artificial intelligence system&quot; if it is intended to (i) perform a narrow procedural task, (ii) improve the result of a previously completed human activity, (iii) detect any decision-making patterns or any deviations from pre-existing decision-making patterns, or (iv) perform a preparatory task to an assessment relevant to a consequential decision. &quot;High-risk artificial intelligence system&quot; does not include any of the following technologies:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. Anti-fraud technology that does not use facial recognition technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Anti-malware technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Anti-virus technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Artificial intelligence-enabled video games;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. Calculators;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. Cybersecurity technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. Databases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. Data storage;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. Firewall technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. Internet domain registration;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. Internet website loading;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. Networking;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. Spam and robocall filtering;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. Spell-checking technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. Spreadsheets;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">16. Web caching;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">17. Web hosting or any similar technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">18. Autonomous vehicle technology; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">19. Technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an acceptable use policy that prohibits generating content that is discriminatory or unlawful.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Intentional and substantial modification&quot; means any deliberate change made to (i) an artificial intelligence system that results, at the time when the change is implemented and any time thereafter, in any new material risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects compliance of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. &quot;Intentional and substantial modification&quot; does not include (a) any customization made by deployers based on legitimate nondiscriminatory business justifications and within the scope and purpose of the artificial intelligence tool; (b) any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (1) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (2) such change (A) is made to such high-risk artificial intelligence system as a result of any learning described in clause (1) and (B) was predetermined by the deployer or the third party contracted by the deployer and included within the initial impact assessment of such high-risk artificial intelligence system as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Machine learning&quot; means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Person&quot; includes any individual, corporation, partnership, association, cooperative, limited liability company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. &quot;Person&quot; does not include any government or political subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Principal basis&quot; means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Red-teaming&quot; means adversarial testing to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Substantial factor&quot; means a factor that is (i) the principal basis for making a consequential decision, (ii) capable of altering the outcome of a consequential decision, and (iii) generated by an artificial intelligence system. &quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as the principal basis to make a consequential decision concerning the consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Synthetic content&quot; means information, such as images, video, audio clips, and text, that has been significantly modified or generated by algorithms, including by artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique, or process, that (i) derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by, other persons who can obtain economic value from its disclosure or use and (ii) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-608. Operating standards for developers of high-risk artificial intelligence systems.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each developer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a developer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the developer complied with the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No developer of a high-risk artificial intelligence system shall offer, sell, lease, give, or otherwise provide to a deployer or other developer a high-risk artificial intelligence system unless the developer makes available to the deployer or other developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement disclosing the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Documentation disclosing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">a. The known or reasonably known limitations of such high-risk artificial intelligence system, including any and all known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">b. The purpose of such high-risk artificial intelligence system and the intended benefits and uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">c. A summary describing how such high-risk artificial intelligence system was evaluated for performance before such high-risk artificial intelligence system was licensed, sold, leased, given, or otherwise made available to a deployer or other developer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">d. The measures the developer has taken to mitigate reasonable foreseeable risks of algorithmic discrimination that the developer knows arises from deployment or use of such high-risk artificial intelligence system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">e. How an individual can use such high-risk artificial intelligence system and monitor the performance of such high-risk artificial intelligence system for any risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Documentation including a (i) description of how the high-risk artificial intelligence system was evaluated for performance and for mitigation of algorithmic discrimination before such system was made available to the deployer or other developer; (ii) description of the intended outputs of the high-risk artificial intelligence system; (iii) description of the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and (iv) description of how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when such system is used to make, or is a substantial factor in making, a consequential decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any additional documentation that is reasonably necessary to assist the deployer or other developer in understanding the outputs and monitoring performance of the high-risk artificial intelligence system for risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Each developer that offers, sells, leases, gives, or otherwise makes available to a deployer or other developer a high-risk artificial intelligence system shall make available to the deployer or other developer to the extent feasible and necessary, information and documentation through artifacts such as system cards or predeployment impact assessments, including any risk management policy designed and implemented and any relevant impact assessment completed, and such documentation and information shall enable the deployer, other developer, or a third party contracted by the deployer to complete an impact assessment as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. A developer that also serves as a deployer for any high-risk artificial intelligence system shall not be required to generate the documentation required by this section unless such high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer or as otherwise required by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this section shall be construed to require a developer to disclose any trade secret, as defined in &sect; 59.1-336, information that could create a security risk, or other confidential or proprietary information.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For any disclosure required pursuant to this section, each developer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. 1. Each developer of a high-risk generative artificial intelligence system that generates or modifies synthetic content shall ensure that the outputs of such high-risk artificial intelligence system (i) are identifiable and detectable in a manner that is accessible by consumers; (ii) comply with any applicable accessibility requirements, as synthetic content; and (iii) such marking must be applied at the time the output is generated;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. If such synthetic content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for marking outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The marking of outputs required by subdivision 1 shall not apply to (i) synthetic content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Where multiple developers directly contribute to the development of a high-risk artificial intelligence system, each developer shall be subject to the obligations and operating standards applicable to developers pursuant to this section solely with respect to its activities contributing to the development of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-609. Operating standards for deployers of high-risk artificial intelligence systems.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each deployer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a deployer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the deployer complied with the provisions of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has designed and implemented a risk management policy and program for such high-risk artificial intelligence system. The risk management policy shall specify the principles, processes, and personnel that the deployer shall use in maintaining the risk management program to identify, mitigate, and document any risk of algorithmic discrimination that is a reasonably foreseeable consequence of deploying or using such high-risk artificial intelligence system to make a consequential decision. Each risk management policy and program designed, implemented, and maintained pursuant to this subsection shall be reasonable considering the guidance and standards set forth in the latest version of:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. The Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Standard ISO/IEC 42001 of the International Organization for Standardization;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A nationally or internationally recognized risk management framework for artificial intelligence systems with requirements that are substantially equivalent to, and at least as stringent as, the requirements set forth in this section; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any risk management framework for artificial intelligence systems that the Attorney General may designate and is substantially equivalent to, and at least as stringent as, the guidance and standards described in subdivision 1.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Except as provided in this subsection, no deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has completed an impact assessment for such high-risk artificial intelligence system. The deployer shall complete an impact assessment for a high-risk artificial intelligence system (i) before the deployer initially deploys such high-risk artificial intelligence system and (ii) before a significant update to such high-risk artificial intelligence system is used to make a consequential decision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each impact assessment completed pursuant to this subsection shall include, at a minimum:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement by the deployer disclosing (i) the purpose, intended use cases and deployment context of, and benefits afforded by the high-risk artificial intelligence system and (ii) whether the deployment or use of the high-risk artificial intelligence system poses any known or reasonably foreseeable risk of algorithmic discrimination and, if so, (a) the nature of such algorithmic discrimination and (b) the steps that have been taken, to the extent feasible, to mitigate such risk;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. For each post-deployment impact assessment completed pursuant to this subsection, whether the intended use cases of the high-risk artificial intelligence system as updated were consistent with, or varied from, the developer&#39;s intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A description of (i) the categories of data the high-risk artificial intelligence system processes as inputs and (ii) the outputs such high-risk artificial intelligence system produces;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. If the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. A list of any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. A description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that such high-risk artificial intelligence system is in use when such high-risk artificial intelligence system is in use;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. A description of any post-deployment monitoring performed and user safeguards provided concerning such high-risk artificial intelligence system, including any oversight process established by the deployer to address issues arising from deployment or use of such high-risk artificial intelligence system as such issues arise; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. An analysis of such high-risk artificial intelligence system&#39;s validity and reliability in accordance with standard industry practices and a description of any metrics used to evaluate the performance and known limitations of such high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed or used by a deployer. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations. If a deployer completes an impact assessment for the purpose of complying with another applicable law or regulation, such impact assessment shall be deemed to satisfy the requirements established in this subsection if such impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection. A deployer that completes an impact assessment pursuant to this subsection shall maintain such impact assessment and all records concerning such impact assessment for three years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Throughout the period of time that a high-risk artificial intelligence system is deployed and for a period of at least three years following the final deployment of such high-risk artificial intelligence system, the deployer shall retain all records concerning each impact assessment conducted on the high-risk artificial intelligence system, including all raw data used to evaluate the performance and known limitations of such system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Not later than the time that a deployer uses a high-risk artificial intelligence system to interact with a consumer, the deployer shall disclose to the consumer that the deployer is interacting with an artificial intelligence system disclosing (i) the purpose of such high-risk artificial intelligence system, (ii) the nature of such system, (iii) the nature of the consequential decision, (iv) the contact information for the deployer, and (v) a description of the artificial intelligence system in plain language of such system, which shall include (a) a description of the personal characteristics or attributes that such system will measure or assess, (b) the method by which the system measures or assesses such attributes or characteristics, (c) how such attributes or characteristics are relevant to the consequential decisions for which the system should be used, (d) any human components of such system, and (e) how any automated components of such system are used to inform such consequential decisions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A deployer that has deployed a high-risk artificial intelligence system to make a consequential decision concerning a consumer shall transmit to the consumer the consequential decision without undue delay. If such consequential decision is adverse to such consumer and based on personal data beyond information that the consumer provided directly to the deployer, the deployer shall provide to the consumer (a) a statement disclosing the principal reason or reasons for the consequential decision, including (1) the degree to which and manner in which the high-risk artificial intelligence system contributed to the consequential decision, (2) the type of data that was processed by such system in making the consequential decision, and (3) the sources of such data; (b) pursuant to subdivision A 2 of &sect; 59.1-577, an opportunity to correct any inaccuracies in the consumer&#39;s personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and (c) an opportunity to appeal such adverse consequential decision concerning the consumer arising from the deployment of such system. Any such appeal shall allow for human review, if technically reasonable and practicable, unless providing the opportunity for appeal is not in the best interest of the consumer, including instances in which any delay might pose a risk to the life or safety of such consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each deployer shall make available, in a manner that is clear and readily available, a statement summarizing how such deployer manages any reasonably foreseeable risk of algorithmic discrimination that may arise from the use or deployment of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each deployer shall, no later than 30 days after the deployer is notified by the developer that the developer has performed an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. Any deployer who performs an intentional and substantial modification to any high-risk artificial intelligence system shall comply with the documentation and disclosure requirements for developers pursuant to subsections B through G of &sect; 59.1-608.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Nothing in this section shall be construed to require a deployer to disclose any trade secret, as defined in &sect; 59.1-336, information that could create a security risk, or other confidential or proprietary information.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-610. Exemptions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Nothing in this chapter shall be construed to restrict a developer&#39;s or deployer&#39;s ability to (i) comply with federal, state, or municipal ordinances or regulations; (ii) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by federal, state, local, or other governmental authorities; (iii) cooperate with law-enforcement agencies concerning conduct or activity that the developer or deployer reasonably and in good faith believes may violate federal, state, or local law, ordinances, or regulations; (iv) investigate, establish, exercise, prepare for, or defend legal claims; (v) provide a product or service specifically requested by a consumer; (vi) perform under a contract to which a consumer is a party, including fulfilling the terms of a written warranty; (vii) take steps at the request of a consumer prior to entering into a contract; (viii) take immediate steps to protect an interest that is essential for the life or physical safety of the consumer or another individual; (ix) prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, or malicious or deceptive activities; (x) take actions to prevent, detect, protect against, report, or respond to the production, generation, incorporation, or synthesization of child sex abuse material, or any illegal activity, preserve the integrity or security of systems, or investigate, report, or prosecute those responsible for any such action; (xi) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is approved, monitored, and governed by an institutional review board that determines, or similar independent oversight entities that determine, (a) that the expected benefits of the research outweigh the risks associated with such research and (b) whether the developer or deployer has implemented reasonable safeguards to mitigate the risks associated with such research; (xii) assist another developer or deployer with any of the obligations imposed by this chapter; or (xiii) take any action that is in the public interest in the areas of public health, community health, or population health, but solely to the extent that such action is subject to suitable and specific measures to safeguard the public.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. The obligations imposed on developers or deployers by this chapter shall not restrict a developer&#39;s or deployer&#39;s ability to (i) conduct internal research to develop, improve, or repair products, services, or technologies; (ii) effectuate a product recall; (iii) identify and repair technical errors that impair existing or intended functionality; or (iv) perform internal operations that are reasonably aligned with the expectations of the consumer or reasonably anticipated based on the consumer&#39;s existing relationship with the developer or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer to disclose trade secrets or information protected from disclosure by state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. The obligations imposed on developers or deployers by this chapter shall not apply where compliance by the developer or deployer with such obligations would violate an evidentiary privilege under federal law or the laws of the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer that adversely affects the legally protected rights or freedoms of any person, including the rights of any person to freedom of speech or freedom of the press guaranteed in the First Amendment to the Constitution of the United States or under the Virginia Human Rights Act (&sect; 2.2-3900 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The obligations imposed on developers or deployers by this chapter shall not apply to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the U.S. Department of Commerce, the U.S. Department of Defense, and the National Aeronautics and Space Administration, unless such artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For the purposes of this subsection:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Affiliate&quot; means the same as that term is defined in &sect; 6.2-899.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Bank&quot; means the same as that term is defined in &sect; 6.2-800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Credit union&quot; means the same as that term is defined in &sect; 6.2-1300.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Federal credit union&quot; means a credit union duly organized under federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Mortgage lender&quot; means the same as that term is defined in &sect; 6.2-1600.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state bank&quot; means the same as that term is defined in &sect; 6.2-836.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state credit union&quot; means a credit union organized and doing business in another state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Savings institution&quot; means the same as that term is defined in &sect; 6.2-1100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Subsidiary&quot; means the same as that term is defined in &sect; 6.2-700.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The obligations imposed on developers or deployers by this chapter shall be deemed satisfied for any bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or any affiliate [ </u>\n    <strike class=\"amendmentDeletedText\">or</strike>\n    <u class=\"amendmentInsertedText\"> , ] subsidiary [ , or service provider ] thereof if such bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or affiliate [  or , ]  subsidiary [ , or service provider ] is subject to the jurisdiction of any state or federal regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. For purposes of this subsection, &quot;insurer&quot; means the same as that term is defined in &sect; 38.2-100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The provisions of this chapter shall not apply to any insurer, or any high-risk artificial intelligence system developed by or for or deployed by an insurer for use in the business of insurance, if such insurer is regulated and supervised by the State Corporation Commission or a comparable federal regulating body and subject to examination by such entity under any existing statutes, rules, or regulations pertaining to unfair trade practices and unfair discrimination prohibited under Chapter 5 (&sect; 38.2-500 et seq.) of Title 38.2, or published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations aid in the prevention and mitigation of algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. Nothing in this chapter shall be construed to delegate existing regulatory oversight of the business of insurance to any department or agency other than the Bureau of Insurance of the Virginia State Corporation Commission.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. The provisions of this chapter shall not apply to the development of an artificial intelligence system that is used exclusively for research, training, testing, or other pre-deployment activities performed by active participants of any sandbox software or sandbox environment established and subject to oversight by a designated agency or other government entity and that is in compliance with the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">J. The provisions of this chapter shall not apply to a developer or deployer, or other person who develops, deploys, puts into service, or intentionally modifies, as applicable, a high-risk artificial intelligence system that (i) has been approved, authorized, certified, cleared, developed, or granted by a federal agency acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency or (ii) is in compliance with standards established by a federal agency or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">K. The provisions of this chapter shall not apply to a developer or deployer, or other person that (i) facilitates or engages in the provision of telehealth services, as defined in &sect; 32.1-122.03:1, or (ii) is a covered entity within the meaning of the federal Health Insurance Portability and Accountability Act of 1996 (42 U.S.C. &sect; 1320d et seq.) and the regulations promulgated under such federal act, as both may be amended from time to time, and is providing (a) health care recommendations that (1) are generated by an artificial intelligence system and (2) require a health care provider, as defined in &sect; 8.01-581.1, to take action to implement the recommendations or (b) services utilizing an artificial intelligence system for an administrative, quality measurement, security, or internal cost or performance improvement function.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">L. If a developer or deployer engages in any action authorized by an exemption set forth in this section, the developer or deployer bears the burden of demonstrating that such action qualifies for such exemption.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">M. If a developer or deployer withholds information pursuant to an exemption set forth in this chapter for which disclosure would otherwise be required by this chapter, including the exemption from disclosure of trade secrets, the developer or deployer shall notify the subject of disclosure and provide a basis for withholding the information. If a developer or deployer redacts any information pursuant to an exemption from disclosure, the developer or deployer shall notify the subject of disclosure that the developer or deployer is redacting such information and provide the basis for such decision to redact.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-611. Enforcement; civil penalties.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. The Attorney General shall have exclusive authority to enforce the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any violation of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of &sect; 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. If a developer or deployer refuses to disclose, redacts, or omits information based on the exemption from disclosure of trade secrets, such developer or deployer shall affirmatively state to the Attorney General that the basis for nondisclosure, redaction, or omission is because such information is a trade secret. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand shall not constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be exempt from disclosure under the Virginia Freedom of Information Act (&sect; 2.2-3700 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any violation of this chapter. The circuit court having jurisdiction may enjoin such violation notwithstanding the existence of an adequate alternative remedy at law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Any person who violates the provisions of this chapter shall be subject to a civil penalty in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil penalty in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil penalties shall be paid into the Literary Fund.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each violation of this chapter shall constitute a separate violation and shall be subject to any civil penalties imposed under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The Attorney General may require that a developer disclose to the Attorney General any statement or documentation described in this chapter if such statement or documentation is relevant to an investigation conducted by the Attorney General. The Attorney General may also require that a deployer disclose to the Attorney General any risk management policy designed and implemented, impact assessment completed, or record maintained pursuant to this chapter if such risk management policy, impact assessment, or record is relevant to an investigation conducted by the Attorney General.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. In an action brought by the Attorney General pursuant to this section, it shall be an affirmative defense that the developer or deployer (i) discovers a violation of any provision of this chapter through red-teaming or other method; (ii) no later than 45 days after discovering such violation (a) cures such violation and (b) provides notice to the Attorney General in a form and manner as prescribed by the Attorney General that such violation has been cured and evidence that any harm caused by such violation has been mitigated; and (iii) is otherwise in compliance with the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Prior to causing an action against a developer or deployer for a violation of this chapter pursuant to subsection C, the Attorney General shall determine, in consultation with the developer or deployer, if it is possible to cure the violation. If it is possible to cure such violation, the Attorney General may issue a notice of violation to the developer or deployer and afford the developer or deployer the opportunity to cure such violation within 45 days of the receipt of such notice of violation. In determining whether to grant such opportunity to cure such violation, the Attorney General shall consider (i) the number of violations; (ii) the size and complexity of the developer or deployer; (iii) the nature and extent of the developer&#39;s or deployer&#39;s business; (iv) the substantial likelihood of injury to the public; (v) the safety of persons or property; and (vi) whether such violation was likely caused by human or technical error. If the developer or deployer fails to cure such violation within 45 days of the receipt of such notice of violation, the Attorney General may proceed with such action.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Nothing in this chapter shall create a private cause of action in favor of any person aggrieved by a violation of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&sect; 59.1-612. Construction of chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. This chapter is declared to be remedial, with the purposes of protecting consumers and ensuring consumers receive information about consequential decisions affecting them. The provisions of this chapter granting rights or protections to consumers shall be construed broadly and exemptions construed narrowly.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. If any provision of this chapter or its application to any person or circumstance is held invalid, the invalidity shall not affect other provisions or applications of this chapter that can be given effect without the invalid provision or application, and to this end all the provisions of this chapter are hereby expressly declared to be severable.</u>\n   </p>\n   <effective_clause>\n    <p class=\"indent\">2. That the provisions of this act shall become effective on July 1, 2026.</p>\n   </effective_clause>\n   <p class=\"indent\">3. That compliance with the provisions of Chapter 58 (&sect; 59.1-607 et seq.) of Title 59.1 of the Code of Virginia, as created by this act, shall not (i) relieve a person from liability for any causes of action that existed at common law or by statute prior to July 1, 2026, or (ii) be construed to modify or otherwise affect, preempt, limit, or displace any causes of action that existed at common law or by statute prior to July 1, 2026.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2024 VA H 2094 | | Author: | Maldonado  \n---|---  \nVersion: | Engrossed  \nVersion Date: | 02/03/2025  \n  \nHOUSE BILL NO. 2094\n\nAMENDMENT IN THE NATURE OF A SUBSTITUTE\n\n(Proposed by the House Committee on Appropriations)\n\n(Patron Prior to Substitute Delegate Maldonado)\n\nHouse Amendments in [ ] - February 3, 2025\n\nA BILL to amend the Code of Virginia by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612,\nrelating to high-risk artificial intelligence; development, deployment, and\nuse; civil penalties.\n\nBe it enacted by the General Assembly of Virginia:\n\n1\\. That the Code of Virginia is amended by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as\nfollows:\n\n_CHAPTER 58._\n\n_HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT._\n\n_§ 59.1-607. Definitions._\n\n_As used in this chapter, unless the context requires a different meaning:_\n\n_\" Algorithmic discrimination\" means the use of an artificial intelligence\nsystem that results in an unlawful differential treatment or impact that\ndisfavors an individual or group of individuals on the basis of their actual\nor perceived age, color, disability, ethnicity, genetic information, limited\nproficiency in the English language, national origin, race, religion,\nreproductive health, sex, sexual orientation, veteran status, or other\nclassification protected under state or federal law. \"Algorithmic\ndiscrimination\" does not include (i) the offer, license, or use of a high-risk\nartificial intelligence system by a developer or deployer for the sole purpose\nof the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; (ii) the expansion of an applicant, customer, or participant pool to\nincrease diversity or redress historical discrimination; or (iii) an act or\nomission by or on behalf of a private club or other establishment not in fact\nopen to the public, as set forth in Title II of the Civil Rights Act of 1964,\n42 U.S.C. § 2000a(e), as amended from time to time._\n\n_quot;Artificial intelligence system \" means any machine learning-based system\nthat, for any explicit or implicit objective, infers from the inputs such\nsystem receives how to generate outputs, including content, decisions,\npredictions, and recommendations, that can influence physical or virtual\nenvironments. \"Artificial intelligence system\" does not include any artificial\nintelligence system or general purpose artificial intelligence model that is\nused for development, prototyping, and research activities before such\nartificial intelligence system or general purpose artificial intelligence\nmodel is [_ ~~released on the market~~ _made available to consumers ] ._\n\n_\" Consequential decision\" means any decision that has a material legal, or\nsimilarly significant, effect on the provision or denial to any consumer of\n(i) parole, probation, a pardon, or any other release from incarceration or\ncourt supervision; (ii) education enrollment or an education opportunity;\n(iii) access to employment; (iv) a financial or lending service; (v) access to\nhealth care services; (vi) housing; (vii) insurance; (viii) marital status; or\n(ix) a legal service._\n\n_\" Consumer\" means a natural person who is a resident of the Commonwealth and\nis acting only in an individual or household context. \"Consumer\" does not\ninclude a natural person acting in a commercial or employment context._\n\n_\" Deployer\" means any person doing business in the Commonwealth that deploys\nor uses a high-risk artificial intelligence system to make a consequential\ndecision in the Commonwealth._\n\n_\" Developer\" means any person doing business in the Commonwealth that\ndevelops or intentionally and substantially modifies a high-risk artificial\nintelligence system that is offered, sold, leased, given, or otherwise\nprovided to consumers in the Commonwealth._\n\n_\" General-purpose artificial intelligence model\" means a model used by an\nartificial intelligence system or other system that (i) displays significant\ngenerality, (ii) is capable of competently performing a wide range of distinct\ntasks, and (iii) can be integrated into a variety of downstream applications\nor systems. \"General-purpose artificial intelligence model\" does not include\nany artificial intelligence model that is used for development, prototyping,\nand research activities before such artificial intelligence model is released\non the market._\n\n_\" Generative artificial intelligence\" means an artificial intelligence system\nthat is capable of producing and used to produce synthetic content, including\naudio, images, text, and videos._\n\n_\" Generative artificial intelligence system\" means any artificial\nintelligence system or service that incorporates generative artificial\nintelligence._\n\n_\" High-risk artificial intelligence system\" means any artificial intelligence\nsystem that is specifically intended to autonomously make, or be a substantial\nfactor in making, a consequential decision. A system or service is not a\n\"high-risk artificial intelligence system\" if it is intended to (i) perform a\nnarrow procedural task, (ii) improve the result of a previously completed\nhuman activity, (iii) detect any decision-making patterns or any deviations\nfrom pre-existing decision-making patterns, or (iv) perform a preparatory task\nto an assessment relevant to a consequential decision. \"High-risk artificial\nintelligence system\" does not include any of the following technologies:_\n\n_1\\. Anti-fraud technology that does not use facial recognition technology;_\n\n_2\\. Anti-malware technology;_\n\n_3\\. Anti-virus technology;_\n\n_4\\. Artificial intelligence-enabled video games;_\n\n_5\\. Calculators;_\n\n_6\\. Cybersecurity technology;_\n\n_7\\. Databases;_\n\n_8\\. Data storage;_\n\n_9\\. Firewall technology;_\n\n_10\\. Internet domain registration;_\n\n_11\\. Internet website loading;_\n\n_12\\. Networking;_\n\n_13\\. Spam and robocall filtering;_\n\n_14\\. Spell-checking technology;_\n\n_15\\. Spreadsheets;_\n\n_16\\. Web caching;_\n\n_17\\. Web hosting or any similar technology;_\n\n_18\\. Autonomous vehicle technology; or_\n\n_19\\. Technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an acceptable use\npolicy that prohibits generating content that is discriminatory or unlawful._\n\n_\" Intentional and substantial modification\" means any deliberate change made\nto (i) an artificial intelligence system that results, at the time when the\nchange is implemented and any time thereafter, in any new material risk of\nalgorithmic discrimination or (ii) a general-purpose artificial intelligence\nmodel that affects compliance of the general-purpose artificial intelligence\nmodel, materially changes the purpose of the general-purpose artificial\nintelligence model, or results in any new reasonably foreseeable risk of\nalgorithmic discrimination. \"Intentional and substantial modification\" does\nnot include (a) any customization made by deployers based on legitimate\nnondiscriminatory business justifications and within the scope and purpose of\nthe artificial intelligence tool; (b) any change made to a high-risk\nartificial intelligence system, or the performance of a high-risk artificial\nintelligence system, if (1) the high-risk artificial intelligence system\ncontinues to learn after such high-risk artificial intelligence system is\noffered, sold, leased, licensed, given, or otherwise made available to a\ndeployer, or deployed, and (2) such change (A) is made to such high-risk\nartificial intelligence system as a result of any learning described in clause\n(1) and (B) was predetermined by the deployer or the third party contracted by\nthe deployer and included within the initial impact assessment of such high-\nrisk artificial intelligence system as required in § 59.1-609._\n\n_\" Machine learning\" means the development of algorithms to build data-derived\nstatistical models that are capable of drawing inferences from previously\nunseen data without explicit human instruction._\n\n_\" Person\" includes any individual, corporation, partnership, association,\ncooperative, limited liability company, trust, joint venture, or any other\nlegal or commercial entity and any successor, representative, agent, agency,\nor instrumentality thereof. \"Person\" does not include any government or\npolitical subdivision._\n\n_\" Principal basis\" means the use of an output of a high-risk artificial\nintelligence system to make a decision without (i) human review, oversight,\ninvolvement, or intervention or (ii) meaningful consideration by a human._\n\n_\" Red-teaming\" means adversarial testing to identify the potential adverse\nbehaviors or outcomes of an artificial intelligence system, identify how such\nbehaviors or outcomes occur, and stress test the safeguards against such\nbehaviors or outcomes._\n\n_\" Substantial factor\" means a factor that is (i) the principal basis for\nmaking a consequential decision, (ii) capable of altering the outcome of a\nconsequential decision, and (iii) generated by an artificial intelligence\nsystem. \"Substantial factor\" includes any use of an artificial intelligence\nsystem to generate any content, decision, prediction, or recommendation\nconcerning a consumer that is used as the principal basis to make a\nconsequential decision concerning the consumer._\n\n_\" Synthetic content\" means information, such as images, video, audio clips,\nand text, that has been significantly modified or generated by algorithms,\nincluding by artificial intelligence._\n\n_\" Trade secret\" means information, including a formula, pattern, compilation,\nprogram, device, method, technique, or process, that (i) derives independent\neconomic value, actual or potential, from not being generally known to, and\nnot being readily ascertainable by proper means by, other persons who can\nobtain economic value from its disclosure or use and (ii) is the subject of\nefforts that are reasonable under the circumstances to maintain its secrecy._\n\n_§ 59.1-608. Operating standards for developers of high-risk artificial\nintelligence systems._\n\n_A. Each developer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-611, there shall be a\nrebuttable presumption that a developer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if the\ndeveloper complied with the requirements of this section._\n\n_B. No developer of a high-risk artificial intelligence system shall offer,\nsell, lease, give, or otherwise provide to a deployer or other developer a\nhigh-risk artificial intelligence system unless the developer makes available\nto the deployer or other developer:_\n\n_1\\. A statement disclosing the intended uses of such high-risk artificial\nintelligence system;_\n\n_2\\. Documentation disclosing the following:_\n\n_a. The known or reasonably known limitations of such high-risk artificial\nintelligence system, including any and all known or reasonably foreseeable\nrisks of algorithmic discrimination arising from the intended uses of such\nhigh-risk artificial intelligence system;_\n\n_b. The purpose of such high-risk artificial intelligence system and the\nintended benefits and uses of such high-risk artificial intelligence system;_\n\n_c. A summary describing how such high-risk artificial intelligence system was\nevaluated for performance before such high-risk artificial intelligence system\nwas licensed, sold, leased, given, or otherwise made available to a deployer\nor other developer;_\n\n_d. The measures the developer has taken to mitigate reasonable foreseeable\nrisks of algorithmic discrimination that the developer knows arises from\ndeployment or use of such high-risk artificial intelligence system; and_\n\n_e. How an individual can use such high-risk artificial intelligence system\nand monitor the performance of such high-risk artificial intelligence system\nfor any risk of algorithmic discrimination;_\n\n_3\\. Documentation including a (i) description of how the high-risk artificial\nintelligence system was evaluated for performance and for mitigation of\nalgorithmic discrimination before such system was made available to the\ndeployer or other developer; (ii) description of the intended outputs of the\nhigh-risk artificial intelligence system; (iii) description of the measures\nthe developer has taken to mitigate known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the reasonably foreseeable\ndeployment of the high-risk artificial intelligence system; and (iv)\ndescription of how the high-risk artificial intelligence system should be\nused, not be used, and be monitored by an individual when such system is used\nto make, or is a substantial factor in making, a consequential decision; and_\n\n_4\\. Any additional documentation that is reasonably necessary to assist the\ndeployer or other developer in understanding the outputs and monitoring\nperformance of the high-risk artificial intelligence system for risks of\nalgorithmic discrimination._\n\n_C. Each developer that offers, sells, leases, gives, or otherwise makes\navailable to a deployer or other developer a high-risk artificial intelligence\nsystem shall make available to the deployer or other developer to the extent\nfeasible and necessary, information and documentation through artifacts such\nas system cards or predeployment impact assessments, including any risk\nmanagement policy designed and implemented and any relevant impact assessment\ncompleted, and such documentation and information shall enable the deployer,\nother developer, or a third party contracted by the deployer to complete an\nimpact assessment as required in § 59.1-609._\n\n_D. A developer that also serves as a deployer for any high-risk artificial\nintelligence system shall not be required to generate the documentation\nrequired by this section unless such high-risk artificial intelligence system\nis provided to an unaffiliated entity acting as a deployer or as otherwise\nrequired by law._\n\n_E. Nothing in this section shall be construed to require a developer to\ndisclose any trade secret, as defined in § 59.1-336, information that could\ncreate a security risk, or other confidential or proprietary information._\n\n_F. High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_G. For any disclosure required pursuant to this section, each developer\nshall, no later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_H. 1. Each developer of a high-risk generative artificial intelligence system\nthat generates or modifies synthetic content shall ensure that the outputs of\nsuch high-risk artificial intelligence system (i) are identifiable and\ndetectable in a manner that is accessible by consumers; (ii) comply with any\napplicable accessibility requirements, as synthetic content; and (iii) such\nmarking must be applied at the time the output is generated;_\n\n_2\\. If such synthetic content is an audio, image, or video format that forms\npart of an evidently artistic, creative, satirical, fictional analogous work\nor program, such requirement for marking outputs of high-risk artificial\nintelligence systems pursuant to subdivision 1 shall be limited to a manner\nthat does not hinder the display or enjoyment of such work or program._\n\n_3\\. The marking of outputs required by subdivision 1 shall not apply to (i)\nsynthetic content that consists exclusively of text, is published to inform\nthe public on any matter of public interest, or is unlikely to mislead a\nreasonable person consuming such synthetic content or (ii) the outputs of a\nhigh-risk artificial intelligence system that performs an assistive function\nfor standard editing, does not substantially alter the input data provided by\nthe developer, or is used to detect, prevent, investigate, or prosecute any\ncrime as authorized by law._\n\n_I. Where multiple developers directly contribute to the development of a\nhigh-risk artificial intelligence system, each developer shall be subject to\nthe obligations and operating standards applicable to developers pursuant to\nthis section solely with respect to its activities contributing to the\ndevelopment of the high-risk artificial intelligence system._\n\n_§ 59.1-609. Operating standards for deployers of high-risk artificial\nintelligence systems._\n\n_A. Each deployer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-611, there shall be a\nrebuttable presumption that a deployer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if the\ndeployer complied with the provisions of this section._\n\n_B. No deployer shall deploy or use a high-risk artificial intelligence system\nto make a consequential decision unless the deployer has designed and\nimplemented a risk management policy and program for such high-risk artificial\nintelligence system. The risk management policy shall specify the principles,\nprocesses, and personnel that the deployer shall use in maintaining the risk\nmanagement program to identify, mitigate, and document any risk of algorithmic\ndiscrimination that is a reasonably foreseeable consequence of deploying or\nusing such high-risk artificial intelligence system to make a consequential\ndecision. Each risk management policy and program designed, implemented, and\nmaintained pursuant to this subsection shall be reasonable considering the\nguidance and standards set forth in the latest version of:_\n\n_1\\. The Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology;_\n\n_2\\. Standard ISO/IEC 42001 of the International Organization for\nStandardization;_\n\n_3\\. A nationally or internationally recognized risk management framework for\nartificial intelligence systems with requirements that are substantially\nequivalent to, and at least as stringent as, the requirements set forth in\nthis section; or_\n\n_4\\. Any risk management framework for artificial intelligence systems that\nthe Attorney General may designate and is substantially equivalent to, and at\nleast as stringent as, the guidance and standards described in subdivision 1._\n\n_High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_C. Except as provided in this subsection, no deployer shall deploy or use a\nhigh-risk artificial intelligence system to make a consequential decision\nunless the deployer has completed an impact assessment for such high-risk\nartificial intelligence system. The deployer shall complete an impact\nassessment for a high-risk artificial intelligence system (i) before the\ndeployer initially deploys such high-risk artificial intelligence system and\n(ii) before a significant update to such high-risk artificial intelligence\nsystem is used to make a consequential decision._\n\n_Each impact assessment completed pursuant to this subsection shall include,\nat a minimum:_\n\n_1\\. A statement by the deployer disclosing (i) the purpose, intended use\ncases and deployment context of, and benefits afforded by the high-risk\nartificial intelligence system and (ii) whether the deployment or use of the\nhigh-risk artificial intelligence system poses any known or reasonably\nforeseeable risk of algorithmic discrimination and, if so, (a) the nature of\nsuch algorithmic discrimination and (b) the steps that have been taken, to the\nextent feasible, to mitigate such risk;_\n\n_2\\. For each post-deployment impact assessment completed pursuant to this\nsubsection, whether the intended use cases of the high-risk artificial\nintelligence system as updated were consistent with, or varied from, the\ndeveloper 's intended uses of such high-risk artificial intelligence system;_\n\n_3\\. A description of (i) the categories of data the high-risk artificial\nintelligence system processes as inputs and (ii) the outputs such high-risk\nartificial intelligence system produces;_\n\n_4\\. If the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize such high-risk artificial intelligence system;_\n\n_5\\. A list of any metrics used to evaluate the performance and known\nlimitations of the high-risk artificial intelligence system;_\n\n_6\\. A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that such high-risk artificial intelligence system is in use when\nsuch high-risk artificial intelligence system is in use;_\n\n_7\\. A description of any post-deployment monitoring performed and user\nsafeguards provided concerning such high-risk artificial intelligence system,\nincluding any oversight process established by the deployer to address issues\narising from deployment or use of such high-risk artificial intelligence\nsystem as such issues arise; and_\n\n_8\\. An analysis of such high-risk artificial intelligence system 's validity\nand reliability in accordance with standard industry practices and a\ndescription of any metrics used to evaluate the performance and known\nlimitations of such high-risk artificial intelligence system._\n\n_A single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed or used by a deployer. High-risk\nartificial intelligence systems that are in conformity with the latest version\nof the Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology, Standard ISO/IEC 42001 of the\nInternational Organization for Standardization, or another nationally or\ninternationally recognized risk management framework for artificial\nintelligence systems, or parts thereof, shall be presumed to be in conformity\nwith related requirements set out in this section and in associated\nregulations. If a deployer completes an impact assessment for the purpose of\ncomplying with another applicable law or regulation, such impact assessment\nshall be deemed to satisfy the requirements established in this subsection if\nsuch impact assessment is reasonably similar in scope and effect to the impact\nassessment that would otherwise be completed pursuant to this subsection. A\ndeployer that completes an impact assessment pursuant to this subsection shall\nmaintain such impact assessment and all records concerning such impact\nassessment for three years._\n\n_Throughout the period of time that a high-risk artificial intelligence system\nis deployed and for a period of at least three years following the final\ndeployment of such high-risk artificial intelligence system, the deployer\nshall retain all records concerning each impact assessment conducted on the\nhigh-risk artificial intelligence system, including all raw data used to\nevaluate the performance and known limitations of such system._\n\n_D. Not later than the time that a deployer uses a high-risk artificial\nintelligence system to interact with a consumer, the deployer shall disclose\nto the consumer that the deployer is interacting with an artificial\nintelligence system disclosing (i) the purpose of such high-risk artificial\nintelligence system, (ii) the nature of such system, (iii) the nature of the\nconsequential decision, (iv) the contact information for the deployer, and (v)\na description of the artificial intelligence system in plain language of such\nsystem, which shall include (a) a description of the personal characteristics\nor attributes that such system will measure or assess, (b) the method by which\nthe system measures or assesses such attributes or characteristics, (c) how\nsuch attributes or characteristics are relevant to the consequential decisions\nfor which the system should be used, (d) any human components of such system,\nand (e) how any automated components of such system are used to inform such\nconsequential decisions._\n\n_A deployer that has deployed a high-risk artificial intelligence system to\nmake a consequential decision concerning a consumer shall transmit to the\nconsumer the consequential decision without undue delay. If such consequential\ndecision is adverse to such consumer and based on personal data beyond\ninformation that the consumer provided directly to the deployer, the deployer\nshall provide to the consumer (a) a statement disclosing the principal reason\nor reasons for the consequential decision, including (1) the degree to which\nand manner in which the high-risk artificial intelligence system contributed\nto the consequential decision, (2) the type of data that was processed by such\nsystem in making the consequential decision, and (3) the sources of such data;\n(b) pursuant to subdivision A 2 of § 59.1-577, an opportunity to correct any\ninaccuracies in the consumer's personal data that the high-risk artificial\nintelligence system processed in making, or as a substantial factor in making,\nthe consequential decision; and (c) an opportunity to appeal such adverse\nconsequential decision concerning the consumer arising from the deployment of\nsuch system. Any such appeal shall allow for human review, if technically\nreasonable and practicable, unless providing the opportunity for appeal is not\nin the best interest of the consumer, including instances in which any delay\nmight pose a risk to the life or safety of such consumer._\n\n_E. Each deployer shall make available, in a manner that is clear and readily\navailable, a statement summarizing how such deployer manages any reasonably\nforeseeable risk of algorithmic discrimination that may arise from the use or\ndeployment of the high-risk artificial intelligence system._\n\n_F. For any disclosure required pursuant to this section, each deployer shall,\nno later than 30 days after the deployer is notified by the developer that the\ndeveloper has performed an intentional and substantial modification to any\nhigh-risk artificial intelligence system, update such disclosure as necessary\nto ensure that such disclosure remains accurate._\n\n_G. Any deployer who performs an intentional and substantial modification to\nany high-risk artificial intelligence system shall comply with the\ndocumentation and disclosure requirements for developers pursuant to\nsubsections B through G of § 59.1-608._\n\n_H. Nothing in this section shall be construed to require a deployer to\ndisclose any trade secret, as defined in § 59.1-336, information that could\ncreate a security risk, or other confidential or proprietary information._\n\n_§ 59.1-610. Exemptions._\n\n_A. Nothing in this chapter shall be construed to restrict a developer 's or\ndeployer's ability to (i) comply with federal, state, or municipal ordinances\nor regulations; (ii) comply with a civil, criminal, or regulatory inquiry,\ninvestigation, subpoena, or summons by federal, state, local, or other\ngovernmental authorities; (iii) cooperate with law-enforcement agencies\nconcerning conduct or activity that the developer or deployer reasonably and\nin good faith believes may violate federal, state, or local law, ordinances,\nor regulations; (iv) investigate, establish, exercise, prepare for, or defend\nlegal claims; (v) provide a product or service specifically requested by a\nconsumer; (vi) perform under a contract to which a consumer is a party,\nincluding fulfilling the terms of a written warranty; (vii) take steps at the\nrequest of a consumer prior to entering into a contract; (viii) take immediate\nsteps to protect an interest that is essential for the life or physical safety\nof the consumer or another individual; (ix) prevent, detect, protect against,\nor respond to security incidents, identity theft, fraud, harassment, or\nmalicious or deceptive activities; (x) take actions to prevent, detect,\nprotect against, report, or respond to the production, generation,\nincorporation, or synthesization of child sex abuse material, or any illegal\nactivity, preserve the integrity or security of systems, or investigate,\nreport, or prosecute those responsible for any such action; (xi) engage in\npublic or peer-reviewed scientific or statistical research in the public\ninterest that adheres to all other applicable ethics and privacy laws and is\napproved, monitored, and governed by an institutional review board that\ndetermines, or similar independent oversight entities that determine, (a) that\nthe expected benefits of the research outweigh the risks associated with such\nresearch and (b) whether the developer or deployer has implemented reasonable\nsafeguards to mitigate the risks associated with such research; (xii) assist\nanother developer or deployer with any of the obligations imposed by this\nchapter; or (xiii) take any action that is in the public interest in the areas\nof public health, community health, or population health, but solely to the\nextent that such action is subject to suitable and specific measures to\nsafeguard the public._\n\n_B. The obligations imposed on developers or deployers by this chapter shall\nnot restrict a developer 's or deployer's ability to (i) conduct internal\nresearch to develop, improve, or repair products, services, or technologies;\n(ii) effectuate a product recall; (iii) identify and repair technical errors\nthat impair existing or intended functionality; or (iv) perform internal\noperations that are reasonably aligned with the expectations of the consumer\nor reasonably anticipated based on the consumer's existing relationship with\nthe developer or deployer._\n\n_C. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer to disclose trade secrets or information protected from\ndisclosure by state or federal law._\n\n_D. The obligations imposed on developers or deployers by this chapter shall\nnot apply where compliance by the developer or deployer with such obligations\nwould violate an evidentiary privilege under federal law or the laws of the\nCommonwealth._\n\n_E. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer that adversely affects the legally protected rights or\nfreedoms of any person, including the rights of any person to freedom of\nspeech or freedom of the press guaranteed in the First Amendment to the\nConstitution of the United States or under the Virginia Human Rights Act ( §\n2.2-3900 et seq.)._\n\n_F. The obligations imposed on developers or deployers by this chapter shall\nnot apply to any artificial intelligence system that is acquired by or for the\nfederal government or any federal agency or department, including the U.S.\nDepartment of Commerce, the U.S. Department of Defense, and the National\nAeronautics and Space Administration, unless such artificial intelligence\nsystem is a high-risk artificial intelligence system that is used to make, or\nis a substantial factor in making, a decision concerning employment or\nhousing._\n\n_G. For the purposes of this subsection:_\n\n_\" Affiliate\" means the same as that term is defined in § 6.2-899._\n\n_\" Bank\" means the same as that term is defined in § 6.2-800._\n\n_\" Credit union\" means the same as that term is defined in § 6.2-1300._\n\n_\" Federal credit union\" means a credit union duly organized under federal\nlaw._\n\n_\" Mortgage lender\" means the same as that term is defined in § 6.2-1600._\n\n_\" Out-of-state bank\" means the same as that term is defined in § 6.2-836._\n\n_\" Out-of-state credit union\" means a credit union organized and doing\nbusiness in another state._\n\n_\" Savings institution\" means the same as that term is defined in § 6.2-1100._\n\n_\" Subsidiary\" means the same as that term is defined in § 6.2-700._\n\n_The obligations imposed on developers or deployers by this chapter shall be\ndeemed satisfied for any bank, out-of-state bank, credit union, federal credit\nunion, mortgage lender, out-of-state credit union, savings institution, or any\naffiliate [_ ~~or~~ _, ] subsidiary [ , or service provider ] thereof if such\nbank, out-of-state bank, credit union, federal credit union, mortgage lender,\nout-of-state credit union, savings institution, or affiliate [ or , ]\nsubsidiary [ , or service provider ] is subject to the jurisdiction of any\nstate or federal regulator under any published guidance or regulations that\napply to the use of high-risk artificial intelligence systems and such\nguidance or regulations._\n\n_H. For purposes of this subsection, \"insurer\" means the same as that term is\ndefined in § 38.2-100._\n\n_The provisions of this chapter shall not apply to any insurer, or any high-\nrisk artificial intelligence system developed by or for or deployed by an\ninsurer for use in the business of insurance, if such insurer is regulated and\nsupervised by the State Corporation Commission or a comparable federal\nregulating body and subject to examination by such entity under any existing\nstatutes, rules, or regulations pertaining to unfair trade practices and\nunfair discrimination prohibited under Chapter 5 ( § 38.2-500 et seq.) of\nTitle 38.2, or published guidance or regulations that apply to the use of\nhigh-risk artificial intelligence systems and such guidance or regulations aid\nin the prevention and mitigation of algorithmic discrimination caused by the\nuse of a high-risk artificial intelligence system or any risk of algorithmic\ndiscrimination that is reasonably foreseeable as a result of the use of a\nhigh-risk artificial intelligence system. Nothing in this chapter shall be\nconstrued to delegate existing regulatory oversight of the business of\ninsurance to any department or agency other than the Bureau of Insurance of\nthe Virginia State Corporation Commission._\n\n_I. The provisions of this chapter shall not apply to the development of an\nartificial intelligence system that is used exclusively for research,\ntraining, testing, or other pre-deployment activities performed by active\nparticipants of any sandbox software or sandbox environment established and\nsubject to oversight by a designated agency or other government entity and\nthat is in compliance with the provisions of this chapter._\n\n_J. The provisions of this chapter shall not apply to a developer or deployer,\nor other person who develops, deploys, puts into service, or intentionally\nmodifies, as applicable, a high-risk artificial intelligence system that (i)\nhas been approved, authorized, certified, cleared, developed, or granted by a\nfederal agency acting within the scope of the federal agency 's authority, or\nby a regulated entity subject to the supervision and regulation of the Federal\nHousing Finance Agency or (ii) is in compliance with standards established by\na federal agency or by a regulated entity subject to the supervision and\nregulation of the Federal Housing Finance Agency, if the standards are\nsubstantially equivalent or more stringent than the requirements of this\nchapter._\n\n_K. The provisions of this chapter shall not apply to a developer or deployer,\nor other person that (i) facilitates or engages in the provision of telehealth\nservices, as defined in § 32.1-122.03:1, or (ii) is a covered entity within\nthe meaning of the federal Health Insurance Portability and Accountability Act\nof 1996 (42 U.S.C. § 1320d et seq.) and the regulations promulgated under such\nfederal act, as both may be amended from time to time, and is providing (a)\nhealth care recommendations that (1) are generated by an artificial\nintelligence system and (2) require a health care provider, as defined in §\n8.01-581.1, to take action to implement the recommendations or (b) services\nutilizing an artificial intelligence system for an administrative, quality\nmeasurement, security, or internal cost or performance improvement function._\n\n_L. If a developer or deployer engages in any action authorized by an\nexemption set forth in this section, the developer or deployer bears the\nburden of demonstrating that such action qualifies for such exemption._\n\n_M. If a developer or deployer withholds information pursuant to an exemption\nset forth in this chapter for which disclosure would otherwise be required by\nthis chapter, including the exemption from disclosure of trade secrets, the\ndeveloper or deployer shall notify the subject of disclosure and provide a\nbasis for withholding the information. If a developer or deployer redacts any\ninformation pursuant to an exemption from disclosure, the developer or\ndeployer shall notify the subject of disclosure that the developer or deployer\nis redacting such information and provide the basis for such decision to\nredact._\n\n_§ 59.1-611. Enforcement; civil penalties._\n\n_A. The Attorney General shall have exclusive authority to enforce the\nprovisions of this chapter._\n\n_B. Whenever the Attorney General has reasonable cause to believe that any\nperson has engaged in or is engaging in any violation of this chapter, the\nAttorney General is empowered to issue a civil investigative demand. The\nprovisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative\ndemands issued pursuant to this section. In rendering and furnishing any\ninformation requested pursuant to a civil investigative demand issued pursuant\nto this section, a developer or deployer may redact or omit any trade secrets\nor information protected from disclosure by state or federal law. If a\ndeveloper or deployer refuses to disclose, redacts, or omits information based\non the exemption from disclosure of trade secrets, such developer or deployer\nshall affirmatively state to the Attorney General that the basis for\nnondisclosure, redaction, or omission is because such information is a trade\nsecret. To the extent that any information requested pursuant to a civil\ninvestigative demand issued pursuant to this section is subject to attorney-\nclient privilege or work-product protection, disclosure of such information\npursuant to the civil investigative demand shall not constitute a waiver of\nsuch privilege or protection. Any information, statement, or documentation\nprovided to the Attorney General pursuant to this section shall be exempt from\ndisclosure under the Virginia Freedom of Information Act (§ 2.2-3700 et\nseq.)._\n\n_C. Notwithstanding any contrary provision of law, the Attorney General may\ncause an action to be brought in the appropriate circuit court in the name of\nthe Commonwealth to enjoin any violation of this chapter. The circuit court\nhaving jurisdiction may enjoin such violation notwithstanding the existence of\nan adequate alternative remedy at law._\n\n_D. Any person who violates the provisions of this chapter shall be subject to\na civil penalty in an amount not to exceed $1,000 plus reasonable attorney\nfees, expenses, and costs, as determined by the court. Any person who\nwillfully violates the provisions of this chapter shall be subject to a civil\npenalty in an amount not less than $1,000 and not more than $10,000 plus\nreasonable attorney fees, expenses, and costs, as determined by the court.\nSuch civil penalties shall be paid into the Literary Fund._\n\n_E. Each violation of this chapter shall constitute a separate violation and\nshall be subject to any civil penalties imposed under this section._\n\n_F. The Attorney General may require that a developer disclose to the Attorney\nGeneral any statement or documentation described in this chapter if such\nstatement or documentation is relevant to an investigation conducted by the\nAttorney General. The Attorney General may also require that a deployer\ndisclose to the Attorney General any risk management policy designed and\nimplemented, impact assessment completed, or record maintained pursuant to\nthis chapter if such risk management policy, impact assessment, or record is\nrelevant to an investigation conducted by the Attorney General._\n\n_G. In an action brought by the Attorney General pursuant to this section, it\nshall be an affirmative defense that the developer or deployer (i) discovers a\nviolation of any provision of this chapter through red-teaming or other\nmethod; (ii) no later than 45 days after discovering such violation (a) cures\nsuch violation and (b) provides notice to the Attorney General in a form and\nmanner as prescribed by the Attorney General that such violation has been\ncured and evidence that any harm caused by such violation has been mitigated;\nand (iii) is otherwise in compliance with the requirements of this chapter._\n\n_H. Prior to causing an action against a developer or deployer for a violation\nof this chapter pursuant to subsection C, the Attorney General shall\ndetermine, in consultation with the developer or deployer, if it is possible\nto cure the violation. If it is possible to cure such violation, the Attorney\nGeneral may issue a notice of violation to the developer or deployer and\nafford the developer or deployer the opportunity to cure such violation within\n45 days of the receipt of such notice of violation. In determining whether to\ngrant such opportunity to cure such violation, the Attorney General shall\nconsider (i) the number of violations; (ii) the size and complexity of the\ndeveloper or deployer; (iii) the nature and extent of the developer 's or\ndeployer's business; (iv) the substantial likelihood of injury to the public;\n(v) the safety of persons or property; and (vi) whether such violation was\nlikely caused by human or technical error. If the developer or deployer fails\nto cure such violation within 45 days of the receipt of such notice of\nviolation, the Attorney General may proceed with such action._\n\n_I. Nothing in this chapter shall create a private cause of action in favor of\nany person aggrieved by a violation of this chapter._\n\n_§ 59.1-612. Construction of chapter._\n\n_A. This chapter is declared to be remedial, with the purposes of protecting\nconsumers and ensuring consumers receive information about consequential\ndecisions affecting them. The provisions of this chapter granting rights or\nprotections to consumers shall be construed broadly and exemptions construed\nnarrowly._\n\n_B. If any provision of this chapter or its application to any person or\ncircumstance is held invalid, the invalidity shall not affect other provisions\nor applications of this chapter that can be given effect without the invalid\nprovision or application, and to this end all the provisions of this chapter\nare hereby expressly declared to be severable._\n\n2\\. That the provisions of this act shall become effective on July 1, 2026.\n\n3\\. That compliance with the provisions of Chapter 58 (§ 59.1-607 et seq.) of\nTitle 59.1 of the Code of Virginia, as created by this act, shall not (i)\nrelieve a person from liability for any causes of action that existed at\ncommon law or by statute prior to July 1, 2026, or (ii) be construed to modify\nor otherwise affect, preempt, limit, or displace any causes of action that\nexisted at common law or by statute prior to July 1, 2026.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "02/12/2025",
      "label": "Recommended as Substituted from Committee",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:VA2024000H2094&verid=VA2024000H2094_20250212_0_RS&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2024 VA H 2094</td> <td><table><tr><td class=\"label\">Author:</td> <td>Maldonado</td></tr> <tr><td class=\"label\">Version:</td> <td>Recommended as Substituted from Committee</td></tr> <tr><td class=\"label\">Version Date:</td> <td>02/12/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">\n    <b>HOUSE BILL NO. 2094</b>\n   </p>\n   <p class=\"center\">AMENDMENT IN THE NATURE OF A SUBSTITUTE</p>\n   <p class=\"center\">(Proposed by the Senate Committee on General Laws and Technology on February 12, 2025)</p>\n   <p class=\"center\">(Patron Prior to Substitute--Delegate Maldonado)</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"indent\">\n    <i>A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, relating to high-risk artificial intelligence; development, deployment, and use; civil penalties.</i>\n   </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">\n     <b>Be it enacted by the General Assembly of Virginia:</b>\n    </p>\n   </span>\n   <p class=\"indent\">\n    <b>1. That the Code of Virginia is amended by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as follows:</b>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">CHAPTER 58.</u>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-607. Definitions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">As used in this chapter, unless the context requires a different meaning:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Algorithmic discrimination&quot; means the use of an artificial intelligence system that results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, sexual orientation, veteran status, or other classification protected under state or federal law. &quot;Algorithmic discrimination&quot; does not include (i) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; (ii) the expansion of an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or (iii) an act or omission by or on behalf of a private club or other establishment not in fact open to the public, as set forth in Title II of the Civil Rights Act of 1964, 42 U.S.C. &sect; 2000a(e), as amended from time to time.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Artificial intelligence system&quot; means any machine learning-based system that, for any explicit or implicit objective, infers from the inputs such system receives how to generate outputs, including content, decisions, predictions, and recommendations, that can influence physical or virtual environments. &quot;Artificial intelligence system&quot; does not include any artificial intelligence system or general purpose artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence system or general purpose artificial intelligence model is made available to deployers or consumers.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consequential decision&quot; means any decision that has a material legal, or similarly significant, effect on the provision or denial to any consumer of (i) parole, probation, a pardon, or any other release from incarceration or court supervision; (ii) education enrollment or an education opportunity; (iii) access to employment; (iv) a financial or lending service; (v) access to health care services; (vi) housing; (vii) insurance; (viii) marital status; or (ix) a legal service.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consumer&quot; means a natural person who is a resident of the Commonwealth and is acting only in an individual or household context. &quot;Consumer&quot; does not include a natural person acting in a commercial or employment context.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Deployer&quot; means any person doing business in the Commonwealth that deploys or uses a high-risk artificial intelligence system to make a consequential decision in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Developer&quot; means any person doing business in the Commonwealth that develops or intentionally and substantially modifies a high-risk artificial intelligence system that is offered, sold, leased, given, or otherwise made available to deployers or consumers in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Facial recognition&quot; means the use of a computer system that, for the purpose of attempting to determine the identity of an unknown individual, uses an algorithm to compare the facial biometric data of an unknown individual derived from a photograph, video, or image to a database of photographs or images and associated facial biometric data in order to identify potential matches to an individual. &quot;Facial recognition&quot; does not include facial verification technology, which involves the process of comparing an image or facial biometric data of a known individual, where such information is provided by that individual, to an image database, or to government documentation containing an image of the known individual, to identify a potential match in pursuit of the individual&#39;s identity.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;General-purpose artificial intelligence model&quot; means a model used by an artificial intelligence system or other system that (i) displays significant generality, (ii) is capable of competently performing a wide range of distinct tasks, and (iii) can be integrated into a variety of downstream applications or systems. &quot;General-purpose artificial intelligence model&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is made available to deployers or consumers.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence&quot; means an artificial intelligence system that is capable of producing and used to produce synthetic content, including audio, images, text, and videos.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence system&quot; means any artificial intelligence system or service that incorporates generative artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that is specifically intended to autonomously make, or be a substantial factor in making, a consequential decision. A system or service is not a &quot;high-risk artificial intelligence system&quot; if it is intended to (i) perform a narrow procedural task, (ii) improve the result of a previously completed human activity, (iii) detect any decision-making patterns or any deviations from pre-existing decision-making patterns, or (iv) perform a preparatory task to an assessment relevant to a consequential decision. &quot;High-risk artificial intelligence system&quot; does not include any of the following technologies:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. Anti-fraud technology that does not use facial recognition technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Anti-malware technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Anti-virus technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Artificial intelligence-enabled video games;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. Autonomous vehicle technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. Calculators;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. Cybersecurity technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. Databases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. Data storage;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. Firewall technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. Internet domain registration;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. Internet website loading;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. Networking;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. Spam and robocall filtering;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. Spell-checking technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">16. Spreadsheets;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">17. Web caching;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">18. Web hosting or any similar technology; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">19. Technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an acceptable use policy that prohibits generating content that is discriminatory or unlawful.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Intentional and substantial modification&quot; means any deliberate change made to (i) an artificial intelligence system that results, at the time when the change is implemented and any time thereafter, in any new material risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects compliance of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. &quot;Intentional and substantial modification&quot; does not include (a) any customization made by deployers that (1) is based on legitimate nondiscriminatory business justifications, (2) is within the scope and purpose of the artificial intelligence tool, and (3) that does not result in a material change to the risks of algorithmic discrimination or (b) any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (1) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (2) such change (A) is made to such high-risk artificial intelligence system as a result of any learning described in clause (b) (1) and (B) was predetermined by the deployer or the third party contracted by the deployer and included within the initial impact assessment of such high-risk artificial intelligence system as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Machine learning&quot; means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Person&quot; includes any individual, corporation, partnership, association, cooperative, limited liability company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. &quot;Person&quot; does not include any government or political subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Principal basis&quot; means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human. </u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Red-teaming&quot; means adversarial testing to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Substantial factor&quot; means a factor that (i) uses the principal basis for making a consequential decision, (ii) is capable of altering the outcome of a consequential decision, and (iii) is generated by an artificial intelligence system. &quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as the principal basis to make a consequential decision concerning the consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Synthetic content&quot; means information, such as images, video, audio clips, and, to the extent practicable, text, that has been significantly modified or generated by algorithms, including by artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique, or process, that (i) derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by, other persons who can obtain economic value from its disclosure or use and (ii) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-608. Operating standards for developers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each developer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination arising from the intended and contracted uses. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a developer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the developer complied with the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No developer of a high-risk artificial intelligence system shall offer, sell, lease, give, or otherwise provide to a deployer or other developer a high-risk artificial intelligence system unless the developer makes available to the deployer or other developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement disclosing the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Documentation disclosing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">a. The known or reasonably known limitations of such high-risk artificial intelligence system, including any and all known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">b. The purpose of such high-risk artificial intelligence system and the intended benefits and uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">c. A summary describing how such high-risk artificial intelligence system was evaluated for performance before such high-risk artificial intelligence system was licensed, sold, leased, given, or otherwise made available to a deployer or other developer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">d. The measures the developer has taken to mitigate reasonable foreseeable risks of algorithmic discrimination that the developer knows arises from deployment or use of such high-risk artificial intelligence system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">e. How an individual can use such high-risk artificial intelligence system and monitor the performance of such high-risk artificial intelligence system for any risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Documentation including (i) a description of how the high-risk artificial intelligence system was evaluated for performance and for mitigation of algorithmic discrimination before such system was made available to the deployer or other developer; (ii) a description of the intended outputs of the high-risk artificial intelligence system; (iii) a description of the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and (iv) a description of how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when such system is used to make, or is a substantial factor in making, a consequential decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any additional documentation that is reasonably necessary to assist the deployer or other developer in understanding the outputs and monitoring performance of the high-risk artificial intelligence system for risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Each developer that offers, sells, leases, gives, or otherwise makes available to a deployer or other developer a high-risk artificial intelligence system shall make available to the deployer or other developer to the extent feasible and necessary, information and documentation through artifacts such as system cards or predeployment impact assessments, including any risk management policy designed and implemented and any relevant impact assessment completed, and such documentation and information shall enable the deployer, other developer, or a third party contracted by the deployer to complete an impact assessment as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. A developer that also serves as a deployer for any high-risk artificial intelligence system shall not be required to generate the documentation required by this section unless such high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer or as otherwise required by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this section shall be construed to require a developer to disclose any trade secret, information that could create a security risk, or other confidential or proprietary information protected under state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For any disclosure required pursuant to this section, each developer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. 1. Each developer of a high-risk generative artificial intelligence system that generates or substantially modifies synthetic content shall ensure that the outputs of such high-risk artificial intelligence system (i) are identifiable and detectable in a manner that is accessible by consumers using industry-standard tools or tools provided by the developer; (ii) comply with any applicable accessibility requirements, as synthetic content, to the extent reasonably feasible; and (iii) apply such identification at the time the output is generated;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. If such synthetic content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for identifying outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The identification of outputs required by subdivision 1 shall not apply to (i) synthetic content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Where multiple developers directly contribute to the development of a high-risk artificial intelligence system, each developer shall be subject to the obligations and operating standards applicable to developers pursuant to this section solely with respect to its activities contributing to the development of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-609. Operating standards for deployers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each deployer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a deployer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the deployer complied with the provisions of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has designed and implemented a risk management policy and program for such high-risk artificial intelligence system. The risk management policy shall specify the principles, processes, and personnel that the deployer shall use in maintaining the risk management program to identify, mitigate, and document any risk of algorithmic discrimination that is a reasonably foreseeable consequence of deploying or using such high-risk artificial intelligence system to make a consequential decision. Each risk management policy and program designed, implemented, and maintained pursuant to this subsection shall be reasonable considering the guidance and standards set forth in the latest version of:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. The Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Standard ISO/IEC 42001 of the International Organization for Standardization;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A nationally or internationally recognized risk management framework for artificial intelligence systems with requirements that are substantially equivalent to, and at least as stringent as, the requirements set forth in this section; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any risk management framework for artificial intelligence systems that the Attorney General may designate and is substantially equivalent to, and at least as stringent as, the guidance and standards described in subdivision 1.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Except as provided in this subsection, no deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has completed an impact assessment for such high-risk artificial intelligence system. The deployer shall complete an impact assessment for a high-risk artificial intelligence system (i) before the deployer initially deploys such high-risk artificial intelligence system and (ii) before a significant update to such high-risk artificial intelligence system is used to make a consequential decision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each impact assessment completed pursuant to this subsection shall include, at a minimum:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement by the deployer disclosing (i) the purpose, intended use cases and deployment context of, and benefits afforded by the high-risk artificial intelligence system and (ii) whether the deployment or use of the high-risk artificial intelligence system poses any known or reasonably foreseeable risk of algorithmic discrimination and, if so, (a) the nature of such algorithmic discrimination and (b) the steps that have been taken, to the extent feasible, to mitigate such risk;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. For each post-deployment impact assessment completed pursuant to this subsection, whether the intended use cases of the high-risk artificial intelligence system as updated were consistent with, or varied from, the developer&#39;s intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A description of (i) the categories of data the high-risk artificial intelligence system processes as inputs and (ii) the outputs such high-risk artificial intelligence system produces;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. If the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. A list of any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. A description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that such high-risk artificial intelligence system is in use when such high-risk artificial intelligence system is in use;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. A description of any post-deployment monitoring performed and user safeguards provided concerning such high-risk artificial intelligence system, including any oversight process established by the deployer to address issues arising from deployment or use of such high-risk artificial intelligence system as such issues arise; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. An analysis of such high-risk artificial intelligence system&#39;s validity and reliability in accordance with standard industry practices and a description of any metrics used to evaluate the performance and known limitations of such high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed or used by a deployer. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations. If a deployer completes an impact assessment for the purpose of complying with another applicable law or regulation, such impact assessment shall be deemed to satisfy the requirements established in this subsection if such impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection. A deployer that completes an impact assessment pursuant to this subsection shall maintain such impact assessment and all records concerning such impact assessment for three years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Throughout the period of time that a high-risk artificial intelligence system is deployed and for a period of at least three years following the final deployment of such high-risk artificial intelligence system, the deployer shall retain all records concerning each impact assessment conducted on the high-risk artificial intelligence system, including all raw data used to evaluate the performance and known limitations of such system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Not later than the time that a deployer uses a high-risk artificial intelligence system to interact with a consumer, the deployer shall disclose to the consumer that the deployer is interacting with an artificial intelligence system disclosing (i) the purpose of such high-risk artificial intelligence system, (ii) the nature of such system, (iii) the nature of the consequential decision, (iv) the contact information for the deployer, and (v) a description of the artificial intelligence system in plain language of such system, which shall include (a) a description of the personal characteristics or attributes that such system will measure or assess, (b) the method by which the system measures or assesses such attributes or characteristics, (c) how such attributes or characteristics are relevant to the consequential decisions for which the system should be used, (d) any human components of such system, and (e) how any automated components of such system are used to inform such consequential decisions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A deployer that has deployed a high-risk artificial intelligence system to make a consequential decision concerning a consumer shall transmit to the consumer the consequential decision without undue delay. If such consequential decision is adverse to such consumer and based on personal data beyond information that the consumer provided directly to the deployer, the deployer shall provide to the consumer (a) a statement disclosing the principal reason or reasons for the consequential decision, including (1) the degree to which and manner in which the high-risk artificial intelligence system contributed to the consequential decision, (2) the type of data that was processed by such system in making the consequential decision, and (3) the sources of such data; (b) pursuant to the provisions of the Consumer Data Protection Act (&sect; 59.1-575 et seq.), an opportunity to correct any inaccuracies in the consumer&#39;s personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and (c) an opportunity to appeal such adverse consequential decision concerning the consumer arising from the deployment of such system. Any such appeal shall allow for human review, if technically reasonable and practicable, unless providing the opportunity for appeal is not in the best interest of the consumer, including instances in which any delay might pose a risk to the life or safety of such consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each deployer shall make available, in a manner that is clear and readily available, a statement summarizing how such deployer manages any reasonably foreseeable risk of algorithmic discrimination that may arise from the use or deployment of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each deployer shall, no later than 30 days after the deployer is notified by the developer that the developer has performed an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. Any deployer who performs an intentional and substantial modification to any high-risk artificial intelligence system shall comply with the documentation and disclosure requirements for developers pursuant to subsections B through G of &sect; 59.1-608.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Nothing in this section shall be construed to require a deployer to disclose any trade secret, information that could create a security risk, or other confidential or proprietary information protected under state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-610. Exemptions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Nothing in this chapter shall be construed to restrict a developer&#39;s or deployer&#39;s ability to (i) comply with federal, state, or municipal ordinances or regulations; (ii) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by federal, state, local, or other governmental authorities; (iii) cooperate with law-enforcement agencies concerning conduct or activity that the developer or deployer reasonably and in good faith believes may violate federal, state, or local law, ordinances, or regulations; (iv) investigate, establish, exercise, prepare for, or defend legal claims; (v) provide a product or service specifically requested by a consumer; (vi) perform under a contract to which a consumer is a party, including fulfilling the terms of a written warranty; (vii) take steps at the request of a consumer prior to entering into a contract; (viii) take immediate steps to protect an interest that is essential for the life or physical safety of the consumer or another individual; (ix) prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, or malicious or deceptive activities; (x) take actions to prevent, detect, protect against, report, or respond to the production, generation, incorporation, or synthesization of child sex abuse material, or any illegal activity, preserve the integrity or security of systems, or investigate, report, or prosecute those responsible for any such action; (xi) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is approved, monitored, and governed by an institutional review board that determines, or similar independent oversight entities that determine, (a) that the expected benefits of the research outweigh the risks associated with such research and (b) whether the developer or deployer has implemented reasonable safeguards to mitigate the risks associated with such research; (xii) assist another developer or deployer with any of the obligations imposed by this chapter; or (xiii) take any action that is in the public interest in the areas of public health, community health, or population health, but solely to the extent that such action is subject to suitable and specific measures to safeguard the public.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. The obligations imposed on developers or deployers by this chapter shall not restrict a developer&#39;s or deployer&#39;s ability to (i) conduct internal research to develop, improve, or repair products, services, or technologies; (ii) effectuate a product recall; (iii) identify and repair technical errors that impair existing or intended functionality; or (iv) perform internal operations that are reasonably aligned with the expectations of the consumer or reasonably anticipated based on the consumer&#39;s existing relationship with the developer or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer to disclose trade secrets or information protected from disclosure by state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. The obligations imposed on developers or deployers by this chapter shall not apply where compliance by the developer or deployer with such obligations would violate an evidentiary privilege under federal law or the laws of the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer that adversely affects the legally protected rights or freedoms of any person, including the rights of any person to freedom of speech or freedom of the press guaranteed in the First Amendment to the Constitution of the United States or under the Virginia Human Rights Act (&sect; 2.2-3900 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The obligations imposed on developers or deployers by this chapter shall not apply to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the U.S. Department of Commerce, the U.S. Department of Defense, and the National Aeronautics and Space Administration, unless such artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For the purposes of this subsection:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Affiliate&quot; means the same as that term is defined in &sect; 6.2-899.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Bank&quot; means the same as that term is defined in &sect; 6.2-800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Credit union&quot; means the same as that term is defined in &sect; 6.2-1300.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Federal credit union&quot; means a credit union duly organized under federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Mortgage lender&quot; means the same as that term is defined in &sect; 6.2-1600.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state bank&quot; means the same as that term is defined in &sect; 6.2-836.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state credit union&quot; means a credit union organized and doing business in another state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Savings institution&quot; means the same as that term is defined in &sect; 6.2-1100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Subsidiary&quot; means the same as that term is defined in &sect; 6.2-700.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The obligations imposed on developers or deployers by this chapter shall be deemed satisfied for any bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or any affiliate, subsidiary, or service provider thereof if such bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or affiliate, subsidiary, or service provider is subject to the jurisdiction of any state or federal regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. For purposes of this subsection, &quot;insurer&quot; means the same as that term is defined in &sect; 38.2-100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The provisions of this chapter shall not apply to any insurer, or any high-risk artificial intelligence system developed by or for or deployed by an insurer for use in the business of insurance, if such insurer is regulated and supervised by the State Corporation Commission or a comparable federal regulating body and subject to examination by such entity under any existing statutes, rules, or regulations pertaining to unfair trade practices and unfair discrimination prohibited under Chapter 5 (&sect; 38.2-500 et seq.) of Title 38.2, or published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations aid in the prevention and mitigation of algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. Nothing in this chapter shall be construed to delegate existing regulatory oversight of the business of insurance to any department or agency other than the Bureau of Insurance of the Virginia State Corporation Commission.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. The provisions of this chapter shall not apply to the development of an artificial intelligence system that is used exclusively for research, training, testing, or other pre-deployment activities performed by active participants of any sandbox software or sandbox environment established and subject to oversight by a designated agency or other government entity and that is in compliance with the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">J. The provisions of this chapter shall not apply to a developer or deployer, or other person who develops, deploys, puts into service, or intentionally modifies, as applicable, a high-risk artificial intelligence system that (i) has been approved, authorized, certified, cleared, developed, or granted by a federal agency acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency or (ii) is in compliance with standards established by a federal agency or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">K. The provisions of this chapter shall not apply to a developer or deployer, or other person that (i) facilitates or engages in the provision of telehealth services, as defined in &sect; 32.1-122.03:1, or (ii) is a covered entity within the meaning of the federal Health Insurance Portability and Accountability Act of 1996 (42 U.S.C. &sect; 1320d et seq.) and the regulations promulgated under such federal act, as both may be amended from time to time, and is providing (a) health care recommendations that (1) are generated by an artificial intelligence system and (2) require a health care provider, as defined in &sect; 8.01-581.1, to take action to implement the recommendations or (b) services utilizing an artificial intelligence system for an administrative, quality measurement, security, or internal cost or performance improvement function.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">L. If a developer or deployer engages in any action authorized by an exemption set forth in this section, the developer or deployer bears the burden of demonstrating that such action qualifies for such exemption.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">M. If a developer or deployer withholds information pursuant to an exemption set forth in this chapter for which disclosure would otherwise be required by this chapter, including the exemption from disclosure of trade secrets, the developer or deployer shall notify the subject of disclosure and provide a basis for withholding the information. If a developer or deployer redacts any information pursuant to an exemption from disclosure, the developer or deployer shall notify the subject of disclosure that the developer or deployer is redacting such information and provide the basis for such decision to redact.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-611. Enforcement; civil penalties.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. The Attorney General shall have exclusive authority to enforce the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any violation of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of &sect; 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. If a developer or deployer refuses to disclose, redacts, or omits information based on the exemption from disclosure of trade secrets, such developer or deployer shall affirmatively state to the Attorney General that the basis for nondisclosure, redaction, or omission is because such information is a trade secret. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand shall not constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be exempt from disclosure under the Virginia Freedom of Information Act (&sect; 2.2-3700 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any violation of this chapter. The circuit court having jurisdiction may enjoin such violation notwithstanding the existence of an adequate alternative remedy at law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Any person who violates the provisions of this chapter shall be subject to a civil penalty in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil penalty in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil penalties shall be paid into the Literary Fund.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each violation of this chapter shall constitute a separate violation and shall be subject to any civil penalties imposed under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The Attorney General may require that a developer disclose to the Attorney General any statement or documentation described in this chapter if such statement or documentation is relevant to an investigation conducted by the Attorney General. The Attorney General may also require that a deployer disclose to the Attorney General any risk management policy designed and implemented, impact assessment completed, or record maintained pursuant to this chapter if such risk management policy, impact assessment, or record is relevant to an investigation conducted by the Attorney General.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. In an action brought by the Attorney General pursuant to this section, it shall be an affirmative defense that the developer or deployer (i) discovers a violation of any provision of this chapter through red-teaming or other method; (ii) no later than 45 days after discovering such violation (a) cures such violation and (b) provides notice to the Attorney General in a form and manner as prescribed by the Attorney General that such violation has been cured and evidence that any harm caused by such violation has been mitigated; and (iii) is otherwise in compliance with the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Prior to causing an action against a developer or deployer for a violation of this chapter pursuant to subsection C, the Attorney General shall determine, in consultation with the developer or deployer, if it is possible to cure the violation. If it is possible to cure such violation, the Attorney General may issue a notice of violation to the developer or deployer and afford the developer or deployer the opportunity to cure such violation within 45 days of the receipt of such notice of violation. In determining whether to grant such opportunity to cure such violation, the Attorney General shall consider (i) the number of violations; (ii) the size and complexity of the developer or deployer; (iii) the nature and extent of the developer&#39;s or deployer&#39;s business; (iv) the substantial likelihood of injury to the public; (v) the safety of persons or property; and (vi) whether such violation was likely caused by human or technical error. If the developer or deployer fails to cure such violation within 45 days of the receipt of such notice of violation, the Attorney General may proceed with such action.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Nothing in this chapter shall create a private cause of action in favor of any person aggrieved by a violation of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-612. Construction of chapter.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. This chapter is declared to be remedial, with the purposes of protecting consumers and ensuring consumers receive information about consequential decisions affecting them. The provisions of this chapter granting rights or protections to consumers shall be construed broadly and exemptions construed narrowly.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. If any provision of this chapter or its application to any person or circumstance is held invalid, the invalidity shall not affect other provisions or applications of this chapter that can be given effect without the invalid provision or application, and to this end all the provisions of this chapter are hereby expressly declared to be severable.</u>\n   </p>\n   <effective_clause>\n    <p class=\"indent\">\n     <b>2. That the provisions of this act shall become effective on July 1, 2026.</b>\n    </p>\n   </effective_clause>\n   <p class=\"indent\">\n    <b>3. That compliance with the provisions of Chapter 58 (&sect; 59.1-607 et seq.) of Title 59.1 of the Code of Virginia, as created by this act, shall not (i) relieve a person from liability for any causes of action that existed at common law or by statute prior to July 1, 2026, or (ii) be construed to modify or otherwise affect, preempt, limit, or displace any causes of action that existed at common law or by statute prior to July 1, 2026.</b>\n   </p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2024 VA H 2094 | | Author: | Maldonado  \n---|---  \nVersion: | Recommended as Substituted from Committee  \nVersion Date: | 02/12/2025  \n  \n**HOUSE BILL NO. 2094**\n\nAMENDMENT IN THE NATURE OF A SUBSTITUTE\n\n(Proposed by the Senate Committee on General Laws and Technology on February\n12, 2025)\n\n(Patron Prior to Substitute--Delegate Maldonado)\n\n_A BILL to amend the Code of Virginia by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612,\nrelating to high-risk artificial intelligence; development, deployment, and\nuse; civil penalties._\n\n**Be it enacted by the General Assembly of Virginia:**\n\n**1\\. That the Code of Virginia is amended by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as\nfollows:**\n\n_CHAPTER 58._\n\n_HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT._\n\n**_§ 59.1-607. Definitions._ **\n\n_As used in this chapter, unless the context requires a different meaning:_\n\n_\" Algorithmic discrimination\" means the use of an artificial intelligence\nsystem that results in an unlawful differential treatment or impact that\ndisfavors an individual or group of individuals on the basis of their actual\nor perceived age, color, disability, ethnicity, genetic information, limited\nproficiency in the English language, national origin, race, religion,\nreproductive health, sex, sexual orientation, veteran status, or other\nclassification protected under state or federal law. \"Algorithmic\ndiscrimination\" does not include (i) the offer, license, or use of a high-risk\nartificial intelligence system by a developer or deployer for the sole purpose\nof the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; (ii) the expansion of an applicant, customer, or participant pool to\nincrease diversity or redress historical discrimination; or (iii) an act or\nomission by or on behalf of a private club or other establishment not in fact\nopen to the public, as set forth in Title II of the Civil Rights Act of 1964,\n42 U.S.C. § 2000a(e), as amended from time to time._\n\n_\" Artificial intelligence system\" means any machine learning-based system\nthat, for any explicit or implicit objective, infers from the inputs such\nsystem receives how to generate outputs, including content, decisions,\npredictions, and recommendations, that can influence physical or virtual\nenvironments. \"Artificial intelligence system\" does not include any artificial\nintelligence system or general purpose artificial intelligence model that is\nused for development, prototyping, and research activities before such\nartificial intelligence system or general purpose artificial intelligence\nmodel is made available to deployers or consumers._\n\n_\" Consequential decision\" means any decision that has a material legal, or\nsimilarly significant, effect on the provision or denial to any consumer of\n(i) parole, probation, a pardon, or any other release from incarceration or\ncourt supervision; (ii) education enrollment or an education opportunity;\n(iii) access to employment; (iv) a financial or lending service; (v) access to\nhealth care services; (vi) housing; (vii) insurance; (viii) marital status; or\n(ix) a legal service._\n\n_\" Consumer\" means a natural person who is a resident of the Commonwealth and\nis acting only in an individual or household context. \"Consumer\" does not\ninclude a natural person acting in a commercial or employment context._\n\n_\" Deployer\" means any person doing business in the Commonwealth that deploys\nor uses a high-risk artificial intelligence system to make a consequential\ndecision in the Commonwealth._\n\n_\" Developer\" means any person doing business in the Commonwealth that\ndevelops or intentionally and substantially modifies a high-risk artificial\nintelligence system that is offered, sold, leased, given, or otherwise made\navailable to deployers or consumers in the Commonwealth._\n\n_\" Facial recognition\" means the use of a computer system that, for the\npurpose of attempting to determine the identity of an unknown individual, uses\nan algorithm to compare the facial biometric data of an unknown individual\nderived from a photograph, video, or image to a database of photographs or\nimages and associated facial biometric data in order to identify potential\nmatches to an individual. \"Facial recognition\" does not include facial\nverification technology, which involves the process of comparing an image or\nfacial biometric data of a known individual, where such information is\nprovided by that individual, to an image database, or to government\ndocumentation containing an image of the known individual, to identify a\npotential match in pursuit of the individual's identity._\n\n_\" General-purpose artificial intelligence model\" means a model used by an\nartificial intelligence system or other system that (i) displays significant\ngenerality, (ii) is capable of competently performing a wide range of distinct\ntasks, and (iii) can be integrated into a variety of downstream applications\nor systems. \"General-purpose artificial intelligence model\" does not include\nany artificial intelligence model that is used for development, prototyping,\nand research activities before such artificial intelligence model is made\navailable to deployers or consumers._\n\n_\" Generative artificial intelligence\" means an artificial intelligence system\nthat is capable of producing and used to produce synthetic content, including\naudio, images, text, and videos._\n\n_\" Generative artificial intelligence system\" means any artificial\nintelligence system or service that incorporates generative artificial\nintelligence._\n\n_\" High-risk artificial intelligence system\" means any artificial intelligence\nsystem that is specifically intended to autonomously make, or be a substantial\nfactor in making, a consequential decision. A system or service is not a\n\"high-risk artificial intelligence system\" if it is intended to (i) perform a\nnarrow procedural task, (ii) improve the result of a previously completed\nhuman activity, (iii) detect any decision-making patterns or any deviations\nfrom pre-existing decision-making patterns, or (iv) perform a preparatory task\nto an assessment relevant to a consequential decision. \"High-risk artificial\nintelligence system\" does not include any of the following technologies:_\n\n_1\\. Anti-fraud technology that does not use facial recognition technology;_\n\n_2\\. Anti-malware technology;_\n\n_3\\. Anti-virus technology;_\n\n_4\\. Artificial intelligence-enabled video games;_\n\n_5\\. Autonomous vehicle technology;_\n\n_6\\. Calculators;_\n\n_7\\. Cybersecurity technology;_\n\n_8\\. Databases;_\n\n_9\\. Data storage;_\n\n_10\\. Firewall technology;_\n\n_11\\. Internet domain registration;_\n\n_12\\. Internet website loading;_\n\n_13\\. Networking;_\n\n_14\\. Spam and robocall filtering;_\n\n_15\\. Spell-checking technology;_\n\n_16\\. Spreadsheets;_\n\n_17\\. Web caching;_\n\n_18\\. Web hosting or any similar technology; or_\n\n_19\\. Technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an acceptable use\npolicy that prohibits generating content that is discriminatory or unlawful._\n\n_\" Intentional and substantial modification\" means any deliberate change made\nto (i) an artificial intelligence system that results, at the time when the\nchange is implemented and any time thereafter, in any new material risk of\nalgorithmic discrimination or (ii) a general-purpose artificial intelligence\nmodel that affects compliance of the general-purpose artificial intelligence\nmodel, materially changes the purpose of the general-purpose artificial\nintelligence model, or results in any new reasonably foreseeable risk of\nalgorithmic discrimination. \"Intentional and substantial modification\" does\nnot include (a) any customization made by deployers that (1) is based on\nlegitimate nondiscriminatory business justifications, (2) is within the scope\nand purpose of the artificial intelligence tool, and (3) that does not result\nin a material change to the risks of algorithmic discrimination or (b) any\nchange made to a high-risk artificial intelligence system, or the performance\nof a high-risk artificial intelligence system, if (1) the high-risk artificial\nintelligence system continues to learn after such high-risk artificial\nintelligence system is offered, sold, leased, licensed, given, or otherwise\nmade available to a deployer, or deployed, and (2) such change (A) is made to\nsuch high-risk artificial intelligence system as a result of any learning\ndescribed in clause (b) (1) and (B) was predetermined by the deployer or the\nthird party contracted by the deployer and included within the initial impact\nassessment of such high-risk artificial intelligence system as required in §\n59.1-609._\n\n_\" Machine learning\" means the development of algorithms to build data-derived\nstatistical models that are capable of drawing inferences from previously\nunseen data without explicit human instruction._\n\n_\" Person\" includes any individual, corporation, partnership, association,\ncooperative, limited liability company, trust, joint venture, or any other\nlegal or commercial entity and any successor, representative, agent, agency,\nor instrumentality thereof. \"Person\" does not include any government or\npolitical subdivision._\n\n_\" Principal basis\" means the use of an output of a high-risk artificial\nintelligence system to make a decision without (i) human review, oversight,\ninvolvement, or intervention or (ii) meaningful consideration by a human. _\n\n_\" Red-teaming\" means adversarial testing to identify the potential adverse\nbehaviors or outcomes of an artificial intelligence system, identify how such\nbehaviors or outcomes occur, and stress test the safeguards against such\nbehaviors or outcomes._\n\n_\" Substantial factor\" means a factor that (i) uses the principal basis for\nmaking a consequential decision, (ii) is capable of altering the outcome of a\nconsequential decision, and (iii) is generated by an artificial intelligence\nsystem. \"Substantial factor\" includes any use of an artificial intelligence\nsystem to generate any content, decision, prediction, or recommendation\nconcerning a consumer that is used as the principal basis to make a\nconsequential decision concerning the consumer._\n\n_\" Synthetic content\" means information, such as images, video, audio clips,\nand, to the extent practicable, text, that has been significantly modified or\ngenerated by algorithms, including by artificial intelligence._\n\n_\" Trade secret\" means information, including a formula, pattern, compilation,\nprogram, device, method, technique, or process, that (i) derives independent\neconomic value, actual or potential, from not being generally known to, and\nnot being readily ascertainable by proper means by, other persons who can\nobtain economic value from its disclosure or use and (ii) is the subject of\nefforts that are reasonable under the circumstances to maintain its secrecy._\n\n**_§ 59.1-608. Operating standards for developers of high-risk artificial\nintelligence systems._ **\n\n_A. Each developer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination arising from the intended and\ncontracted uses. In any enforcement action brought by the Attorney General\npursuant to § 59.1-611, there shall be a rebuttable presumption that a\ndeveloper of a high-risk artificial intelligence system used a reasonable duty\nof care as required by this subsection if the developer complied with the\nrequirements of this section._\n\n_B. No developer of a high-risk artificial intelligence system shall offer,\nsell, lease, give, or otherwise provide to a deployer or other developer a\nhigh-risk artificial intelligence system unless the developer makes available\nto the deployer or other developer:_\n\n_1\\. A statement disclosing the intended uses of such high-risk artificial\nintelligence system;_\n\n_2\\. Documentation disclosing the following:_\n\n_a. The known or reasonably known limitations of such high-risk artificial\nintelligence system, including any and all known or reasonably foreseeable\nrisks of algorithmic discrimination arising from the intended uses of such\nhigh-risk artificial intelligence system;_\n\n_b. The purpose of such high-risk artificial intelligence system and the\nintended benefits and uses of such high-risk artificial intelligence system;_\n\n_c. A summary describing how such high-risk artificial intelligence system was\nevaluated for performance before such high-risk artificial intelligence system\nwas licensed, sold, leased, given, or otherwise made available to a deployer\nor other developer;_\n\n_d. The measures the developer has taken to mitigate reasonable foreseeable\nrisks of algorithmic discrimination that the developer knows arises from\ndeployment or use of such high-risk artificial intelligence system; and_\n\n_e. How an individual can use such high-risk artificial intelligence system\nand monitor the performance of such high-risk artificial intelligence system\nfor any risk of algorithmic discrimination;_\n\n_3\\. Documentation including (i) a description of how the high-risk artificial\nintelligence system was evaluated for performance and for mitigation of\nalgorithmic discrimination before such system was made available to the\ndeployer or other developer; (ii) a description of the intended outputs of the\nhigh-risk artificial intelligence system; (iii) a description of the measures\nthe developer has taken to mitigate known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the reasonably foreseeable\ndeployment of the high-risk artificial intelligence system; and (iv) a\ndescription of how the high-risk artificial intelligence system should be\nused, not be used, and be monitored by an individual when such system is used\nto make, or is a substantial factor in making, a consequential decision; and_\n\n_4\\. Any additional documentation that is reasonably necessary to assist the\ndeployer or other developer in understanding the outputs and monitoring\nperformance of the high-risk artificial intelligence system for risks of\nalgorithmic discrimination._\n\n_C. Each developer that offers, sells, leases, gives, or otherwise makes\navailable to a deployer or other developer a high-risk artificial intelligence\nsystem shall make available to the deployer or other developer to the extent\nfeasible and necessary, information and documentation through artifacts such\nas system cards or predeployment impact assessments, including any risk\nmanagement policy designed and implemented and any relevant impact assessment\ncompleted, and such documentation and information shall enable the deployer,\nother developer, or a third party contracted by the deployer to complete an\nimpact assessment as required in § 59.1-609._\n\n_D. A developer that also serves as a deployer for any high-risk artificial\nintelligence system shall not be required to generate the documentation\nrequired by this section unless such high-risk artificial intelligence system\nis provided to an unaffiliated entity acting as a deployer or as otherwise\nrequired by law._\n\n_E. Nothing in this section shall be construed to require a developer to\ndisclose any trade secret, information that could create a security risk, or\nother confidential or proprietary information protected under state or federal\nlaw._\n\n_F. High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_G. For any disclosure required pursuant to this section, each developer\nshall, no later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_H. 1. Each developer of a high-risk generative artificial intelligence system\nthat generates or substantially modifies synthetic content shall ensure that\nthe outputs of such high-risk artificial intelligence system (i) are\nidentifiable and detectable in a manner that is accessible by consumers using\nindustry-standard tools or tools provided by the developer; (ii) comply with\nany applicable accessibility requirements, as synthetic content, to the extent\nreasonably feasible; and (iii) apply such identification at the time the\noutput is generated;_\n\n_2\\. If such synthetic content is an audio, image, or video format that forms\npart of an evidently artistic, creative, satirical, fictional analogous work\nor program, such requirement for identifying outputs of high-risk artificial\nintelligence systems pursuant to subdivision 1 shall be limited to a manner\nthat does not hinder the display or enjoyment of such work or program._\n\n_3\\. The identification of outputs required by subdivision 1 shall not apply\nto (i) synthetic content that consists exclusively of text, is published to\ninform the public on any matter of public interest, or is unlikely to mislead\na reasonable person consuming such synthetic content or (ii) the outputs of a\nhigh-risk artificial intelligence system that performs an assistive function\nfor standard editing, does not substantially alter the input data provided by\nthe developer, or is used to detect, prevent, investigate, or prosecute any\ncrime as authorized by law._\n\n_I. Where multiple developers directly contribute to the development of a\nhigh-risk artificial intelligence system, each developer shall be subject to\nthe obligations and operating standards applicable to developers pursuant to\nthis section solely with respect to its activities contributing to the\ndevelopment of the high-risk artificial intelligence system._\n\n**_§ 59.1-609. Operating standards for deployers of high-risk artificial\nintelligence systems._ **\n\n_A. Each deployer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-611, there shall be a\nrebuttable presumption that a deployer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if the\ndeployer complied with the provisions of this section._\n\n_B. No deployer shall deploy or use a high-risk artificial intelligence system\nto make a consequential decision unless the deployer has designed and\nimplemented a risk management policy and program for such high-risk artificial\nintelligence system. The risk management policy shall specify the principles,\nprocesses, and personnel that the deployer shall use in maintaining the risk\nmanagement program to identify, mitigate, and document any risk of algorithmic\ndiscrimination that is a reasonably foreseeable consequence of deploying or\nusing such high-risk artificial intelligence system to make a consequential\ndecision. Each risk management policy and program designed, implemented, and\nmaintained pursuant to this subsection shall be reasonable considering the\nguidance and standards set forth in the latest version of:_\n\n_1\\. The Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology;_\n\n_2\\. Standard ISO/IEC 42001 of the International Organization for\nStandardization;_\n\n_3\\. A nationally or internationally recognized risk management framework for\nartificial intelligence systems with requirements that are substantially\nequivalent to, and at least as stringent as, the requirements set forth in\nthis section; or_\n\n_4\\. Any risk management framework for artificial intelligence systems that\nthe Attorney General may designate and is substantially equivalent to, and at\nleast as stringent as, the guidance and standards described in subdivision 1._\n\n_High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_C. Except as provided in this subsection, no deployer shall deploy or use a\nhigh-risk artificial intelligence system to make a consequential decision\nunless the deployer has completed an impact assessment for such high-risk\nartificial intelligence system. The deployer shall complete an impact\nassessment for a high-risk artificial intelligence system (i) before the\ndeployer initially deploys such high-risk artificial intelligence system and\n(ii) before a significant update to such high-risk artificial intelligence\nsystem is used to make a consequential decision._\n\n_Each impact assessment completed pursuant to this subsection shall include,\nat a minimum:_\n\n_1\\. A statement by the deployer disclosing (i) the purpose, intended use\ncases and deployment context of, and benefits afforded by the high-risk\nartificial intelligence system and (ii) whether the deployment or use of the\nhigh-risk artificial intelligence system poses any known or reasonably\nforeseeable risk of algorithmic discrimination and, if so, (a) the nature of\nsuch algorithmic discrimination and (b) the steps that have been taken, to the\nextent feasible, to mitigate such risk;_\n\n_2\\. For each post-deployment impact assessment completed pursuant to this\nsubsection, whether the intended use cases of the high-risk artificial\nintelligence system as updated were consistent with, or varied from, the\ndeveloper 's intended uses of such high-risk artificial intelligence system;_\n\n_3\\. A description of (i) the categories of data the high-risk artificial\nintelligence system processes as inputs and (ii) the outputs such high-risk\nartificial intelligence system produces;_\n\n_4\\. If the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize such high-risk artificial intelligence system;_\n\n_5\\. A list of any metrics used to evaluate the performance and known\nlimitations of the high-risk artificial intelligence system;_\n\n_6\\. A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that such high-risk artificial intelligence system is in use when\nsuch high-risk artificial intelligence system is in use;_\n\n_7\\. A description of any post-deployment monitoring performed and user\nsafeguards provided concerning such high-risk artificial intelligence system,\nincluding any oversight process established by the deployer to address issues\narising from deployment or use of such high-risk artificial intelligence\nsystem as such issues arise; and_\n\n_8\\. An analysis of such high-risk artificial intelligence system 's validity\nand reliability in accordance with standard industry practices and a\ndescription of any metrics used to evaluate the performance and known\nlimitations of such high-risk artificial intelligence system._\n\n_A single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed or used by a deployer. High-risk\nartificial intelligence systems that are in conformity with the latest version\nof the Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology, Standard ISO/IEC 42001 of the\nInternational Organization for Standardization, or another nationally or\ninternationally recognized risk management framework for artificial\nintelligence systems, or parts thereof, shall be presumed to be in conformity\nwith related requirements set out in this section and in associated\nregulations. If a deployer completes an impact assessment for the purpose of\ncomplying with another applicable law or regulation, such impact assessment\nshall be deemed to satisfy the requirements established in this subsection if\nsuch impact assessment is reasonably similar in scope and effect to the impact\nassessment that would otherwise be completed pursuant to this subsection. A\ndeployer that completes an impact assessment pursuant to this subsection shall\nmaintain such impact assessment and all records concerning such impact\nassessment for three years._\n\n_Throughout the period of time that a high-risk artificial intelligence system\nis deployed and for a period of at least three years following the final\ndeployment of such high-risk artificial intelligence system, the deployer\nshall retain all records concerning each impact assessment conducted on the\nhigh-risk artificial intelligence system, including all raw data used to\nevaluate the performance and known limitations of such system._\n\n_D. Not later than the time that a deployer uses a high-risk artificial\nintelligence system to interact with a consumer, the deployer shall disclose\nto the consumer that the deployer is interacting with an artificial\nintelligence system disclosing (i) the purpose of such high-risk artificial\nintelligence system, (ii) the nature of such system, (iii) the nature of the\nconsequential decision, (iv) the contact information for the deployer, and (v)\na description of the artificial intelligence system in plain language of such\nsystem, which shall include (a) a description of the personal characteristics\nor attributes that such system will measure or assess, (b) the method by which\nthe system measures or assesses such attributes or characteristics, (c) how\nsuch attributes or characteristics are relevant to the consequential decisions\nfor which the system should be used, (d) any human components of such system,\nand (e) how any automated components of such system are used to inform such\nconsequential decisions._\n\n_A deployer that has deployed a high-risk artificial intelligence system to\nmake a consequential decision concerning a consumer shall transmit to the\nconsumer the consequential decision without undue delay. If such consequential\ndecision is adverse to such consumer and based on personal data beyond\ninformation that the consumer provided directly to the deployer, the deployer\nshall provide to the consumer (a) a statement disclosing the principal reason\nor reasons for the consequential decision, including (1) the degree to which\nand manner in which the high-risk artificial intelligence system contributed\nto the consequential decision, (2) the type of data that was processed by such\nsystem in making the consequential decision, and (3) the sources of such data;\n(b) pursuant to the provisions of the Consumer Data Protection Act ( §\n59.1-575 et seq.), an opportunity to correct any inaccuracies in the\nconsumer's personal data that the high-risk artificial intelligence system\nprocessed in making, or as a substantial factor in making, the consequential\ndecision; and (c) an opportunity to appeal such adverse consequential decision\nconcerning the consumer arising from the deployment of such system. Any such\nappeal shall allow for human review, if technically reasonable and\npracticable, unless providing the opportunity for appeal is not in the best\ninterest of the consumer, including instances in which any delay might pose a\nrisk to the life or safety of such consumer._\n\n_E. Each deployer shall make available, in a manner that is clear and readily\navailable, a statement summarizing how such deployer manages any reasonably\nforeseeable risk of algorithmic discrimination that may arise from the use or\ndeployment of the high-risk artificial intelligence system._\n\n_F. For any disclosure required pursuant to this section, each deployer shall,\nno later than 30 days after the deployer is notified by the developer that the\ndeveloper has performed an intentional and substantial modification to any\nhigh-risk artificial intelligence system, update such disclosure as necessary\nto ensure that such disclosure remains accurate._\n\n_G. Any deployer who performs an intentional and substantial modification to\nany high-risk artificial intelligence system shall comply with the\ndocumentation and disclosure requirements for developers pursuant to\nsubsections B through G of § 59.1-608._\n\n_H. Nothing in this section shall be construed to require a deployer to\ndisclose any trade secret, information that could create a security risk, or\nother confidential or proprietary information protected under state or federal\nlaw._\n\n**_§ 59.1-610. Exemptions._ **\n\n_A. Nothing in this chapter shall be construed to restrict a developer 's or\ndeployer's ability to (i) comply with federal, state, or municipal ordinances\nor regulations; (ii) comply with a civil, criminal, or regulatory inquiry,\ninvestigation, subpoena, or summons by federal, state, local, or other\ngovernmental authorities; (iii) cooperate with law-enforcement agencies\nconcerning conduct or activity that the developer or deployer reasonably and\nin good faith believes may violate federal, state, or local law, ordinances,\nor regulations; (iv) investigate, establish, exercise, prepare for, or defend\nlegal claims; (v) provide a product or service specifically requested by a\nconsumer; (vi) perform under a contract to which a consumer is a party,\nincluding fulfilling the terms of a written warranty; (vii) take steps at the\nrequest of a consumer prior to entering into a contract; (viii) take immediate\nsteps to protect an interest that is essential for the life or physical safety\nof the consumer or another individual; (ix) prevent, detect, protect against,\nor respond to security incidents, identity theft, fraud, harassment, or\nmalicious or deceptive activities; (x) take actions to prevent, detect,\nprotect against, report, or respond to the production, generation,\nincorporation, or synthesization of child sex abuse material, or any illegal\nactivity, preserve the integrity or security of systems, or investigate,\nreport, or prosecute those responsible for any such action; (xi) engage in\npublic or peer-reviewed scientific or statistical research in the public\ninterest that adheres to all other applicable ethics and privacy laws and is\napproved, monitored, and governed by an institutional review board that\ndetermines, or similar independent oversight entities that determine, (a) that\nthe expected benefits of the research outweigh the risks associated with such\nresearch and (b) whether the developer or deployer has implemented reasonable\nsafeguards to mitigate the risks associated with such research; (xii) assist\nanother developer or deployer with any of the obligations imposed by this\nchapter; or (xiii) take any action that is in the public interest in the areas\nof public health, community health, or population health, but solely to the\nextent that such action is subject to suitable and specific measures to\nsafeguard the public._\n\n_B. The obligations imposed on developers or deployers by this chapter shall\nnot restrict a developer 's or deployer's ability to (i) conduct internal\nresearch to develop, improve, or repair products, services, or technologies;\n(ii) effectuate a product recall; (iii) identify and repair technical errors\nthat impair existing or intended functionality; or (iv) perform internal\noperations that are reasonably aligned with the expectations of the consumer\nor reasonably anticipated based on the consumer's existing relationship with\nthe developer or deployer._\n\n_C. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer to disclose trade secrets or information protected from\ndisclosure by state or federal law._\n\n_D. The obligations imposed on developers or deployers by this chapter shall\nnot apply where compliance by the developer or deployer with such obligations\nwould violate an evidentiary privilege under federal law or the laws of the\nCommonwealth._\n\n_E. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer that adversely affects the legally protected rights or\nfreedoms of any person, including the rights of any person to freedom of\nspeech or freedom of the press guaranteed in the First Amendment to the\nConstitution of the United States or under the Virginia Human Rights Act ( §\n2.2-3900 et seq.)._\n\n_F. The obligations imposed on developers or deployers by this chapter shall\nnot apply to any artificial intelligence system that is acquired by or for the\nfederal government or any federal agency or department, including the U.S.\nDepartment of Commerce, the U.S. Department of Defense, and the National\nAeronautics and Space Administration, unless such artificial intelligence\nsystem is a high-risk artificial intelligence system that is used to make, or\nis a substantial factor in making, a decision concerning employment or\nhousing._\n\n_G. For the purposes of this subsection:_\n\n_\" Affiliate\" means the same as that term is defined in § 6.2-899._\n\n_\" Bank\" means the same as that term is defined in § 6.2-800._\n\n_\" Credit union\" means the same as that term is defined in § 6.2-1300._\n\n_\" Federal credit union\" means a credit union duly organized under federal\nlaw._\n\n_\" Mortgage lender\" means the same as that term is defined in § 6.2-1600._\n\n_\" Out-of-state bank\" means the same as that term is defined in § 6.2-836._\n\n_\" Out-of-state credit union\" means a credit union organized and doing\nbusiness in another state._\n\n_\" Savings institution\" means the same as that term is defined in § 6.2-1100._\n\n_\" Subsidiary\" means the same as that term is defined in § 6.2-700._\n\n_The obligations imposed on developers or deployers by this chapter shall be\ndeemed satisfied for any bank, out-of-state bank, credit union, federal credit\nunion, mortgage lender, out-of-state credit union, savings institution, or any\naffiliate, subsidiary, or service provider thereof if such bank, out-of-state\nbank, credit union, federal credit union, mortgage lender, out-of-state credit\nunion, savings institution, or affiliate, subsidiary, or service provider is\nsubject to the jurisdiction of any state or federal regulator under any\npublished guidance or regulations that apply to the use of high-risk\nartificial intelligence systems and such guidance or regulations._\n\n_H. For purposes of this subsection, \"insurer\" means the same as that term is\ndefined in § 38.2-100._\n\n_The provisions of this chapter shall not apply to any insurer, or any high-\nrisk artificial intelligence system developed by or for or deployed by an\ninsurer for use in the business of insurance, if such insurer is regulated and\nsupervised by the State Corporation Commission or a comparable federal\nregulating body and subject to examination by such entity under any existing\nstatutes, rules, or regulations pertaining to unfair trade practices and\nunfair discrimination prohibited under Chapter 5 ( § 38.2-500 et seq.) of\nTitle 38.2, or published guidance or regulations that apply to the use of\nhigh-risk artificial intelligence systems and such guidance or regulations aid\nin the prevention and mitigation of algorithmic discrimination caused by the\nuse of a high-risk artificial intelligence system or any risk of algorithmic\ndiscrimination that is reasonably foreseeable as a result of the use of a\nhigh-risk artificial intelligence system. Nothing in this chapter shall be\nconstrued to delegate existing regulatory oversight of the business of\ninsurance to any department or agency other than the Bureau of Insurance of\nthe Virginia State Corporation Commission._\n\n_I. The provisions of this chapter shall not apply to the development of an\nartificial intelligence system that is used exclusively for research,\ntraining, testing, or other pre-deployment activities performed by active\nparticipants of any sandbox software or sandbox environment established and\nsubject to oversight by a designated agency or other government entity and\nthat is in compliance with the provisions of this chapter._\n\n_J. The provisions of this chapter shall not apply to a developer or deployer,\nor other person who develops, deploys, puts into service, or intentionally\nmodifies, as applicable, a high-risk artificial intelligence system that (i)\nhas been approved, authorized, certified, cleared, developed, or granted by a\nfederal agency acting within the scope of the federal agency 's authority, or\nby a regulated entity subject to the supervision and regulation of the Federal\nHousing Finance Agency or (ii) is in compliance with standards established by\na federal agency or by a regulated entity subject to the supervision and\nregulation of the Federal Housing Finance Agency, if the standards are\nsubstantially equivalent or more stringent than the requirements of this\nchapter._\n\n_K. The provisions of this chapter shall not apply to a developer or deployer,\nor other person that (i) facilitates or engages in the provision of telehealth\nservices, as defined in § 32.1-122.03:1, or (ii) is a covered entity within\nthe meaning of the federal Health Insurance Portability and Accountability Act\nof 1996 (42 U.S.C. § 1320d et seq.) and the regulations promulgated under such\nfederal act, as both may be amended from time to time, and is providing (a)\nhealth care recommendations that (1) are generated by an artificial\nintelligence system and (2) require a health care provider, as defined in §\n8.01-581.1, to take action to implement the recommendations or (b) services\nutilizing an artificial intelligence system for an administrative, quality\nmeasurement, security, or internal cost or performance improvement function._\n\n_L. If a developer or deployer engages in any action authorized by an\nexemption set forth in this section, the developer or deployer bears the\nburden of demonstrating that such action qualifies for such exemption._\n\n_M. If a developer or deployer withholds information pursuant to an exemption\nset forth in this chapter for which disclosure would otherwise be required by\nthis chapter, including the exemption from disclosure of trade secrets, the\ndeveloper or deployer shall notify the subject of disclosure and provide a\nbasis for withholding the information. If a developer or deployer redacts any\ninformation pursuant to an exemption from disclosure, the developer or\ndeployer shall notify the subject of disclosure that the developer or deployer\nis redacting such information and provide the basis for such decision to\nredact._\n\n**_§ 59.1-611. Enforcement; civil penalties._ **\n\n_A. The Attorney General shall have exclusive authority to enforce the\nprovisions of this chapter._\n\n_B. Whenever the Attorney General has reasonable cause to believe that any\nperson has engaged in or is engaging in any violation of this chapter, the\nAttorney General is empowered to issue a civil investigative demand. The\nprovisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative\ndemands issued pursuant to this section. In rendering and furnishing any\ninformation requested pursuant to a civil investigative demand issued pursuant\nto this section, a developer or deployer may redact or omit any trade secrets\nor information protected from disclosure by state or federal law. If a\ndeveloper or deployer refuses to disclose, redacts, or omits information based\non the exemption from disclosure of trade secrets, such developer or deployer\nshall affirmatively state to the Attorney General that the basis for\nnondisclosure, redaction, or omission is because such information is a trade\nsecret. To the extent that any information requested pursuant to a civil\ninvestigative demand issued pursuant to this section is subject to attorney-\nclient privilege or work-product protection, disclosure of such information\npursuant to the civil investigative demand shall not constitute a waiver of\nsuch privilege or protection. Any information, statement, or documentation\nprovided to the Attorney General pursuant to this section shall be exempt from\ndisclosure under the Virginia Freedom of Information Act (§ 2.2-3700 et\nseq.)._\n\n_C. Notwithstanding any contrary provision of law, the Attorney General may\ncause an action to be brought in the appropriate circuit court in the name of\nthe Commonwealth to enjoin any violation of this chapter. The circuit court\nhaving jurisdiction may enjoin such violation notwithstanding the existence of\nan adequate alternative remedy at law._\n\n_D. Any person who violates the provisions of this chapter shall be subject to\na civil penalty in an amount not to exceed $1,000 plus reasonable attorney\nfees, expenses, and costs, as determined by the court. Any person who\nwillfully violates the provisions of this chapter shall be subject to a civil\npenalty in an amount not less than $1,000 and not more than $10,000 plus\nreasonable attorney fees, expenses, and costs, as determined by the court.\nSuch civil penalties shall be paid into the Literary Fund._\n\n_E. Each violation of this chapter shall constitute a separate violation and\nshall be subject to any civil penalties imposed under this section._\n\n_F. The Attorney General may require that a developer disclose to the Attorney\nGeneral any statement or documentation described in this chapter if such\nstatement or documentation is relevant to an investigation conducted by the\nAttorney General. The Attorney General may also require that a deployer\ndisclose to the Attorney General any risk management policy designed and\nimplemented, impact assessment completed, or record maintained pursuant to\nthis chapter if such risk management policy, impact assessment, or record is\nrelevant to an investigation conducted by the Attorney General._\n\n_G. In an action brought by the Attorney General pursuant to this section, it\nshall be an affirmative defense that the developer or deployer (i) discovers a\nviolation of any provision of this chapter through red-teaming or other\nmethod; (ii) no later than 45 days after discovering such violation (a) cures\nsuch violation and (b) provides notice to the Attorney General in a form and\nmanner as prescribed by the Attorney General that such violation has been\ncured and evidence that any harm caused by such violation has been mitigated;\nand (iii) is otherwise in compliance with the requirements of this chapter._\n\n_H. Prior to causing an action against a developer or deployer for a violation\nof this chapter pursuant to subsection C, the Attorney General shall\ndetermine, in consultation with the developer or deployer, if it is possible\nto cure the violation. If it is possible to cure such violation, the Attorney\nGeneral may issue a notice of violation to the developer or deployer and\nafford the developer or deployer the opportunity to cure such violation within\n45 days of the receipt of such notice of violation. In determining whether to\ngrant such opportunity to cure such violation, the Attorney General shall\nconsider (i) the number of violations; (ii) the size and complexity of the\ndeveloper or deployer; (iii) the nature and extent of the developer 's or\ndeployer's business; (iv) the substantial likelihood of injury to the public;\n(v) the safety of persons or property; and (vi) whether such violation was\nlikely caused by human or technical error. If the developer or deployer fails\nto cure such violation within 45 days of the receipt of such notice of\nviolation, the Attorney General may proceed with such action._\n\n_I. Nothing in this chapter shall create a private cause of action in favor of\nany person aggrieved by a violation of this chapter._\n\n**_§ 59.1-612. Construction of chapter._ **\n\n_A. This chapter is declared to be remedial, with the purposes of protecting\nconsumers and ensuring consumers receive information about consequential\ndecisions affecting them. The provisions of this chapter granting rights or\nprotections to consumers shall be construed broadly and exemptions construed\nnarrowly._\n\n_B. If any provision of this chapter or its application to any person or\ncircumstance is held invalid, the invalidity shall not affect other provisions\nor applications of this chapter that can be given effect without the invalid\nprovision or application, and to this end all the provisions of this chapter\nare hereby expressly declared to be severable._\n\n**2\\. That the provisions of this act shall become effective on July 1,\n2026.**\n\n**3\\. That compliance with the provisions of Chapter 58 ( § 59.1-607 et seq.)\nof Title 59.1 of the Code of Virginia, as created by this act, shall not (i)\nrelieve a person from liability for any causes of action that existed at\ncommon law or by statute prior to July 1, 2026, or (ii) be construed to modify\nor otherwise affect, preempt, limit, or displace any causes of action that\nexisted at common law or by statute prior to July 1, 2026.**\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "03/07/2025",
      "label": "Enrolled",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:VA2024000H2094&verid=VA2024000H2094_20250307_0_ENR&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2024 VA H 2094</td> <td><table><tr><td class=\"label\">Author:</td> <td>Maldonado</td></tr> <tr><td class=\"label\">Version:</td> <td>Enrolled</td></tr> <tr><td class=\"label\">Version Date:</td> <td>03/07/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">VIRGINIA ACTS OF ASSEMBLY -- CHAPTER</p>\n  </div>\n  <a name=\"code_document_section\"></a><div class=\"code\">\n   <p class=\"indent\">\n    <i>An Act to amend the Code of Virginia by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, relating to high-risk artificial intelligence; development, deployment, and use; civil penalties.</i>\n   </p>\n   <p class=\"center\">[H 2094]</p>\n   <p class=\"center\">Approved</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">\n     <b>Be it enacted by the General Assembly of Virginia:</b>\n    </p>\n   </span>\n   <p class=\"indent\">\n    <b>1. That the Code of Virginia is amended by adding in Title 59.1 a chapter numbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as follows:</b>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">CHAPTER 58.</u>\n   </p>\n   <p class=\"center\">\n    <u class=\"amendmentInsertedText\">HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-607. Definitions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">As used in this chapter, unless the context requires a different meaning:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Algorithmic discrimination&quot; means the use of an artificial intelligence system that results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, sexual orientation, veteran status, or other classification protected under state or federal law. &quot;Algorithmic discrimination&quot; does not include (i) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; (ii) the expansion of an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or (iii) an act or omission by or on behalf of a private club or other establishment not in fact open to the public, as set forth in Title II of the Civil Rights Act of 1964, 42 U.S.C. &sect; 2000a(e), as amended from time to time.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Artificial intelligence system&quot; means any machine learning-based system that, for any explicit or implicit objective, infers from the inputs such system receives how to generate outputs, including content, decisions, predictions, and recommendations, that can influence physical or virtual environments. &quot;Artificial intelligence system&quot; does not include any artificial intelligence system or general purpose artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence system or general purpose artificial intelligence model is made available to deployers or consumers.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consequential decision&quot; means any decision that has a material legal, or similarly significant, effect on the provision or denial to any consumer of (i) parole, probation, a pardon, or any other release from incarceration or court supervision; (ii) education enrollment or an education opportunity; (iii) access to employment; (iv) a financial or lending service; (v) access to health care services; (vi) housing; (vii) insurance; (viii) marital status; or (ix) a legal service.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Consumer&quot; means a natural person who is a resident of the Commonwealth and is acting only in an individual or household context. &quot;Consumer&quot; does not include a natural person acting in a commercial or employment context.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Deployer&quot; means any person doing business in the Commonwealth that deploys or uses a high-risk artificial intelligence system to make a consequential decision in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Developer&quot; means any person doing business in the Commonwealth that develops or intentionally and substantially modifies a high-risk artificial intelligence system that is offered, sold, leased, given, or otherwise made available to deployers or consumers in the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Facial recognition&quot; means the use of a computer system that, for the purpose of attempting to determine the identity of an unknown individual, uses an algorithm to compare the facial biometric data of an unknown individual derived from a photograph, video, or image to a database of photographs or images and associated facial biometric data in order to identify potential matches to an individual. &quot;Facial recognition&quot; does not include facial verification technology, which involves the process of comparing an image or facial biometric data of a known individual, where such information is provided by that individual, to an image database, or to government documentation containing an image of the known individual, to identify a potential match in pursuit of the individual&#39;s identity.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;General-purpose artificial intelligence model&quot; means a model used by an artificial intelligence system or other system that (i) displays significant generality, (ii) is capable of competently performing a wide range of distinct tasks, and (iii) can be integrated into a variety of downstream applications or systems. &quot;General-purpose artificial intelligence model&quot; does not include any artificial intelligence model that is used for development, prototyping, and research activities before such artificial intelligence model is made available to deployers or consumers.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence&quot; means an artificial intelligence system that is capable of producing and used to produce synthetic content, including audio, images, text, and videos.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Generative artificial intelligence system&quot; means any artificial intelligence system or service that incorporates generative artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that is specifically intended to autonomously make, or be a substantial factor in making, a consequential decision. A system or service is not a &quot;high-risk artificial intelligence system&quot; if it is intended to (i) perform a narrow procedural task, (ii) improve the result of a previously completed human activity, (iii) detect any decision-making patterns or any deviations from pre-existing decision-making patterns, or (iv) perform a preparatory task to an assessment relevant to a consequential decision. &quot;High-risk artificial intelligence system&quot; does not include any of the following technologies:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. Anti-fraud technology that does not use facial recognition technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Anti-malware technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Anti-virus technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Artificial intelligence-enabled video games;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. Autonomous vehicle technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. Calculators;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. Cybersecurity technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. Databases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. Data storage;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. Firewall technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. Internet domain registration;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. Internet website loading;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. Networking;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. Spam and robocall filtering;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. Spell-checking technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">16. Spreadsheets;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">17. Web caching;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">18. Web hosting or any similar technology; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">19. Technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an acceptable use policy that prohibits generating content that is discriminatory or unlawful.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Intentional and substantial modification&quot; means any deliberate change made to (i) an artificial intelligence system that results, at the time when the change is implemented and any time thereafter, in any new material risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects compliance of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. &quot;Intentional and substantial modification&quot; does not include (a) any customization made by deployers that (1) is based on legitimate nondiscriminatory business justifications, (2) is within the scope and purpose of the artificial intelligence tool, and (3) that does not result in a material change to the risks of algorithmic discrimination or (b) any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (1) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (2) such change (A) is made to such high-risk artificial intelligence system as a result of any learning described in clause (b) (1) and (B) was predetermined by the deployer or the third party contracted by the deployer and included within the initial impact assessment of such high-risk artificial intelligence system as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Machine learning&quot; means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Person&quot; includes any individual, corporation, partnership, association, cooperative, limited liability company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. &quot;Person&quot; does not include any government or political subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Principal basis&quot; means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Red-teaming&quot; means adversarial testing to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Substantial factor&quot; means a factor that (i) uses the principal basis for making a consequential decision, (ii) is capable of altering the outcome of a consequential decision, and (iii) is generated by an artificial intelligence system. &quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as the principal basis to make a consequential decision concerning the consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Synthetic content&quot; means information, such as images, video, audio clips, and, to the extent practicable, text, that has been significantly modified or generated by algorithms, including by artificial intelligence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique, or process, that (i) derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by, other persons who can obtain economic value from its disclosure or use and (ii) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-608. Operating standards for developers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each developer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination arising from the intended and contracted uses. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a developer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the developer complied with the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No developer of a high-risk artificial intelligence system shall offer, sell, lease, give, or otherwise provide to a deployer or other developer a high-risk artificial intelligence system unless the developer makes available to the deployer or other developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement disclosing the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Documentation disclosing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">a. The known or reasonably known limitations of such high-risk artificial intelligence system, including any and all known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">b. The purpose of such high-risk artificial intelligence system and the intended benefits and uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">c. A summary describing how such high-risk artificial intelligence system was evaluated for performance before such high-risk artificial intelligence system was licensed, sold, leased, given, or otherwise made available to a deployer or other developer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">d. The measures the developer has taken to mitigate reasonable foreseeable risks of algorithmic discrimination that the developer knows arises from deployment or use of such high-risk artificial intelligence system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">e. How an individual can use such high-risk artificial intelligence system and monitor the performance of such high-risk artificial intelligence system for any risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Documentation including (i) a description of how the high-risk artificial intelligence system was evaluated for performance and for mitigation of algorithmic discrimination before such system was made available to the deployer or other developer; (ii) a description of the intended outputs of the high-risk artificial intelligence system; (iii) a description of the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and (iv) a description of how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when such system is used to make, or is a substantial factor in making, a consequential decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any additional documentation that is reasonably necessary to assist the deployer or other developer in understanding the outputs and monitoring performance of the high-risk artificial intelligence system for risks of algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Each developer that offers, sells, leases, gives, or otherwise makes available to a deployer or other developer a high-risk artificial intelligence system shall make available to the deployer or other developer to the extent feasible and necessary, information and documentation through artifacts such as system cards or predeployment impact assessments, including any risk management policy designed and implemented and any relevant impact assessment completed, and such documentation and information shall enable the deployer, other developer, or a third party contracted by the deployer to complete an impact assessment as required in &sect; 59.1-609.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. A developer that also serves as a deployer for any high-risk artificial intelligence system shall not be required to generate the documentation required by this section unless such high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer or as otherwise required by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this section shall be construed to require a developer to disclose any trade secret, information that could create a security risk, or other confidential or proprietary information protected under state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For any disclosure required pursuant to this section, each developer shall, no later than 90 days after the developer performs an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. 1. Each developer of a high-risk generative artificial intelligence system that generates or substantially modifies synthetic content shall ensure that the outputs of such high-risk artificial intelligence system (i) are identifiable and detectable in a manner that is accessible by consumers using industry-standard tools or tools provided by the developer; (ii) comply with any applicable accessibility requirements, as synthetic content, to the extent reasonably feasible; and (iii) apply such identification at the time the output is generated;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. If such synthetic content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for identifying outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The identification of outputs required by subdivision 1 shall not apply to (i) synthetic content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Where multiple developers directly contribute to the development of a high-risk artificial intelligence system, each developer shall be subject to the obligations and operating standards applicable to developers pursuant to this section solely with respect to its activities contributing to the development of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-609. Operating standards for deployers of high-risk artificial intelligence systems.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Each deployer of a high-risk artificial intelligence system shall use a reasonable duty of care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the Attorney General pursuant to &sect; 59.1-611, there shall be a rebuttable presumption that a deployer of a high-risk artificial intelligence system used a reasonable duty of care as required by this subsection if the deployer complied with the provisions of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. No deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has designed and implemented a risk management policy and program for such high-risk artificial intelligence system. The risk management policy shall specify the principles, processes, and personnel that the deployer shall use in maintaining the risk management program to identify, mitigate, and document any risk of algorithmic discrimination that is a reasonably foreseeable consequence of deploying or using such high-risk artificial intelligence system to make a consequential decision. Each risk management policy and program designed, implemented, and maintained pursuant to this subsection shall be reasonable considering the guidance and standards set forth in the latest version of:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. The Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Standard ISO/IEC 42001 of the International Organization for Standardization;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A nationally or internationally recognized risk management framework for artificial intelligence systems with requirements that are substantially equivalent to, and at least as stringent as, the requirements set forth in this section; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Any risk management framework for artificial intelligence systems that the Attorney General may designate and is substantially equivalent to, and at least as stringent as, the guidance and standards described in subdivision 1.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Except as provided in this subsection, no deployer shall deploy or use a high-risk artificial intelligence system to make a consequential decision unless the deployer has completed an impact assessment for such high-risk artificial intelligence system. The deployer shall complete an impact assessment for a high-risk artificial intelligence system (i) before the deployer initially deploys such high-risk artificial intelligence system and (ii) before a significant update to such high-risk artificial intelligence system is used to make a consequential decision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Each impact assessment completed pursuant to this subsection shall include, at a minimum:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A statement by the deployer disclosing (i) the purpose, intended use cases and deployment context of, and benefits afforded by the high-risk artificial intelligence system and (ii) whether the deployment or use of the high-risk artificial intelligence system poses any known or reasonably foreseeable risk of algorithmic discrimination and, if so, (a) the nature of such algorithmic discrimination and (b) the steps that have been taken, to the extent feasible, to mitigate such risk;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. For each post-deployment impact assessment completed pursuant to this subsection, whether the intended use cases of the high-risk artificial intelligence system as updated were consistent with, or varied from, the developer&#39;s intended uses of such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A description of (i) the categories of data the high-risk artificial intelligence system processes as inputs and (ii) the outputs such high-risk artificial intelligence system produces;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. If the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize such high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. A list of any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. A description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that such high-risk artificial intelligence system is in use when such high-risk artificial intelligence system is in use;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. A description of any post-deployment monitoring performed and user safeguards provided concerning such high-risk artificial intelligence system, including any oversight process established by the deployer to address issues arising from deployment or use of such high-risk artificial intelligence system as such issues arise; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. An analysis of such high-risk artificial intelligence system&#39;s validity and reliability in accordance with standard industry practices and a description of any metrics used to evaluate the performance and known limitations of such high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed or used by a deployer. High-risk artificial intelligence systems that are in conformity with the latest version of the Artificial Intelligence Risk Management Framework published by the National Institute of Standards and Technology, Standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, or parts thereof, shall be presumed to be in conformity with related requirements set out in this section and in associated regulations. If a deployer completes an impact assessment for the purpose of complying with another applicable law or regulation, such impact assessment shall be deemed to satisfy the requirements established in this subsection if such impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection. A deployer that completes an impact assessment pursuant to this subsection shall maintain such impact assessment and all records concerning such impact assessment for three years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Throughout the period of time that a high-risk artificial intelligence system is deployed and for a period of at least three years following the final deployment of such high-risk artificial intelligence system, the deployer shall retain all records concerning each impact assessment conducted on the high-risk artificial intelligence system, including all raw data used to evaluate the performance and known limitations of such system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Not later than the time that a deployer uses a high-risk artificial intelligence system to interact with a consumer, the deployer shall disclose to the consumer that the deployer is interacting with an artificial intelligence system disclosing (i) the purpose of such high-risk artificial intelligence system, (ii) the nature of such system, (iii) the nature of the consequential decision, (iv) the contact information for the deployer, and (v) a description of the artificial intelligence system in plain language of such system, which shall include (a) a description of the personal characteristics or attributes that such system will measure or assess, (b) the method by which the system measures or assesses such attributes or characteristics, (c) how such attributes or characteristics are relevant to the consequential decisions for which the system should be used, (d) any human components of such system, and (e) how any automated components of such system are used to inform such consequential decisions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A deployer that has deployed a high-risk artificial intelligence system to make a consequential decision concerning a consumer shall transmit to the consumer the consequential decision without undue delay. If such consequential decision is adverse to such consumer and based on personal data beyond information that the consumer provided directly to the deployer, the deployer shall provide to the consumer (a) a statement disclosing the principal reason or reasons for the consequential decision, including (1) the degree to which and manner in which the high-risk artificial intelligence system contributed to the consequential decision, (2) the type of data that was processed by such system in making the consequential decision, and (3) the sources of such data; (b) pursuant to the provisions of the Consumer Data Protection Act (&sect; 59.1-575 et seq.), an opportunity to correct any inaccuracies in the consumer&#39;s personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and (c) an opportunity to appeal such adverse consequential decision concerning the consumer arising from the deployment of such system. Any such appeal shall allow for human review, if technically reasonable and practicable, unless providing the opportunity for appeal is not in the best interest of the consumer, including instances in which any delay might pose a risk to the life or safety of such consumer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each deployer shall make available, in a manner that is clear and readily available, a statement summarizing how such deployer manages any reasonably foreseeable risk of algorithmic discrimination that may arise from the use or deployment of the high-risk artificial intelligence system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. For any disclosure required pursuant to this section, each deployer shall, no later than 30 days after the deployer is notified by the developer that the developer has performed an intentional and substantial modification to any high-risk artificial intelligence system, update such disclosure as necessary to ensure that such disclosure remains accurate.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. Any deployer who performs an intentional and substantial modification to any high-risk artificial intelligence system shall comply with the documentation and disclosure requirements for developers pursuant to subsections B through G of &sect; 59.1-608.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Nothing in this section shall be construed to require a deployer to disclose any trade secret, information that could create a security risk, or other confidential or proprietary information protected under state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-610. Exemptions.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. Nothing in this chapter shall be construed to restrict a developer&#39;s or deployer&#39;s ability to (i) comply with federal, state, or municipal ordinances or regulations; (ii) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by federal, state, local, or other governmental authorities; (iii) cooperate with law-enforcement agencies concerning conduct or activity that the developer or deployer reasonably and in good faith believes may violate federal, state, or local law, ordinances, or regulations; (iv) investigate, establish, exercise, prepare for, or defend legal claims; (v) provide a product or service specifically requested by a consumer; (vi) perform under a contract to which a consumer is a party, including fulfilling the terms of a written warranty; (vii) take steps at the request of a consumer prior to entering into a contract; (viii) take immediate steps to protect an interest that is essential for the life or physical safety of the consumer or another individual; (ix) prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, or malicious or deceptive activities; (x) take actions to prevent, detect, protect against, report, or respond to the production, generation, incorporation, or synthesization of child sex abuse material, or any illegal activity, preserve the integrity or security of systems, or investigate, report, or prosecute those responsible for any such action; (xi) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is approved, monitored, and governed by an institutional review board that determines, or similar independent oversight entities that determine, (a) that the expected benefits of the research outweigh the risks associated with such research and (b) whether the developer or deployer has implemented reasonable safeguards to mitigate the risks associated with such research; (xii) assist another developer or deployer with any of the obligations imposed by this chapter; or (xiii) take any action that is in the public interest in the areas of public health, community health, or population health, but solely to the extent that such action is subject to suitable and specific measures to safeguard the public.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. The obligations imposed on developers or deployers by this chapter shall not restrict a developer&#39;s or deployer&#39;s ability to (i) conduct internal research to develop, improve, or repair products, services, or technologies; (ii) effectuate a product recall; (iii) identify and repair technical errors that impair existing or intended functionality; or (iv) perform internal operations that are reasonably aligned with the expectations of the consumer or reasonably anticipated based on the consumer&#39;s existing relationship with the developer or deployer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer to disclose trade secrets or information protected from disclosure by state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. The obligations imposed on developers or deployers by this chapter shall not apply where compliance by the developer or deployer with such obligations would violate an evidentiary privilege under federal law or the laws of the Commonwealth.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Nothing in this chapter shall be construed to impose any obligation on a developer or deployer that adversely affects the legally protected rights or freedoms of any person, including the rights of any person to freedom of speech or freedom of the press guaranteed in the First Amendment to the Constitution of the United States or under the Virginia Human Rights Act (&sect; 2.2-3900 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The obligations imposed on developers or deployers by this chapter shall not apply to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the U.S. Department of Commerce, the U.S. Department of Defense, and the National Aeronautics and Space Administration, unless such artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. For the purposes of this subsection:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Affiliate&quot; means the same as that term is defined in &sect; 6.2-899.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Bank&quot; means the same as that term is defined in &sect; 6.2-800.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Credit union&quot; means the same as that term is defined in &sect; 6.2-1300.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Federal credit union&quot; means a credit union duly organized under federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Mortgage lender&quot; means the same as that term is defined in &sect; 6.2-1600.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state bank&quot; means the same as that term is defined in &sect; 6.2-836.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Out-of-state credit union&quot; means a credit union organized and doing business in another state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Savings institution&quot; means the same as that term is defined in &sect; 6.2-1100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">&quot;Subsidiary&quot; means the same as that term is defined in &sect; 6.2-700.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The obligations imposed on developers or deployers by this chapter shall be deemed satisfied for any bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or any affiliate, subsidiary, or service provider thereof if such bank, out-of-state bank, credit union, federal credit union, mortgage lender, out-of-state credit union, savings institution, or affiliate, subsidiary, or service provider is subject to the jurisdiction of any state or federal regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. For purposes of this subsection, &quot;insurer&quot; means the same as that term is defined in &sect; 38.2-100.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">The provisions of this chapter shall not apply to any insurer, or any high-risk artificial intelligence system developed by or for or deployed by an insurer for use in the business of insurance, if such insurer is regulated and supervised by the State Corporation Commission or a comparable federal regulating body and subject to examination by such entity under any existing statutes, rules, or regulations pertaining to unfair trade practices and unfair discrimination prohibited under Chapter 5 (&sect; 38.2-500 et seq.) of Title 38.2, or published guidance or regulations that apply to the use of high-risk artificial intelligence systems and such guidance or regulations aid in the prevention and mitigation of algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. Nothing in this chapter shall be construed to delegate existing regulatory oversight of the business of insurance to any department or agency other than the Bureau of Insurance of the Virginia State Corporation Commission.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. The provisions of this chapter shall not apply to the development of an artificial intelligence system that is used exclusively for research, training, testing, or other pre-deployment activities performed by active participants of any sandbox software or sandbox environment established and subject to oversight by a designated agency or other government entity and that is in compliance with the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">J. The provisions of this chapter shall not apply to a developer or deployer, or other person who develops, deploys, puts into service, or intentionally modifies, as applicable, a high-risk artificial intelligence system that (i) has been approved, authorized, certified, cleared, developed, or granted by a federal agency acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency or (ii) is in compliance with standards established by a federal agency or by a regulated entity subject to the supervision and regulation of the Federal Housing Finance Agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">K. The provisions of this chapter shall not apply to a developer or deployer, or other person that (i) facilitates or engages in the provision of telehealth services, as defined in &sect; 32.1-122.03:1, or (ii) is a covered entity within the meaning of the federal Health Insurance Portability and Accountability Act of 1996 (42 U.S.C. &sect; 1320d et seq.) and the regulations promulgated under such federal act, as both may be amended from time to time, and is providing (a) health care recommendations that (1) are generated by an artificial intelligence system and (2) require a health care provider, as defined in &sect; 8.01-581.1, to take action to implement the recommendations or (b) services utilizing an artificial intelligence system for an administrative, quality measurement, security, or internal cost or performance improvement function.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">L. If a developer or deployer engages in any action authorized by an exemption set forth in this section, the developer or deployer bears the burden of demonstrating that such action qualifies for such exemption.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">M. If a developer or deployer withholds information pursuant to an exemption set forth in this chapter for which disclosure would otherwise be required by this chapter, including the exemption from disclosure of trade secrets, the developer or deployer shall notify the subject of disclosure and provide a basis for withholding the information. If a developer or deployer redacts any information pursuant to an exemption from disclosure, the developer or deployer shall notify the subject of disclosure that the developer or deployer is redacting such information and provide the basis for such decision to redact.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-611. Enforcement; civil penalties.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. The Attorney General shall have exclusive authority to enforce the provisions of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any violation of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of &sect; 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. If a developer or deployer refuses to disclose, redacts, or omits information based on the exemption from disclosure of trade secrets, such developer or deployer shall affirmatively state to the Attorney General that the basis for nondisclosure, redaction, or omission is because such information is a trade secret. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand shall not constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be exempt from disclosure under the Virginia Freedom of Information Act (&sect; 2.2-3700 et seq.).</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">C. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any violation of this chapter. The circuit court having jurisdiction may enjoin such violation notwithstanding the existence of an adequate alternative remedy at law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">D. Any person who violates the provisions of this chapter shall be subject to a civil penalty in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil penalty in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil penalties shall be paid into the Literary Fund.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">E. Each violation of this chapter shall constitute a separate violation and shall be subject to any civil penalties imposed under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">F. The Attorney General may require that a developer disclose to the Attorney General any statement or documentation described in this chapter if such statement or documentation is relevant to an investigation conducted by the Attorney General. The Attorney General may also require that a deployer disclose to the Attorney General any risk management policy designed and implemented, impact assessment completed, or record maintained pursuant to this chapter if such risk management policy, impact assessment, or record is relevant to an investigation conducted by the Attorney General.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">G. In an action brought by the Attorney General pursuant to this section, it shall be an affirmative defense that the developer or deployer (i) discovers a violation of any provision of this chapter through red-teaming or other method; (ii) no later than 45 days after discovering such violation (a) cures such violation and (b) provides notice to the Attorney General in a form and manner as prescribed by the Attorney General that such violation has been cured and evidence that any harm caused by such violation has been mitigated; and (iii) is otherwise in compliance with the requirements of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">H. Prior to causing an action against a developer or deployer for a violation of this chapter pursuant to subsection C, the Attorney General shall determine, in consultation with the developer or deployer, if it is possible to cure the violation. If it is possible to cure such violation, the Attorney General may issue a notice of violation to the developer or deployer and afford the developer or deployer the opportunity to cure such violation within 45 days of the receipt of such notice of violation. In determining whether to grant such opportunity to cure such violation, the Attorney General shall consider (i) the number of violations; (ii) the size and complexity of the developer or deployer; (iii) the nature and extent of the developer&#39;s or deployer&#39;s business; (iv) the substantial likelihood of injury to the public; (v) the safety of persons or property; and (vi) whether such violation was likely caused by human or technical error. If the developer or deployer fails to cure such violation within 45 days of the receipt of such notice of violation, the Attorney General may proceed with such action.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">I. Nothing in this chapter shall create a private cause of action in favor of any person aggrieved by a violation of this chapter.</u>\n   </p>\n   <p class=\"indent\">\n    <b>\n     <u class=\"amendmentInsertedText\">&sect; 59.1-612. Construction of chapter.</u>\n    </b>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">A. This chapter is declared to be remedial, with the purposes of protecting consumers and ensuring consumers receive information about consequential decisions affecting them. The provisions of this chapter granting rights or protections to consumers shall be construed broadly and exemptions construed narrowly.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">B. If any provision of this chapter or its application to any person or circumstance is held invalid, the invalidity shall not affect other provisions or applications of this chapter that can be given effect without the invalid provision or application, and to this end all the provisions of this chapter are hereby expressly declared to be severable.</u>\n   </p>\n   <effective_clause>\n    <p class=\"indent\">\n     <b>2. That the provisions of this act shall become effective on July 1, 2026.</b>\n    </p>\n   </effective_clause>\n   <p class=\"indent\">\n    <b>3. That compliance with the provisions of Chapter 58 (&sect; 59.1-607 et seq.) of Title 59.1 of the Code of Virginia, as created by this act, shall not (i) relieve a person from liability for any causes of action that existed at common law or by statute prior to July 1, 2026, or (ii) be construed to modify or otherwise affect, preempt, limit, or displace any causes of action that existed at common law or by statute prior to July 1, 2026.</b>\n   </p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2024 VA H 2094 | | Author: | Maldonado  \n---|---  \nVersion: | Enrolled  \nVersion Date: | 03/07/2025  \n  \nVIRGINIA ACTS OF ASSEMBLY -- CHAPTER\n\n_An Act to amend the Code of Virginia by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612,\nrelating to high-risk artificial intelligence; development, deployment, and\nuse; civil penalties._\n\n[H 2094]\n\nApproved\n\n**Be it enacted by the General Assembly of Virginia:**\n\n**1\\. That the Code of Virginia is amended by adding in Title 59.1 a chapter\nnumbered 58, consisting of sections numbered 59.1-607 through 59.1-612, as\nfollows:**\n\n_CHAPTER 58._\n\n_HIGH-RISK ARTIFICIAL INTELLIGENCE DEVELOPER AND DEPLOYER ACT._\n\n**_§ 59.1-607. Definitions._ **\n\n_As used in this chapter, unless the context requires a different meaning:_\n\n_\" Algorithmic discrimination\" means the use of an artificial intelligence\nsystem that results in an unlawful differential treatment or impact that\ndisfavors an individual or group of individuals on the basis of their actual\nor perceived age, color, disability, ethnicity, genetic information, limited\nproficiency in the English language, national origin, race, religion,\nreproductive health, sex, sexual orientation, veteran status, or other\nclassification protected under state or federal law. \"Algorithmic\ndiscrimination\" does not include (i) the offer, license, or use of a high-risk\nartificial intelligence system by a developer or deployer for the sole purpose\nof the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; (ii) the expansion of an applicant, customer, or participant pool to\nincrease diversity or redress historical discrimination; or (iii) an act or\nomission by or on behalf of a private club or other establishment not in fact\nopen to the public, as set forth in Title II of the Civil Rights Act of 1964,\n42 U.S.C. § 2000a(e), as amended from time to time._\n\n_\" Artificial intelligence system\" means any machine learning-based system\nthat, for any explicit or implicit objective, infers from the inputs such\nsystem receives how to generate outputs, including content, decisions,\npredictions, and recommendations, that can influence physical or virtual\nenvironments. \"Artificial intelligence system\" does not include any artificial\nintelligence system or general purpose artificial intelligence model that is\nused for development, prototyping, and research activities before such\nartificial intelligence system or general purpose artificial intelligence\nmodel is made available to deployers or consumers._\n\n_\" Consequential decision\" means any decision that has a material legal, or\nsimilarly significant, effect on the provision or denial to any consumer of\n(i) parole, probation, a pardon, or any other release from incarceration or\ncourt supervision; (ii) education enrollment or an education opportunity;\n(iii) access to employment; (iv) a financial or lending service; (v) access to\nhealth care services; (vi) housing; (vii) insurance; (viii) marital status; or\n(ix) a legal service._\n\n_\" Consumer\" means a natural person who is a resident of the Commonwealth and\nis acting only in an individual or household context. \"Consumer\" does not\ninclude a natural person acting in a commercial or employment context._\n\n_\" Deployer\" means any person doing business in the Commonwealth that deploys\nor uses a high-risk artificial intelligence system to make a consequential\ndecision in the Commonwealth._\n\n_\" Developer\" means any person doing business in the Commonwealth that\ndevelops or intentionally and substantially modifies a high-risk artificial\nintelligence system that is offered, sold, leased, given, or otherwise made\navailable to deployers or consumers in the Commonwealth._\n\n_\" Facial recognition\" means the use of a computer system that, for the\npurpose of attempting to determine the identity of an unknown individual, uses\nan algorithm to compare the facial biometric data of an unknown individual\nderived from a photograph, video, or image to a database of photographs or\nimages and associated facial biometric data in order to identify potential\nmatches to an individual. \"Facial recognition\" does not include facial\nverification technology, which involves the process of comparing an image or\nfacial biometric data of a known individual, where such information is\nprovided by that individual, to an image database, or to government\ndocumentation containing an image of the known individual, to identify a\npotential match in pursuit of the individual's identity._\n\n_\" General-purpose artificial intelligence model\" means a model used by an\nartificial intelligence system or other system that (i) displays significant\ngenerality, (ii) is capable of competently performing a wide range of distinct\ntasks, and (iii) can be integrated into a variety of downstream applications\nor systems. \"General-purpose artificial intelligence model\" does not include\nany artificial intelligence model that is used for development, prototyping,\nand research activities before such artificial intelligence model is made\navailable to deployers or consumers._\n\n_\" Generative artificial intelligence\" means an artificial intelligence system\nthat is capable of producing and used to produce synthetic content, including\naudio, images, text, and videos._\n\n_\" Generative artificial intelligence system\" means any artificial\nintelligence system or service that incorporates generative artificial\nintelligence._\n\n_\" High-risk artificial intelligence system\" means any artificial intelligence\nsystem that is specifically intended to autonomously make, or be a substantial\nfactor in making, a consequential decision. A system or service is not a\n\"high-risk artificial intelligence system\" if it is intended to (i) perform a\nnarrow procedural task, (ii) improve the result of a previously completed\nhuman activity, (iii) detect any decision-making patterns or any deviations\nfrom pre-existing decision-making patterns, or (iv) perform a preparatory task\nto an assessment relevant to a consequential decision. \"High-risk artificial\nintelligence system\" does not include any of the following technologies:_\n\n_1\\. Anti-fraud technology that does not use facial recognition technology;_\n\n_2\\. Anti-malware technology;_\n\n_3\\. Anti-virus technology;_\n\n_4\\. Artificial intelligence-enabled video games;_\n\n_5\\. Autonomous vehicle technology;_\n\n_6\\. Calculators;_\n\n_7\\. Cybersecurity technology;_\n\n_8\\. Databases;_\n\n_9\\. Data storage;_\n\n_10\\. Firewall technology;_\n\n_11\\. Internet domain registration;_\n\n_12\\. Internet website loading;_\n\n_13\\. Networking;_\n\n_14\\. Spam and robocall filtering;_\n\n_15\\. Spell-checking technology;_\n\n_16\\. Spreadsheets;_\n\n_17\\. Web caching;_\n\n_18\\. Web hosting or any similar technology; or_\n\n_19\\. Technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an acceptable use\npolicy that prohibits generating content that is discriminatory or unlawful._\n\n_\" Intentional and substantial modification\" means any deliberate change made\nto (i) an artificial intelligence system that results, at the time when the\nchange is implemented and any time thereafter, in any new material risk of\nalgorithmic discrimination or (ii) a general-purpose artificial intelligence\nmodel that affects compliance of the general-purpose artificial intelligence\nmodel, materially changes the purpose of the general-purpose artificial\nintelligence model, or results in any new reasonably foreseeable risk of\nalgorithmic discrimination. \"Intentional and substantial modification\" does\nnot include (a) any customization made by deployers that (1) is based on\nlegitimate nondiscriminatory business justifications, (2) is within the scope\nand purpose of the artificial intelligence tool, and (3) that does not result\nin a material change to the risks of algorithmic discrimination or (b) any\nchange made to a high-risk artificial intelligence system, or the performance\nof a high-risk artificial intelligence system, if (1) the high-risk artificial\nintelligence system continues to learn after such high-risk artificial\nintelligence system is offered, sold, leased, licensed, given, or otherwise\nmade available to a deployer, or deployed, and (2) such change (A) is made to\nsuch high-risk artificial intelligence system as a result of any learning\ndescribed in clause (b) (1) and (B) was predetermined by the deployer or the\nthird party contracted by the deployer and included within the initial impact\nassessment of such high-risk artificial intelligence system as required in §\n59.1-609._\n\n_\" Machine learning\" means the development of algorithms to build data-derived\nstatistical models that are capable of drawing inferences from previously\nunseen data without explicit human instruction._\n\n_\" Person\" includes any individual, corporation, partnership, association,\ncooperative, limited liability company, trust, joint venture, or any other\nlegal or commercial entity and any successor, representative, agent, agency,\nor instrumentality thereof. \"Person\" does not include any government or\npolitical subdivision._\n\n_\" Principal basis\" means the use of an output of a high-risk artificial\nintelligence system to make a decision without (i) human review, oversight,\ninvolvement, or intervention or (ii) meaningful consideration by a human._\n\n_\" Red-teaming\" means adversarial testing to identify the potential adverse\nbehaviors or outcomes of an artificial intelligence system, identify how such\nbehaviors or outcomes occur, and stress test the safeguards against such\nbehaviors or outcomes._\n\n_\" Substantial factor\" means a factor that (i) uses the principal basis for\nmaking a consequential decision, (ii) is capable of altering the outcome of a\nconsequential decision, and (iii) is generated by an artificial intelligence\nsystem. \"Substantial factor\" includes any use of an artificial intelligence\nsystem to generate any content, decision, prediction, or recommendation\nconcerning a consumer that is used as the principal basis to make a\nconsequential decision concerning the consumer._\n\n_\" Synthetic content\" means information, such as images, video, audio clips,\nand, to the extent practicable, text, that has been significantly modified or\ngenerated by algorithms, including by artificial intelligence._\n\n_\" Trade secret\" means information, including a formula, pattern, compilation,\nprogram, device, method, technique, or process, that (i) derives independent\neconomic value, actual or potential, from not being generally known to, and\nnot being readily ascertainable by proper means by, other persons who can\nobtain economic value from its disclosure or use and (ii) is the subject of\nefforts that are reasonable under the circumstances to maintain its secrecy._\n\n**_§ 59.1-608. Operating standards for developers of high-risk artificial\nintelligence systems._ **\n\n_A. Each developer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination arising from the intended and\ncontracted uses. In any enforcement action brought by the Attorney General\npursuant to § 59.1-611, there shall be a rebuttable presumption that a\ndeveloper of a high-risk artificial intelligence system used a reasonable duty\nof care as required by this subsection if the developer complied with the\nrequirements of this section._\n\n_B. No developer of a high-risk artificial intelligence system shall offer,\nsell, lease, give, or otherwise provide to a deployer or other developer a\nhigh-risk artificial intelligence system unless the developer makes available\nto the deployer or other developer:_\n\n_1\\. A statement disclosing the intended uses of such high-risk artificial\nintelligence system;_\n\n_2\\. Documentation disclosing the following:_\n\n_a. The known or reasonably known limitations of such high-risk artificial\nintelligence system, including any and all known or reasonably foreseeable\nrisks of algorithmic discrimination arising from the intended uses of such\nhigh-risk artificial intelligence system;_\n\n_b. The purpose of such high-risk artificial intelligence system and the\nintended benefits and uses of such high-risk artificial intelligence system;_\n\n_c. A summary describing how such high-risk artificial intelligence system was\nevaluated for performance before such high-risk artificial intelligence system\nwas licensed, sold, leased, given, or otherwise made available to a deployer\nor other developer;_\n\n_d. The measures the developer has taken to mitigate reasonable foreseeable\nrisks of algorithmic discrimination that the developer knows arises from\ndeployment or use of such high-risk artificial intelligence system; and_\n\n_e. How an individual can use such high-risk artificial intelligence system\nand monitor the performance of such high-risk artificial intelligence system\nfor any risk of algorithmic discrimination;_\n\n_3\\. Documentation including (i) a description of how the high-risk artificial\nintelligence system was evaluated for performance and for mitigation of\nalgorithmic discrimination before such system was made available to the\ndeployer or other developer; (ii) a description of the intended outputs of the\nhigh-risk artificial intelligence system; (iii) a description of the measures\nthe developer has taken to mitigate known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the reasonably foreseeable\ndeployment of the high-risk artificial intelligence system; and (iv) a\ndescription of how the high-risk artificial intelligence system should be\nused, not be used, and be monitored by an individual when such system is used\nto make, or is a substantial factor in making, a consequential decision; and_\n\n_4\\. Any additional documentation that is reasonably necessary to assist the\ndeployer or other developer in understanding the outputs and monitoring\nperformance of the high-risk artificial intelligence system for risks of\nalgorithmic discrimination._\n\n_C. Each developer that offers, sells, leases, gives, or otherwise makes\navailable to a deployer or other developer a high-risk artificial intelligence\nsystem shall make available to the deployer or other developer to the extent\nfeasible and necessary, information and documentation through artifacts such\nas system cards or predeployment impact assessments, including any risk\nmanagement policy designed and implemented and any relevant impact assessment\ncompleted, and such documentation and information shall enable the deployer,\nother developer, or a third party contracted by the deployer to complete an\nimpact assessment as required in § 59.1-609._\n\n_D. A developer that also serves as a deployer for any high-risk artificial\nintelligence system shall not be required to generate the documentation\nrequired by this section unless such high-risk artificial intelligence system\nis provided to an unaffiliated entity acting as a deployer or as otherwise\nrequired by law._\n\n_E. Nothing in this section shall be construed to require a developer to\ndisclose any trade secret, information that could create a security risk, or\nother confidential or proprietary information protected under state or federal\nlaw._\n\n_F. High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_G. For any disclosure required pursuant to this section, each developer\nshall, no later than 90 days after the developer performs an intentional and\nsubstantial modification to any high-risk artificial intelligence system,\nupdate such disclosure as necessary to ensure that such disclosure remains\naccurate._\n\n_H. 1. Each developer of a high-risk generative artificial intelligence system\nthat generates or substantially modifies synthetic content shall ensure that\nthe outputs of such high-risk artificial intelligence system (i) are\nidentifiable and detectable in a manner that is accessible by consumers using\nindustry-standard tools or tools provided by the developer; (ii) comply with\nany applicable accessibility requirements, as synthetic content, to the extent\nreasonably feasible; and (iii) apply such identification at the time the\noutput is generated;_\n\n_2\\. If such synthetic content is an audio, image, or video format that forms\npart of an evidently artistic, creative, satirical, fictional analogous work\nor program, such requirement for identifying outputs of high-risk artificial\nintelligence systems pursuant to subdivision 1 shall be limited to a manner\nthat does not hinder the display or enjoyment of such work or program._\n\n_3\\. The identification of outputs required by subdivision 1 shall not apply\nto (i) synthetic content that consists exclusively of text, is published to\ninform the public on any matter of public interest, or is unlikely to mislead\na reasonable person consuming such synthetic content or (ii) the outputs of a\nhigh-risk artificial intelligence system that performs an assistive function\nfor standard editing, does not substantially alter the input data provided by\nthe developer, or is used to detect, prevent, investigate, or prosecute any\ncrime as authorized by law._\n\n_I. Where multiple developers directly contribute to the development of a\nhigh-risk artificial intelligence system, each developer shall be subject to\nthe obligations and operating standards applicable to developers pursuant to\nthis section solely with respect to its activities contributing to the\ndevelopment of the high-risk artificial intelligence system._\n\n**_§ 59.1-609. Operating standards for deployers of high-risk artificial\nintelligence systems._ **\n\n_A. Each deployer of a high-risk artificial intelligence system shall use a\nreasonable duty of care to protect consumers from any known or reasonably\nforeseeable risks of algorithmic discrimination. In any enforcement action\nbrought by the Attorney General pursuant to § 59.1-611, there shall be a\nrebuttable presumption that a deployer of a high-risk artificial intelligence\nsystem used a reasonable duty of care as required by this subsection if the\ndeployer complied with the provisions of this section._\n\n_B. No deployer shall deploy or use a high-risk artificial intelligence system\nto make a consequential decision unless the deployer has designed and\nimplemented a risk management policy and program for such high-risk artificial\nintelligence system. The risk management policy shall specify the principles,\nprocesses, and personnel that the deployer shall use in maintaining the risk\nmanagement program to identify, mitigate, and document any risk of algorithmic\ndiscrimination that is a reasonably foreseeable consequence of deploying or\nusing such high-risk artificial intelligence system to make a consequential\ndecision. Each risk management policy and program designed, implemented, and\nmaintained pursuant to this subsection shall be reasonable considering the\nguidance and standards set forth in the latest version of:_\n\n_1\\. The Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology;_\n\n_2\\. Standard ISO/IEC 42001 of the International Organization for\nStandardization;_\n\n_3\\. A nationally or internationally recognized risk management framework for\nartificial intelligence systems with requirements that are substantially\nequivalent to, and at least as stringent as, the requirements set forth in\nthis section; or_\n\n_4\\. Any risk management framework for artificial intelligence systems that\nthe Attorney General may designate and is substantially equivalent to, and at\nleast as stringent as, the guidance and standards described in subdivision 1._\n\n_High-risk artificial intelligence systems that are in conformity with the\nlatest version of the Artificial Intelligence Risk Management Framework\npublished by the National Institute of Standards and Technology, Standard\nISO/IEC 42001 of the International Organization for Standardization, or\nanother nationally or internationally recognized risk management framework for\nartificial intelligence systems, or parts thereof, shall be presumed to be in\nconformity with related requirements set out in this section and in associated\nregulations._\n\n_C. Except as provided in this subsection, no deployer shall deploy or use a\nhigh-risk artificial intelligence system to make a consequential decision\nunless the deployer has completed an impact assessment for such high-risk\nartificial intelligence system. The deployer shall complete an impact\nassessment for a high-risk artificial intelligence system (i) before the\ndeployer initially deploys such high-risk artificial intelligence system and\n(ii) before a significant update to such high-risk artificial intelligence\nsystem is used to make a consequential decision._\n\n_Each impact assessment completed pursuant to this subsection shall include,\nat a minimum:_\n\n_1\\. A statement by the deployer disclosing (i) the purpose, intended use\ncases and deployment context of, and benefits afforded by the high-risk\nartificial intelligence system and (ii) whether the deployment or use of the\nhigh-risk artificial intelligence system poses any known or reasonably\nforeseeable risk of algorithmic discrimination and, if so, (a) the nature of\nsuch algorithmic discrimination and (b) the steps that have been taken, to the\nextent feasible, to mitigate such risk;_\n\n_2\\. For each post-deployment impact assessment completed pursuant to this\nsubsection, whether the intended use cases of the high-risk artificial\nintelligence system as updated were consistent with, or varied from, the\ndeveloper 's intended uses of such high-risk artificial intelligence system;_\n\n_3\\. A description of (i) the categories of data the high-risk artificial\nintelligence system processes as inputs and (ii) the outputs such high-risk\nartificial intelligence system produces;_\n\n_4\\. If the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize such high-risk artificial intelligence system;_\n\n_5\\. A list of any metrics used to evaluate the performance and known\nlimitations of the high-risk artificial intelligence system;_\n\n_6\\. A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that such high-risk artificial intelligence system is in use when\nsuch high-risk artificial intelligence system is in use;_\n\n_7\\. A description of any post-deployment monitoring performed and user\nsafeguards provided concerning such high-risk artificial intelligence system,\nincluding any oversight process established by the deployer to address issues\narising from deployment or use of such high-risk artificial intelligence\nsystem as such issues arise; and_\n\n_8\\. An analysis of such high-risk artificial intelligence system 's validity\nand reliability in accordance with standard industry practices and a\ndescription of any metrics used to evaluate the performance and known\nlimitations of such high-risk artificial intelligence system._\n\n_A single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed or used by a deployer. High-risk\nartificial intelligence systems that are in conformity with the latest version\nof the Artificial Intelligence Risk Management Framework published by the\nNational Institute of Standards and Technology, Standard ISO/IEC 42001 of the\nInternational Organization for Standardization, or another nationally or\ninternationally recognized risk management framework for artificial\nintelligence systems, or parts thereof, shall be presumed to be in conformity\nwith related requirements set out in this section and in associated\nregulations. If a deployer completes an impact assessment for the purpose of\ncomplying with another applicable law or regulation, such impact assessment\nshall be deemed to satisfy the requirements established in this subsection if\nsuch impact assessment is reasonably similar in scope and effect to the impact\nassessment that would otherwise be completed pursuant to this subsection. A\ndeployer that completes an impact assessment pursuant to this subsection shall\nmaintain such impact assessment and all records concerning such impact\nassessment for three years._\n\n_Throughout the period of time that a high-risk artificial intelligence system\nis deployed and for a period of at least three years following the final\ndeployment of such high-risk artificial intelligence system, the deployer\nshall retain all records concerning each impact assessment conducted on the\nhigh-risk artificial intelligence system, including all raw data used to\nevaluate the performance and known limitations of such system._\n\n_D. Not later than the time that a deployer uses a high-risk artificial\nintelligence system to interact with a consumer, the deployer shall disclose\nto the consumer that the deployer is interacting with an artificial\nintelligence system disclosing (i) the purpose of such high-risk artificial\nintelligence system, (ii) the nature of such system, (iii) the nature of the\nconsequential decision, (iv) the contact information for the deployer, and (v)\na description of the artificial intelligence system in plain language of such\nsystem, which shall include (a) a description of the personal characteristics\nor attributes that such system will measure or assess, (b) the method by which\nthe system measures or assesses such attributes or characteristics, (c) how\nsuch attributes or characteristics are relevant to the consequential decisions\nfor which the system should be used, (d) any human components of such system,\nand (e) how any automated components of such system are used to inform such\nconsequential decisions._\n\n_A deployer that has deployed a high-risk artificial intelligence system to\nmake a consequential decision concerning a consumer shall transmit to the\nconsumer the consequential decision without undue delay. If such consequential\ndecision is adverse to such consumer and based on personal data beyond\ninformation that the consumer provided directly to the deployer, the deployer\nshall provide to the consumer (a) a statement disclosing the principal reason\nor reasons for the consequential decision, including (1) the degree to which\nand manner in which the high-risk artificial intelligence system contributed\nto the consequential decision, (2) the type of data that was processed by such\nsystem in making the consequential decision, and (3) the sources of such data;\n(b) pursuant to the provisions of the Consumer Data Protection Act ( §\n59.1-575 et seq.), an opportunity to correct any inaccuracies in the\nconsumer's personal data that the high-risk artificial intelligence system\nprocessed in making, or as a substantial factor in making, the consequential\ndecision; and (c) an opportunity to appeal such adverse consequential decision\nconcerning the consumer arising from the deployment of such system. Any such\nappeal shall allow for human review, if technically reasonable and\npracticable, unless providing the opportunity for appeal is not in the best\ninterest of the consumer, including instances in which any delay might pose a\nrisk to the life or safety of such consumer._\n\n_E. Each deployer shall make available, in a manner that is clear and readily\navailable, a statement summarizing how such deployer manages any reasonably\nforeseeable risk of algorithmic discrimination that may arise from the use or\ndeployment of the high-risk artificial intelligence system._\n\n_F. For any disclosure required pursuant to this section, each deployer shall,\nno later than 30 days after the deployer is notified by the developer that the\ndeveloper has performed an intentional and substantial modification to any\nhigh-risk artificial intelligence system, update such disclosure as necessary\nto ensure that such disclosure remains accurate._\n\n_G. Any deployer who performs an intentional and substantial modification to\nany high-risk artificial intelligence system shall comply with the\ndocumentation and disclosure requirements for developers pursuant to\nsubsections B through G of § 59.1-608._\n\n_H. Nothing in this section shall be construed to require a deployer to\ndisclose any trade secret, information that could create a security risk, or\nother confidential or proprietary information protected under state or federal\nlaw._\n\n**_§ 59.1-610. Exemptions._ **\n\n_A. Nothing in this chapter shall be construed to restrict a developer 's or\ndeployer's ability to (i) comply with federal, state, or municipal ordinances\nor regulations; (ii) comply with a civil, criminal, or regulatory inquiry,\ninvestigation, subpoena, or summons by federal, state, local, or other\ngovernmental authorities; (iii) cooperate with law-enforcement agencies\nconcerning conduct or activity that the developer or deployer reasonably and\nin good faith believes may violate federal, state, or local law, ordinances,\nor regulations; (iv) investigate, establish, exercise, prepare for, or defend\nlegal claims; (v) provide a product or service specifically requested by a\nconsumer; (vi) perform under a contract to which a consumer is a party,\nincluding fulfilling the terms of a written warranty; (vii) take steps at the\nrequest of a consumer prior to entering into a contract; (viii) take immediate\nsteps to protect an interest that is essential for the life or physical safety\nof the consumer or another individual; (ix) prevent, detect, protect against,\nor respond to security incidents, identity theft, fraud, harassment, or\nmalicious or deceptive activities; (x) take actions to prevent, detect,\nprotect against, report, or respond to the production, generation,\nincorporation, or synthesization of child sex abuse material, or any illegal\nactivity, preserve the integrity or security of systems, or investigate,\nreport, or prosecute those responsible for any such action; (xi) engage in\npublic or peer-reviewed scientific or statistical research in the public\ninterest that adheres to all other applicable ethics and privacy laws and is\napproved, monitored, and governed by an institutional review board that\ndetermines, or similar independent oversight entities that determine, (a) that\nthe expected benefits of the research outweigh the risks associated with such\nresearch and (b) whether the developer or deployer has implemented reasonable\nsafeguards to mitigate the risks associated with such research; (xii) assist\nanother developer or deployer with any of the obligations imposed by this\nchapter; or (xiii) take any action that is in the public interest in the areas\nof public health, community health, or population health, but solely to the\nextent that such action is subject to suitable and specific measures to\nsafeguard the public._\n\n_B. The obligations imposed on developers or deployers by this chapter shall\nnot restrict a developer 's or deployer's ability to (i) conduct internal\nresearch to develop, improve, or repair products, services, or technologies;\n(ii) effectuate a product recall; (iii) identify and repair technical errors\nthat impair existing or intended functionality; or (iv) perform internal\noperations that are reasonably aligned with the expectations of the consumer\nor reasonably anticipated based on the consumer's existing relationship with\nthe developer or deployer._\n\n_C. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer to disclose trade secrets or information protected from\ndisclosure by state or federal law._\n\n_D. The obligations imposed on developers or deployers by this chapter shall\nnot apply where compliance by the developer or deployer with such obligations\nwould violate an evidentiary privilege under federal law or the laws of the\nCommonwealth._\n\n_E. Nothing in this chapter shall be construed to impose any obligation on a\ndeveloper or deployer that adversely affects the legally protected rights or\nfreedoms of any person, including the rights of any person to freedom of\nspeech or freedom of the press guaranteed in the First Amendment to the\nConstitution of the United States or under the Virginia Human Rights Act ( §\n2.2-3900 et seq.)._\n\n_F. The obligations imposed on developers or deployers by this chapter shall\nnot apply to any artificial intelligence system that is acquired by or for the\nfederal government or any federal agency or department, including the U.S.\nDepartment of Commerce, the U.S. Department of Defense, and the National\nAeronautics and Space Administration, unless such artificial intelligence\nsystem is a high-risk artificial intelligence system that is used to make, or\nis a substantial factor in making, a decision concerning employment or\nhousing._\n\n_G. For the purposes of this subsection:_\n\n_\" Affiliate\" means the same as that term is defined in § 6.2-899._\n\n_\" Bank\" means the same as that term is defined in § 6.2-800._\n\n_\" Credit union\" means the same as that term is defined in § 6.2-1300._\n\n_\" Federal credit union\" means a credit union duly organized under federal\nlaw._\n\n_\" Mortgage lender\" means the same as that term is defined in § 6.2-1600._\n\n_\" Out-of-state bank\" means the same as that term is defined in § 6.2-836._\n\n_\" Out-of-state credit union\" means a credit union organized and doing\nbusiness in another state._\n\n_\" Savings institution\" means the same as that term is defined in § 6.2-1100._\n\n_\" Subsidiary\" means the same as that term is defined in § 6.2-700._\n\n_The obligations imposed on developers or deployers by this chapter shall be\ndeemed satisfied for any bank, out-of-state bank, credit union, federal credit\nunion, mortgage lender, out-of-state credit union, savings institution, or any\naffiliate, subsidiary, or service provider thereof if such bank, out-of-state\nbank, credit union, federal credit union, mortgage lender, out-of-state credit\nunion, savings institution, or affiliate, subsidiary, or service provider is\nsubject to the jurisdiction of any state or federal regulator under any\npublished guidance or regulations that apply to the use of high-risk\nartificial intelligence systems and such guidance or regulations._\n\n_H. For purposes of this subsection, \"insurer\" means the same as that term is\ndefined in § 38.2-100._\n\n_The provisions of this chapter shall not apply to any insurer, or any high-\nrisk artificial intelligence system developed by or for or deployed by an\ninsurer for use in the business of insurance, if such insurer is regulated and\nsupervised by the State Corporation Commission or a comparable federal\nregulating body and subject to examination by such entity under any existing\nstatutes, rules, or regulations pertaining to unfair trade practices and\nunfair discrimination prohibited under Chapter 5 ( § 38.2-500 et seq.) of\nTitle 38.2, or published guidance or regulations that apply to the use of\nhigh-risk artificial intelligence systems and such guidance or regulations aid\nin the prevention and mitigation of algorithmic discrimination caused by the\nuse of a high-risk artificial intelligence system or any risk of algorithmic\ndiscrimination that is reasonably foreseeable as a result of the use of a\nhigh-risk artificial intelligence system. Nothing in this chapter shall be\nconstrued to delegate existing regulatory oversight of the business of\ninsurance to any department or agency other than the Bureau of Insurance of\nthe Virginia State Corporation Commission._\n\n_I. The provisions of this chapter shall not apply to the development of an\nartificial intelligence system that is used exclusively for research,\ntraining, testing, or other pre-deployment activities performed by active\nparticipants of any sandbox software or sandbox environment established and\nsubject to oversight by a designated agency or other government entity and\nthat is in compliance with the provisions of this chapter._\n\n_J. The provisions of this chapter shall not apply to a developer or deployer,\nor other person who develops, deploys, puts into service, or intentionally\nmodifies, as applicable, a high-risk artificial intelligence system that (i)\nhas been approved, authorized, certified, cleared, developed, or granted by a\nfederal agency acting within the scope of the federal agency 's authority, or\nby a regulated entity subject to the supervision and regulation of the Federal\nHousing Finance Agency or (ii) is in compliance with standards established by\na federal agency or by a regulated entity subject to the supervision and\nregulation of the Federal Housing Finance Agency, if the standards are\nsubstantially equivalent or more stringent than the requirements of this\nchapter._\n\n_K. The provisions of this chapter shall not apply to a developer or deployer,\nor other person that (i) facilitates or engages in the provision of telehealth\nservices, as defined in § 32.1-122.03:1, or (ii) is a covered entity within\nthe meaning of the federal Health Insurance Portability and Accountability Act\nof 1996 (42 U.S.C. § 1320d et seq.) and the regulations promulgated under such\nfederal act, as both may be amended from time to time, and is providing (a)\nhealth care recommendations that (1) are generated by an artificial\nintelligence system and (2) require a health care provider, as defined in §\n8.01-581.1, to take action to implement the recommendations or (b) services\nutilizing an artificial intelligence system for an administrative, quality\nmeasurement, security, or internal cost or performance improvement function._\n\n_L. If a developer or deployer engages in any action authorized by an\nexemption set forth in this section, the developer or deployer bears the\nburden of demonstrating that such action qualifies for such exemption._\n\n_M. If a developer or deployer withholds information pursuant to an exemption\nset forth in this chapter for which disclosure would otherwise be required by\nthis chapter, including the exemption from disclosure of trade secrets, the\ndeveloper or deployer shall notify the subject of disclosure and provide a\nbasis for withholding the information. If a developer or deployer redacts any\ninformation pursuant to an exemption from disclosure, the developer or\ndeployer shall notify the subject of disclosure that the developer or deployer\nis redacting such information and provide the basis for such decision to\nredact._\n\n**_§ 59.1-611. Enforcement; civil penalties._ **\n\n_A. The Attorney General shall have exclusive authority to enforce the\nprovisions of this chapter._\n\n_B. Whenever the Attorney General has reasonable cause to believe that any\nperson has engaged in or is engaging in any violation of this chapter, the\nAttorney General is empowered to issue a civil investigative demand. The\nprovisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative\ndemands issued pursuant to this section. In rendering and furnishing any\ninformation requested pursuant to a civil investigative demand issued pursuant\nto this section, a developer or deployer may redact or omit any trade secrets\nor information protected from disclosure by state or federal law. If a\ndeveloper or deployer refuses to disclose, redacts, or omits information based\non the exemption from disclosure of trade secrets, such developer or deployer\nshall affirmatively state to the Attorney General that the basis for\nnondisclosure, redaction, or omission is because such information is a trade\nsecret. To the extent that any information requested pursuant to a civil\ninvestigative demand issued pursuant to this section is subject to attorney-\nclient privilege or work-product protection, disclosure of such information\npursuant to the civil investigative demand shall not constitute a waiver of\nsuch privilege or protection. Any information, statement, or documentation\nprovided to the Attorney General pursuant to this section shall be exempt from\ndisclosure under the Virginia Freedom of Information Act (§ 2.2-3700 et\nseq.)._\n\n_C. Notwithstanding any contrary provision of law, the Attorney General may\ncause an action to be brought in the appropriate circuit court in the name of\nthe Commonwealth to enjoin any violation of this chapter. The circuit court\nhaving jurisdiction may enjoin such violation notwithstanding the existence of\nan adequate alternative remedy at law._\n\n_D. Any person who violates the provisions of this chapter shall be subject to\na civil penalty in an amount not to exceed $1,000 plus reasonable attorney\nfees, expenses, and costs, as determined by the court. Any person who\nwillfully violates the provisions of this chapter shall be subject to a civil\npenalty in an amount not less than $1,000 and not more than $10,000 plus\nreasonable attorney fees, expenses, and costs, as determined by the court.\nSuch civil penalties shall be paid into the Literary Fund._\n\n_E. Each violation of this chapter shall constitute a separate violation and\nshall be subject to any civil penalties imposed under this section._\n\n_F. The Attorney General may require that a developer disclose to the Attorney\nGeneral any statement or documentation described in this chapter if such\nstatement or documentation is relevant to an investigation conducted by the\nAttorney General. The Attorney General may also require that a deployer\ndisclose to the Attorney General any risk management policy designed and\nimplemented, impact assessment completed, or record maintained pursuant to\nthis chapter if such risk management policy, impact assessment, or record is\nrelevant to an investigation conducted by the Attorney General._\n\n_G. In an action brought by the Attorney General pursuant to this section, it\nshall be an affirmative defense that the developer or deployer (i) discovers a\nviolation of any provision of this chapter through red-teaming or other\nmethod; (ii) no later than 45 days after discovering such violation (a) cures\nsuch violation and (b) provides notice to the Attorney General in a form and\nmanner as prescribed by the Attorney General that such violation has been\ncured and evidence that any harm caused by such violation has been mitigated;\nand (iii) is otherwise in compliance with the requirements of this chapter._\n\n_H. Prior to causing an action against a developer or deployer for a violation\nof this chapter pursuant to subsection C, the Attorney General shall\ndetermine, in consultation with the developer or deployer, if it is possible\nto cure the violation. If it is possible to cure such violation, the Attorney\nGeneral may issue a notice of violation to the developer or deployer and\nafford the developer or deployer the opportunity to cure such violation within\n45 days of the receipt of such notice of violation. In determining whether to\ngrant such opportunity to cure such violation, the Attorney General shall\nconsider (i) the number of violations; (ii) the size and complexity of the\ndeveloper or deployer; (iii) the nature and extent of the developer 's or\ndeployer's business; (iv) the substantial likelihood of injury to the public;\n(v) the safety of persons or property; and (vi) whether such violation was\nlikely caused by human or technical error. If the developer or deployer fails\nto cure such violation within 45 days of the receipt of such notice of\nviolation, the Attorney General may proceed with such action._\n\n_I. Nothing in this chapter shall create a private cause of action in favor of\nany person aggrieved by a violation of this chapter._\n\n**_§ 59.1-612. Construction of chapter._ **\n\n_A. This chapter is declared to be remedial, with the purposes of protecting\nconsumers and ensuring consumers receive information about consequential\ndecisions affecting them. The provisions of this chapter granting rights or\nprotections to consumers shall be construed broadly and exemptions construed\nnarrowly._\n\n_B. If any provision of this chapter or its application to any person or\ncircumstance is held invalid, the invalidity shall not affect other provisions\nor applications of this chapter that can be given effect without the invalid\nprovision or application, and to this end all the provisions of this chapter\nare hereby expressly declared to be severable._\n\n**2\\. That the provisions of this act shall become effective on July 1,\n2026.**\n\n**3\\. That compliance with the provisions of Chapter 58 ( § 59.1-607 et seq.)\nof Title 59.1 of the Code of Virginia, as created by this act, shall not (i)\nrelieve a person from liability for any causes of action that existed at\ncommon law or by statute prior to July 1, 2026, or (ii) be construed to modify\nor otherwise affect, preempt, limit, or displace any causes of action that\nexisted at common law or by statute prior to July 1, 2026.**\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": true
    }
  ]
}