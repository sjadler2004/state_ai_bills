{
  "bill_id": "NY2025000S1169",
  "source_url": "http://custom.statenet.com/public/resources.cgi?id=ID:bill:NY2025000S1169&cuiq=93d84396-c63b-526a-b152-38b7f79b4cfd&client_md=e4f6fea4-27b4-5d41-b7d3-766fe52569f0",
  "versions": [
    {
      "date": "01/08/2025",
      "label": "Introduced",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:NY2025000S1169&verid=NY2025000S1169_20250108_0_I&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 NY S 1169</td> <td><table><tr><td class=\"label\">Author:</td> <td>Gonzalez</td></tr> <tr><td class=\"label\">Version:</td> <td>Introduced</td></tr> <tr><td class=\"label\">Version Date:</td> <td>01/08/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">\n    <b>STATE OF NEW YORK</b>\n   </p>\n   <p class=\"center\">1169 </p>\n   <p class=\"center\">2025-2026 Regular Sessions </p>\n   <p class=\"center\">\n    <b>IN SENATE</b>\n   </p>\n   <p class=\"center\">January 8, 2025 </p>\n   <p class=\"indent\">Introduced by Sen. GONZALEZ -- read twice and ordered printed, and when printed to be committed to the Committee on Internet and Technology </p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"indent\">AN ACT to amend the civil rights law and the executive law, in relation to the use of artificial intelligence systems </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">\n     <b>The People of the State of New York, represented in Senate and Assembly, do enact as follows:</b>\n    </p>\n   </span>\n   <p class=\"indent\">Section 1. This act shall be known and may be cited as the &quot;New York artificial intelligence act (New York AI act)&quot;. </p>\n   <p class=\"indent\">Section 2. Legislative findings and intent. The legislature finds and declares the following: </p>\n   <p class=\"indent\">(a) A revolution in artificial intelligence (AI) has advanced to the point that comprehensive regulations must be enacted to protect New Yorkers. </p>\n   <p class=\"indent\">(b) Artificial intelligence is already an integral part of New Yorkers&#39; daily lives. In the private sector, AI is currently in use in areas such as education, health care, employment, insurance, credit scoring, public safety, retail, banking and financial services, media, and more with little transparency or oversight. A growing body of research shows that AI systems that are deployed without adequate testing, sufficient oversight and robust guardrails can harm consumers and deny historically disadvantaged groups the full measure of their civil rights and liberties, thereby further entrenching inequalities. The legislature must act to ensure that all uses of AI, especially those that affect important life chances, are free from harmful biases, protect our privacy, and work for the public good. </p>\n   <p class=\"indent\">(c) Safe innovation must remain a priority for the state. New York state is home to thousands of technology start-ups, many of which experiment with new applications of AI and which have the potential to find new ways to employ technology at the service of New Yorkers. The goal of the legislature is to encourage safe innovation in the AI sector by providing clear guidance for AI development, testing, and validation both before a product is launched and throughout the product&#39;s life cycle. </p>\n   <p class=\"indent\">(d) New York must establish that the burden of responsibility of proving that AI products do not cause harm to New Yorkers will be shouldered by the developers and deployers of AI. While government and civil society must act to audit and enforce human rights laws around the use of AI, the companies employing and profiting from the use of AI must lead in ensuring that their products are free from algorithmic discrimination. </p>\n   <p class=\"indent\">(e) Close collaboration and communication between New York state and industry partners is key to ensuring that innovation can occur with safeguards to protect all New Yorkers. This legislation will ensure that lines of communication exist and that there is clear statutory authority to investigate and prosecute entities that break the law. </p>\n   <p class=\"indent\">(f) As new forms of AI are developed beyond what is currently technologically feasible, the goal of the legislature is to use this section as a guiding light for future regulations. </p>\n   <p class=\"indent\">(g) Lastly, it is in the interest of all New Yorkers that certain uses of AI that infringe on fundamental rights, deepen structural inequality, or that result in unequal access to services shall be banned. </p>\n   <p class=\"indent\">Section 3. The civil rights law is amended by adding a new article 8-A to read as follows: </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">ARTICLE 8-A</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">PROTECTIONS REGARDING USE OF ARTIFICIAL INTELLIGENCE</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 85. Definitions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">86. Unlawful discriminatory practices.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">86-a. Deployer and developer obligations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">86-b. Whistleblower protections.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">87. Audits.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">88. High-risk AI system reporting requirements.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89. Risk management policy and program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89-a. Social scoring AI systems prohibited.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89-b. Enforcement.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 85. Definitions. The following terms shall have the following meanings:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. &quot;Algorithmic discrimination&quot; means any condition in which the use of an AI system contributes to unjustified differential treatment or impacts, disfavoring people based on their actual or perceived age, race, ethnicity, creed, religion, color, national origin, citizenship or immigration status, sexual orientation, gender identity, gender expression, military status, sex, disability, predisposing genetic characteristics, familial status, marital status, pregnancy, pregnancy outcomes, disability, height, weight, reproductive health care or autonomy, status as a victim of domestic violence or other classification protected under state or federal laws. Algorithmic discrimination shall not include:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) a developer&#39;s or deployer&#39;s testing of their own AI system to identify, mitigate, and prevent discriminatory bias;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) expanding an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) an act or omission by or on behalf of a private club or other establishment that is not in fact open to the public, as set forth in Title II of the federal Civil Rights Act of 1964, 42 U.S.C. section 2000a(e), as amended.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. &quot;Artificial intelligence system&quot; or &quot;AI system&quot; means a machinebased system or combination of systems, that for explicit and implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Artificial intelligence shall not include any software used primarily for basic computerized processes, such as anti-malware, anti-virus, auto-correct functions, calculators, databases, data storage, electronic communications, firewall, internet domain registration, internet website loading, networking, spam and robocall-filtering, spellcheck tools, spreadsheets, web caching, web hosting, or any tool that relates only to internal management affairs such as ordering office supplies or processing payments, and that do not materially affect the rights, liberties, benefits, safety or welfare of any individual within the state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. &quot;Auditor&quot; shall refer to an independent entity including but not limited to an individual, non-profit, firm, corporation, partnership, cooperative, or association commissioned to perform an audit.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. &quot;Consequential decision&quot; means a decision or judgment that has a material, legal or similarly significant effect on an individual&#39;s life relating to the impact of, access to, or the cost, terms, or availability of, any of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) Employment, workers&#39; management, or self-employment, including, but not limited to, all of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) Pay or promotion;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) Hiring or termination; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) Automated task allocation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) Education and vocational training, including, but not limited to, all of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) Assessment or grading, including, but not limited to, detecting student cheating or plagiarism;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) Accreditation;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) Certification;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) Admissions; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(v) Financial aid or scholarships.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) Housing or lodging, including rental or short-term housing or lodging.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) Essential utilities, including electricity, heat, water, internet or telecommunications access, or transportation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(e) Family planning, including adoption services or reproductive services, as well as assessments related to child protective services.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(f) Health care or health insurance, including mental health care, dental, or vision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(g) Financial services, including a financial service provided by a mortgage company, mortgage broker, or creditor.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(h) Law enforcement activities, including the allocation of law enforcement personnel or assets, the enforcement of laws, maintaining public order, or managing public safety.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) Government services.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(j) Legal services.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. &quot;Deployer&quot; means a person, partnership, association or corporation that uses an AI system or commerce in the state of New York or provides an AI system for use by the general public in the state of New York. A developer may also be considered a deployer if its actions satisfy this definition.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. &quot;Deployer-employer&quot; means a deployer that is an employer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. &quot;Developer&quot; means a person, partnership, or corporation that designs, codes, or produces an AI system, or creates a substantial change with respect to an AI system, whether for its own use in the state of New York or for use by a third party in the state of New York.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. &quot;Developer-employer&quot; means a developer that is an employer.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. &quot;Employee&quot; means an individual who performs services for and under the control and direction of an employer for wages or other remuneration, including former employees, or natural persons employed as independent contractors to carry out work in furtherance of an employer&#39;s business enterprise who are not themselves employers.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. &quot;Employer&quot; means any person, firm, partnership, institution, corporation, or association that employs one or more employees.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. &quot;End user&quot; means any individual or group of individuals that:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) is the subject of a consequential decision made entirely by or with the assistance of an AI system; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) interacts, directly or indirectly, with the relevant AI system on behalf of an individual or group that is the subject of a consequential decision made entirely by or with the assistance of an AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. &quot;High-risk AI system&quot; means any AI system that, when deployed: (a) is a substantial factor in making a consequential decision; or (b) will have a material impact on the statutory or constitutional rights, civil liberties, safety, or welfare of an individual in the state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. &quot;Software stack&quot; means the group of individual software components that work together to support the execution of an AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">14. &quot;Substantial change&quot; means any (a) deliberate modification to an AI system that would result in material inaccuracies in the reports created under section eighty-eight of this article; or (b) unintentional and substantial change in the data that the AI system uses as input data.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">15. &quot;Substantial factor&quot; means a factor that assists in making a consequential decision or is capable of altering the outcome of a consequential decision. &quot;Substantial factor&quot; includes, but is not limited to, any use of an AI system to generate any content, decision, prediction, or recommendation that is used as a basis, in whole or in part, to make a consequential decision regarding an end user.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 86. Unlawful discriminatory practices. It shall be an unlawful discriminatory practice:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. for a developer or deployer to use, sell, or share a high-risk AI system or a product featuring a high-risk AI system that produces algorithmic discrimination; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. for a developer to use, sell, or share a high-risk AI system or a product featuring a high-risk AI system that has not passed an independent audit, in accordance with section eighty-seven of this article, that has found that the product does not in fact produce algorithmic discrimination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 86-a. Deployer and developer obligations. 1. (a) Any deployer that employs a high-risk AI system for a consequential decision must inform the end user at least five business days prior to the use of such system for the making of a consequential decision in clear, conspicuous, and consumer-friendly terms, made available in each of the languages in which the company offers its end services, that AI systems will be used to make a decision or to assist in making a decision. The deployer must allow sufficient time and opportunity in a clear, conspicuous, and consumer-friendly manner for the consumer to opt-out of the automated process and for the decision to be made by a human representative. A consumer may not be punished or face any other adverse action for opting out of a decision by an AI system and the deployer must render a decision to the consumer within forty-five days.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) If a deployer employs a high-risk AI system for a consequential decision to determine whether to or on what terms to confer a benefit on an end user, the deployer shall offer the end user the option to waive their right to advance notice of five business days under this subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) If the end user clearly and affirmatively waives their right to five business days&#39; notice, the deployer shall then inform the end user at least one business day before the making of the consequential decision in clear, conspicuous, and consumer-friendly terms, made available in each of the languages in which the company offers its end services, that AI systems will be used to make a decision or to assist in making a decision. The deployer must allow sufficient time and opportunity in a clear, conspicuous, and consumer-friendly manner for the consumer to opt-out of the automated process and for the decision to be made by a human representative. A consumer may not be punished or face any other adverse action for opting out of a decision by an AI system and the deployer must render a decision to the consumer within forty-five days.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Any deployer that employs a high-risk AI system for a consequential decision must inform the end user within five days in a clear, conspicuous, and consumer-friendly manner if a consequential decision has been made entirely by or with assistance of an automated system. The deployer must then provide and explain a process for the end user to appeal the decision, which must at minimum allow the end user to (a) formally contest the decision, (b) provide information to support their position, and (c) obtain meaningful human review of the decision. A deployer must respond to an end user&#39;s appeal within forty-five days of receipt of the appeal. That period may be extended once by forty-five additional days where reasonably necessary, taking into account the complexity and number of appeals. The deployer must inform the end user of any such extension within forty-five days of receipt of the appeal, together with the reasons for the delay.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The deployer or developer of a high-risk AI system is legally responsible for quality and accuracy of all consequential decisions made, including any bias, algorithmic discrimination, and/or misinformation resulting from the operation of the AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. The rights and obligations under this section may not be waived by any person, partnership, association or corporation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 86-b. Whistleblower protections. 1. Developer-employers and/or deployer-employers of high-risk AI systems shall not:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) prevent an employee from disclosing information to the attorney general, including through terms and conditions of employment or seeking to enforce terms and conditions of employment, if the employee has reasonable cause to believe the information indicates a violation of this article; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) retaliate against an employee for disclosing information to the attorney general pursuant to this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. An employee harmed by a violation of this article may petition a court for appropriate relief as provided in subdivision five of section seven hundred forty of the labor law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Developer-employers and deployer-employers of high-risk AI systems shall provide a clear notice to all employees working on such AI systems of their rights and responsibilities under this article, including the right of employees of contractors and subcontractors to use the developer&#39;s internal process for making protected disclosures pursuant to subdivision four of this section. A developer-employer or deployer-employer is presumed to be in compliance with the requirements of this subdivision if the developer-employer or deployer-employer does either of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) at all times post and display within all workplaces maintained by the developer-employer or deployer-employer a notice to all employees of their rights and responsibilities under this article, ensure that all new employees receive equivalent notice, and ensure that employees who work remotely periodically receive an equivalent notice; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) no less frequently than once every year, provides written notice to all employees of their rights and responsibilities under this article and ensures that the notice is received and acknowledged by all of those employees.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Each developer-employer and deployer-employer shall provide a reasonable internal process through which an employee may anonymously disclose information to the developer if the employee believes in good faith that the information indicates that the developer has violated any provision of this article or any other law, or has made false or materially misleading statements related to its safety and security protocol, or failed to disclose known risks to employees, including, at a minimum, a monthly update to the person who made the disclosure regarding the status of the developer&#39;s investigation of the disclosure and the actions taken by the developer in response to the disclosure.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. This section does not limit protections provided to employees under section seven hundred forty of the labor law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 87. Audits. 1. Prior to deployment of a high-risk AI system, six months after deployment, and at least every eighteen months thereafter for each calendar year a high-risk AI system is in use after the first post-deployment audit, every developer or deployer of a high-risk AI system shall cause to be conducted at least one third-party audit in compliance with the provisions of this section to ensure that the product does not produce algorithmic discrimination and complies with the provisions of this article. Regardless of final findings, the deployer or developer shall deliver all audits conducted under this section to the attorney general.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. A deployer or developer may hire more than one auditor to fulfill the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The audit shall include the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) an analysis of data management policies including whether personal or sensitive data relating to a consumer is subject to data security protection standards that comply with the requirements of section eight hundred ninety-nine-bb of the general business law;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) an analysis of the system accuracy and reliability according to each specified use case listed in the entity&#39;s reporting document filed by the developer or deployer under section eighty-eight of this article;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) disparate impacts and a determination of whether the product produces algorithmic discrimination in violation of this article by each intended and foreseeable identified use as identified by the deployer and developer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) analysis of how the technology complies with existing relevant federal, state, and local privacy and data privacy laws; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(e) an evaluation of the developer&#39;s or deployer&#39;s documented risk management policy and program required under section eighty-nine of this article for conformity with subdivision one of such section eighty-nine.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. The attorney general may promulgate further rules as necessary to ensure that audits under this section assess whether or not AI systems produce algorithmic discrimination and otherwise comply with the provisions of this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. The independent auditor shall have complete and unredacted copies of all reports previously filed by the deployer or developer under section eighty-eight of this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. An audit conducted under this section shall be completed in its entirety without the assistance of an AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. (a) An auditor shall be an independent entity including but not limited to an individual, non-profit, firm, corporation, partnership, cooperative, or association.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) For the purposes of this article, no auditor may be commissioned by a developer or deployer of an AI system if such entity has already been commissioned to provide any auditing or non-auditing service, including but not limited to financial auditing, cybersecurity auditing, or consulting services of any type, to the commissioning company in the past twelve months.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) Fees paid to auditors may not be contingent on the result of the audit and the commissioning company shall not provide any incentives or bonuses for a positive audit result.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. The attorney general may promulgate further rules to ensure (a) the independence of auditors under this section, and (b) that teams conducting audits incorporate feedback from communities that may foreseeably be the subject of algorithmic discrimination with respect to the AI system being audited.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 88. High-risk AI system reporting requirements. 1. Every developer and deployer of a high-risk AI system shall comply with the reporting requirements of this section. Regardless of final findings, reports shall be filed with the attorney general prior to deployment of a highrisk AI system and then annually, or after each substantial change to the system, whichever comes first.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Together with each report required to be filed under this section, developers and deployers shall file with the attorney general a copy of the last completed independent audit required by this article and a legal attestation that the high-risk AI system: (a) does not violate any provision of this article; or (b) may violate or does violate one or more provisions of this article, that there is a plan of remediation to bring the high-risk AI system into compliance with this article, and a summary of such plan of remediation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Developers of high-risk AI systems shall file with the attorney general a report containing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) a description of the system including:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a description of the system&#39;s software stack;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) the purpose of the system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) the system&#39;s use to end users; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) reasonably foreseeable uses outside of the current or intended uses;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(v) how the system should be used or not used;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) the intended outputs of the system and whether the outputs can be or are otherwise appropriate to be used for any purpose not previously articulated;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) the methods for training of their models including:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) any pre-processing steps taken to prepare datasets for the training of a model underlying a high-risk AI system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) datasheets comprehensively describing the datasets upon which models were trained and evaluated, how and why datasets were collected, how that training data will be used and maintained going forward through the development cycle; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) steps taken to ensure compliance with privacy, data privacy, data security, and copyright laws;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) detailed use and data management policies;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(e) any other information necessary to allow the deployer to understand the outputs and monitor the system for compliance with this article;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(f) any other information necessary to allow the deployer to comply with the requirements of subdivision four of this section; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(g) for any high-risk AI system that is a substantial factor in making a consequential decision:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a detailed description of the proposed uses of the system, including what consequential decisions the system will support;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) a detailed description of the system&#39;s capabilities and any developer-imposed limitations, including capabilities outside of its intended use, when the system should not be used, any safeguards or guardrails in place to protect against unintended, inappropriate, or disallowed uses, and testing of any such safeguards or guardrails;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) an internal risk assessment including documentation and results of testing conducted to identify all reasonably foreseeable risks related to algorithmic discrimination, accuracy and reliability, privacy and autonomy, and safety and security, as well as actions taken to address those risks, and subsequent testing to assess the efficacy of actions taken to address risks; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) whether the system should be monitored, and if so, how such system should be monitored.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Deployers of high-risk AI systems shall file with the attorney general a report containing the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) a description of the system including:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a description of the system&#39;s software stack;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) the purpose of the system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) the system&#39;s use to end users; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) reasonably foreseeable uses outside of the current or intended uses;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) the intended outputs of the system and whether the outputs can be or are otherwise appropriate to be used for any purpose not previously articulated;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) assessment of the relative benefits and costs to the consumer given the system&#39;s purpose, capabilities, and probable use cases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) whether the deployer collects revenue or plans to collect revenue from use of the high-risk AI system, and if so, how it monetizes or plans to monetize use of the system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(e) for any high-risk AI system that is a substantial factor in making a consequential decision:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a detailed description of the proposed uses of the system, including what consequential decisions the system will support;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) whether the system is designed to make consequential decisions itself or whether and how it supports consequential decisions;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) a detailed description of the system&#39;s capabilities and any deployer-imposed limitations, including capabilities outside of its intended use, when the system should not be used, any safeguards or guardrails in place to protect against unintended, inappropriate, or disallowed uses, and testing of any such safeguards or guardrails;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) an assessment of the relative benefits and costs to the consumer given the system&#39;s purpose, capabilities, and probable use cases;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(v) an internal risk assessment including documentation and results of testing conducted to identify all reasonably foreseeable risks related to algorithmic discrimination, accuracy and reliability, privacy and autonomy, and safety and security, as well as actions taken to address those risks, and subsequent testing to assess the efficacy of actions taken to address risks; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(vi) whether the system should be monitored, and if so, how such system should be monitored.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. The attorney general shall:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) promulgate rules for a process whereby developers and deployers may request redaction of portions of reports required under this section to ensure that they are not required to disclose sensitive and protected information; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) maintain an online database that is accessible to the general public with reports, redacted in accordance with this subdivision, and audits required by this article which shall be updated biannually.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. For high-risk AI systems which are already in deployment at the time of the effective date of this article, developers and deployers shall have eighteen months from such effective date to complete and file the reports and independent audit required by this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89. Risk management policy and program. 1. Each developer or deployer of high-risk AI systems shall plan, document, and implement a risk management policy and program to govern development or deployment, as applicable, of such high-risk AI system. The risk management policy and program shall specify and incorporate the principles, processes, and personnel that the deployer uses to identify, document, and mitigate known or reasonably foreseeable risks of algorithmic discrimination covered under subdivision one of section eighty-six of this article. The risk management policy and program shall be an iterative process planned, implemented, and regularly and systematically reviewed and updated over the life cycle of a high-risk AI system, requiring regular, systematic review and updates, including updates to documentation. A risk management policy and program implemented and maintained pursuant to this section shall be reasonable considering:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) The guidance and standards set forth in version 1.0 of the &quot;Artificial Intelligence Risk Management Framework&quot; published by the National Institute of Standards and Technology in the United States department of commerce, or the latest version of the &quot;Artificial Intelligence Risk Management Framework&quot; published by the National Institute of Standards and Technology if, in the attorney general&#39;s discretion, the latest version of the &quot;Artificial Intelligence Risk Management Framework&quot; published by the National Institute of Standards and Technology in the United States department of commerce is at least as stringent as version 1.0;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) The size and complexity of the developer or deployer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) The nature, scope, and intended uses of the high-risk AI system developed or deployed; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) The sensitivity and volume of data processed in connection with the high-risk AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. A risk management policy and program implemented pursuant to subdivision one of this section may cover multiple high-risk AI systems developed by the same developer or deployed by the same deployer if sufficient.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The attorney general may require a developer or a deployer to disclose the risk management policy and program implemented pursuant to subdivision one of this section in a form and manner prescribed by the attorney general. The attorney general may evaluate the risk management policy and program to ensure compliance with this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89-a. Social scoring AI systems prohibited. No person, partnership, association or corporation shall develop, deploy, use, or sell an AI system which evaluates or classifies the trustworthiness of natural persons over a certain period of time based on their social behavior or known or predicted personal or personality characteristics, with the social score leading to either or both of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. differential treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. differential treatment of certain natural persons or whole groups thereof that is unjustified or disproportionate to their social behavior or its gravity.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89-b. Enforcement. 1. Whenever there shall be a violation of any provision of this article, an application may be made by the attorney general in the name of the people of the state of New York, to the supreme court having jurisdiction by a special proceeding to issue an injunction, and upon notice to the respondent of not less than ten days, to enjoin and restrain the continuance of such violation; and if it shall appear to the satisfaction of the court that the respondent has, in fact, violated this article, an injunction may be issued by the court, enjoining and restraining any further violations, without requiring proof that any person has, in fact, been injured or damaged thereby. In any such proceeding, the court may make allowances to the attorney general as provided in paragraph six of subdivision (a) of section eighty-three hundred three of the civil practice law and rules, and direct restitution. Whenever the court shall determine that a violation of this article has occurred, the court may impose a civil penalty of not more than twenty thousand dollars for each violation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. There shall be a private right of action by plenary proceeding for any person harmed by any violation of this article by any natural person or entity. The court shall award compensatory damages and legal fees to the prevailing party.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. In evaluating any motion to dismiss a plenary proceeding commenced pursuant to subdivision two of this section, the court shall presume the specified AI system was created and/or operated in violation of a specified law or laws and that such violation caused the harm or harms alleged.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) A defendant can rebut presumptions made pursuant to this subdivision through clear and convincing evidence that the specified AI system did not cause the harm or harms alleged and/or did not violate the alleged law or laws. An algorithmic audit can be considered as evidence in rebutting such presumptions, but the mere existence of such an audit, without additional evidence, shall not be considered clear and convincing evidence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) Where such presumptions are not rebutted pursuant to this subdivision, the action shall not be dismissed.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) Where such presumptions are rebutted pursuant to this subdivision, a motion to dismiss an action shall be adjudicated without any consideration of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. The supreme court in the state shall have jurisdiction over any action, claim, or lawsuit to enforce the provisions of this article.</u>\n   </p>\n   <p class=\"indent\">Section 4. Section 296 of the executive law is amended by adding a new subdivision 23 to read as follows: </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">23. It shall be an unlawful discriminatory practice under this section for a deployer or a developer, as such terms are defined in section eighty-five of the civil rights law, to engage in an unlawful discriminatory practice under section eighty-six of the civil rights law.</u>\n   </p>\n   <p class=\"indent\">Section 5. This act shall take effect immediately.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 NY S 1169 | | Author: | Gonzalez  \n---|---  \nVersion: | Introduced  \nVersion Date: | 01/08/2025  \n  \n**STATE OF NEW YORK**\n\n1169\n\n2025-2026 Regular Sessions\n\n**IN SENATE**\n\nJanuary 8, 2025\n\nIntroduced by Sen. GONZALEZ -- read twice and ordered printed, and when\nprinted to be committed to the Committee on Internet and Technology\n\nAN ACT to amend the civil rights law and the executive law, in relation to the\nuse of artificial intelligence systems\n\n**The People of the State of New York, represented in Senate and Assembly, do\nenact as follows:**\n\nSection 1. This act shall be known and may be cited as the \"New York\nartificial intelligence act (New York AI act)\".\n\nSection 2. Legislative findings and intent. The legislature finds and declares\nthe following:\n\n(a) A revolution in artificial intelligence (AI) has advanced to the point\nthat comprehensive regulations must be enacted to protect New Yorkers.\n\n(b) Artificial intelligence is already an integral part of New Yorkers' daily\nlives. In the private sector, AI is currently in use in areas such as\neducation, health care, employment, insurance, credit scoring, public safety,\nretail, banking and financial services, media, and more with little\ntransparency or oversight. A growing body of research shows that AI systems\nthat are deployed without adequate testing, sufficient oversight and robust\nguardrails can harm consumers and deny historically disadvantaged groups the\nfull measure of their civil rights and liberties, thereby further entrenching\ninequalities. The legislature must act to ensure that all uses of AI,\nespecially those that affect important life chances, are free from harmful\nbiases, protect our privacy, and work for the public good.\n\n(c) Safe innovation must remain a priority for the state. New York state is\nhome to thousands of technology start-ups, many of which experiment with new\napplications of AI and which have the potential to find new ways to employ\ntechnology at the service of New Yorkers. The goal of the legislature is to\nencourage safe innovation in the AI sector by providing clear guidance for AI\ndevelopment, testing, and validation both before a product is launched and\nthroughout the product's life cycle.\n\n(d) New York must establish that the burden of responsibility of proving that\nAI products do not cause harm to New Yorkers will be shouldered by the\ndevelopers and deployers of AI. While government and civil society must act to\naudit and enforce human rights laws around the use of AI, the companies\nemploying and profiting from the use of AI must lead in ensuring that their\nproducts are free from algorithmic discrimination.\n\n(e) Close collaboration and communication between New York state and industry\npartners is key to ensuring that innovation can occur with safeguards to\nprotect all New Yorkers. This legislation will ensure that lines of\ncommunication exist and that there is clear statutory authority to investigate\nand prosecute entities that break the law.\n\n(f) As new forms of AI are developed beyond what is currently technologically\nfeasible, the goal of the legislature is to use this section as a guiding\nlight for future regulations.\n\n(g) Lastly, it is in the interest of all New Yorkers that certain uses of AI\nthat infringe on fundamental rights, deepen structural inequality, or that\nresult in unequal access to services shall be banned.\n\nSection 3. The civil rights law is amended by adding a new article 8-A to read\nas follows:\n\n_ARTICLE 8-A_\n\n_PROTECTIONS REGARDING USE OF ARTIFICIAL INTELLIGENCE_\n\n_Section 85. Definitions._\n\n_86\\. Unlawful discriminatory practices._\n\n_86-a. Deployer and developer obligations._\n\n_86-b. Whistleblower protections._\n\n_87\\. Audits._\n\n_88\\. High-risk AI system reporting requirements._\n\n_89\\. Risk management policy and program._\n\n_89-a. Social scoring AI systems prohibited._\n\n_89-b. Enforcement._\n\n_Section 85. Definitions. The following terms shall have the following\nmeanings:_\n\n_1. \"Algorithmic discrimination\" means any condition in which the use of an AI\nsystem contributes to unjustified differential treatment or impacts,\ndisfavoring people based on their actual or perceived age, race, ethnicity,\ncreed, religion, color, national origin, citizenship or immigration status,\nsexual orientation, gender identity, gender expression, military status, sex,\ndisability, predisposing genetic characteristics, familial status, marital\nstatus, pregnancy, pregnancy outcomes, disability, height, weight,\nreproductive health care or autonomy, status as a victim of domestic violence\nor other classification protected under state or federal laws. Algorithmic\ndiscrimination shall not include:_\n\n_(a) a developer 's or deployer's testing of their own AI system to identify,\nmitigate, and prevent discriminatory bias;_\n\n_(b) expanding an applicant, customer, or participant pool to increase\ndiversity or redress historical discrimination; or_\n\n_(c) an act or omission by or on behalf of a private club or other\nestablishment that is not in fact open to the public, as set forth in Title II\nof the federal Civil Rights Act of 1964, 42 U.S.C. section 2000a(e), as\namended._\n\n_2. \"Artificial intelligence system\" or \"AI system\" means a machinebased\nsystem or combination of systems, that for explicit and implicit objectives,\ninfers, from the input it receives, how to generate outputs such as\npredictions, content, recommendations, or decisions that can influence\nphysical or virtual environments. Artificial intelligence shall not include\nany software used primarily for basic computerized processes, such as anti-\nmalware, anti-virus, auto-correct functions, calculators, databases, data\nstorage, electronic communications, firewall, internet domain registration,\ninternet website loading, networking, spam and robocall-filtering, spellcheck\ntools, spreadsheets, web caching, web hosting, or any tool that relates only\nto internal management affairs such as ordering office supplies or processing\npayments, and that do not materially affect the rights, liberties, benefits,\nsafety or welfare of any individual within the state._\n\n_3. \"Auditor\" shall refer to an independent entity including but not limited\nto an individual, non-profit, firm, corporation, partnership, cooperative, or\nassociation commissioned to perform an audit._\n\n_4. \"Consequential decision\" means a decision or judgment that has a material,\nlegal or similarly significant effect on an individual's life relating to the\nimpact of, access to, or the cost, terms, or availability of, any of the\nfollowing:_\n\n_(a) Employment, workers ' management, or self-employment, including, but not\nlimited to, all of the following:_\n\n_(i) Pay or promotion;_\n\n_(ii) Hiring or termination; and_\n\n_(iii) Automated task allocation._\n\n_(b) Education and vocational training, including, but not limited to, all of\nthe following:_\n\n_(i) Assessment or grading, including, but not limited to, detecting student\ncheating or plagiarism;_\n\n_(ii) Accreditation;_\n\n_(iii) Certification;_\n\n_(iv) Admissions; and_\n\n_(v) Financial aid or scholarships._\n\n_(c) Housing or lodging, including rental or short-term housing or lodging._\n\n_(d) Essential utilities, including electricity, heat, water, internet or\ntelecommunications access, or transportation._\n\n_(e) Family planning, including adoption services or reproductive services, as\nwell as assessments related to child protective services._\n\n_(f) Health care or health insurance, including mental health care, dental, or\nvision._\n\n_(g) Financial services, including a financial service provided by a mortgage\ncompany, mortgage broker, or creditor._\n\n_(h) Law enforcement activities, including the allocation of law enforcement\npersonnel or assets, the enforcement of laws, maintaining public order, or\nmanaging public safety._\n\n_(i) Government services._\n\n_(j) Legal services._\n\n_5. \"Deployer\" means a person, partnership, association or corporation that\nuses an AI system or commerce in the state of New York or provides an AI\nsystem for use by the general public in the state of New York. A developer may\nalso be considered a deployer if its actions satisfy this definition._\n\n_6. \"Deployer-employer\" means a deployer that is an employer._\n\n_7. \"Developer\" means a person, partnership, or corporation that designs,\ncodes, or produces an AI system, or creates a substantial change with respect\nto an AI system, whether for its own use in the state of New York or for use\nby a third party in the state of New York._\n\n_8. \"Developer-employer\" means a developer that is an employer._\n\n_9. \"Employee\" means an individual who performs services for and under the\ncontrol and direction of an employer for wages or other remuneration,\nincluding former employees, or natural persons employed as independent\ncontractors to carry out work in furtherance of an employer's business\nenterprise who are not themselves employers._\n\n_10. \"Employer\" means any person, firm, partnership, institution, corporation,\nor association that employs one or more employees._\n\n_11. \"End user\" means any individual or group of individuals that:_\n\n_(a) is the subject of a consequential decision made entirely by or with the\nassistance of an AI system; or_\n\n_(b) interacts, directly or indirectly, with the relevant AI system on behalf\nof an individual or group that is the subject of a consequential decision made\nentirely by or with the assistance of an AI system._\n\n_12. \"High-risk AI system\" means any AI system that, when deployed: (a) is a\nsubstantial factor in making a consequential decision; or (b) will have a\nmaterial impact on the statutory or constitutional rights, civil liberties,\nsafety, or welfare of an individual in the state._\n\n_13. \"Software stack\" means the group of individual software components that\nwork together to support the execution of an AI system._\n\n_14. \"Substantial change\" means any (a) deliberate modification to an AI\nsystem that would result in material inaccuracies in the reports created under\nsection eighty-eight of this article; or (b) unintentional and substantial\nchange in the data that the AI system uses as input data._\n\n_15. \"Substantial factor\" means a factor that assists in making a\nconsequential decision or is capable of altering the outcome of a\nconsequential decision. \"Substantial factor\" includes, but is not limited to,\nany use of an AI system to generate any content, decision, prediction, or\nrecommendation that is used as a basis, in whole or in part, to make a\nconsequential decision regarding an end user._\n\n_Section 86. Unlawful discriminatory practices. It shall be an unlawful\ndiscriminatory practice:_\n\n_1\\. for a developer or deployer to use, sell, or share a high-risk AI system\nor a product featuring a high-risk AI system that produces algorithmic\ndiscrimination; or_\n\n_2\\. for a developer to use, sell, or share a high-risk AI system or a product\nfeaturing a high-risk AI system that has not passed an independent audit, in\naccordance with section eighty-seven of this article, that has found that the\nproduct does not in fact produce algorithmic discrimination._\n\n_Section 86-a. Deployer and developer obligations. 1. (a) Any deployer that\nemploys a high-risk AI system for a consequential decision must inform the end\nuser at least five business days prior to the use of such system for the\nmaking of a consequential decision in clear, conspicuous, and consumer-\nfriendly terms, made available in each of the languages in which the company\noffers its end services, that AI systems will be used to make a decision or to\nassist in making a decision. The deployer must allow sufficient time and\nopportunity in a clear, conspicuous, and consumer-friendly manner for the\nconsumer to opt-out of the automated process and for the decision to be made\nby a human representative. A consumer may not be punished or face any other\nadverse action for opting out of a decision by an AI system and the deployer\nmust render a decision to the consumer within forty-five days._\n\n_(b) If a deployer employs a high-risk AI system for a consequential decision\nto determine whether to or on what terms to confer a benefit on an end user,\nthe deployer shall offer the end user the option to waive their right to\nadvance notice of five business days under this subdivision._\n\n_(c) If the end user clearly and affirmatively waives their right to five\nbusiness days ' notice, the deployer shall then inform the end user at least\none business day before the making of the consequential decision in clear,\nconspicuous, and consumer-friendly terms, made available in each of the\nlanguages in which the company offers its end services, that AI systems will\nbe used to make a decision or to assist in making a decision. The deployer\nmust allow sufficient time and opportunity in a clear, conspicuous, and\nconsumer-friendly manner for the consumer to opt-out of the automated process\nand for the decision to be made by a human representative. A consumer may not\nbe punished or face any other adverse action for opting out of a decision by\nan AI system and the deployer must render a decision to the consumer within\nforty-five days._\n\n_2\\. Any deployer that employs a high-risk AI system for a consequential\ndecision must inform the end user within five days in a clear, conspicuous,\nand consumer-friendly manner if a consequential decision has been made\nentirely by or with assistance of an automated system. The deployer must then\nprovide and explain a process for the end user to appeal the decision, which\nmust at minimum allow the end user to (a) formally contest the decision, (b)\nprovide information to support their position, and (c) obtain meaningful human\nreview of the decision. A deployer must respond to an end user 's appeal\nwithin forty-five days of receipt of the appeal. That period may be extended\nonce by forty-five additional days where reasonably necessary, taking into\naccount the complexity and number of appeals. The deployer must inform the end\nuser of any such extension within forty-five days of receipt of the appeal,\ntogether with the reasons for the delay._\n\n_3\\. The deployer or developer of a high-risk AI system is legally responsible\nfor quality and accuracy of all consequential decisions made, including any\nbias, algorithmic discrimination, and/or misinformation resulting from the\noperation of the AI system._\n\n_4\\. The rights and obligations under this section may not be waived by any\nperson, partnership, association or corporation._\n\n_Section 86-b. Whistleblower protections. 1. Developer-employers and/or\ndeployer-employers of high-risk AI systems shall not:_\n\n_(a) prevent an employee from disclosing information to the attorney general,\nincluding through terms and conditions of employment or seeking to enforce\nterms and conditions of employment, if the employee has reasonable cause to\nbelieve the information indicates a violation of this article; or_\n\n_(b) retaliate against an employee for disclosing information to the attorney\ngeneral pursuant to this section._\n\n_2\\. An employee harmed by a violation of this article may petition a court\nfor appropriate relief as provided in subdivision five of section seven\nhundred forty of the labor law._\n\n_3\\. Developer-employers and deployer-employers of high-risk AI systems shall\nprovide a clear notice to all employees working on such AI systems of their\nrights and responsibilities under this article, including the right of\nemployees of contractors and subcontractors to use the developer 's internal\nprocess for making protected disclosures pursuant to subdivision four of this\nsection. A developer-employer or deployer-employer is presumed to be in\ncompliance with the requirements of this subdivision if the developer-employer\nor deployer-employer does either of the following:_\n\n_(a) at all times post and display within all workplaces maintained by the\ndeveloper-employer or deployer-employer a notice to all employees of their\nrights and responsibilities under this article, ensure that all new employees\nreceive equivalent notice, and ensure that employees who work remotely\nperiodically receive an equivalent notice; or_\n\n_(b) no less frequently than once every year, provides written notice to all\nemployees of their rights and responsibilities under this article and ensures\nthat the notice is received and acknowledged by all of those employees._\n\n_4\\. Each developer-employer and deployer-employer shall provide a reasonable\ninternal process through which an employee may anonymously disclose\ninformation to the developer if the employee believes in good faith that the\ninformation indicates that the developer has violated any provision of this\narticle or any other law, or has made false or materially misleading\nstatements related to its safety and security protocol, or failed to disclose\nknown risks to employees, including, at a minimum, a monthly update to the\nperson who made the disclosure regarding the status of the developer 's\ninvestigation of the disclosure and the actions taken by the developer in\nresponse to the disclosure._\n\n_5\\. This section does not limit protections provided to employees under\nsection seven hundred forty of the labor law._\n\n_Section 87. Audits. 1. Prior to deployment of a high-risk AI system, six\nmonths after deployment, and at least every eighteen months thereafter for\neach calendar year a high-risk AI system is in use after the first post-\ndeployment audit, every developer or deployer of a high-risk AI system shall\ncause to be conducted at least one third-party audit in compliance with the\nprovisions of this section to ensure that the product does not produce\nalgorithmic discrimination and complies with the provisions of this article.\nRegardless of final findings, the deployer or developer shall deliver all\naudits conducted under this section to the attorney general._\n\n_2\\. A deployer or developer may hire more than one auditor to fulfill the\nrequirements of this section._\n\n_3\\. The audit shall include the following:_\n\n_(a) an analysis of data management policies including whether personal or\nsensitive data relating to a consumer is subject to data security protection\nstandards that comply with the requirements of section eight hundred ninety-\nnine-bb of the general business law;_\n\n_(b) an analysis of the system accuracy and reliability according to each\nspecified use case listed in the entity 's reporting document filed by the\ndeveloper or deployer under section eighty-eight of this article;_\n\n_(c) disparate impacts and a determination of whether the product produces\nalgorithmic discrimination in violation of this article by each intended and\nforeseeable identified use as identified by the deployer and developer;_\n\n_(d) analysis of how the technology complies with existing relevant federal,\nstate, and local privacy and data privacy laws; and_\n\n_(e) an evaluation of the developer 's or deployer's documented risk\nmanagement policy and program required under section eighty-nine of this\narticle for conformity with subdivision one of such section eighty-nine._\n\n_4\\. The attorney general may promulgate further rules as necessary to ensure\nthat audits under this section assess whether or not AI systems produce\nalgorithmic discrimination and otherwise comply with the provisions of this\narticle._\n\n_5\\. The independent auditor shall have complete and unredacted copies of all\nreports previously filed by the deployer or developer under section eighty-\neight of this article._\n\n_6\\. An audit conducted under this section shall be completed in its entirety\nwithout the assistance of an AI system._\n\n_7\\. (a) An auditor shall be an independent entity including but not limited\nto an individual, non-profit, firm, corporation, partnership, cooperative, or\nassociation._\n\n_(b) For the purposes of this article, no auditor may be commissioned by a\ndeveloper or deployer of an AI system if such entity has already been\ncommissioned to provide any auditing or non-auditing service, including but\nnot limited to financial auditing, cybersecurity auditing, or consulting\nservices of any type, to the commissioning company in the past twelve months._\n\n_(c) Fees paid to auditors may not be contingent on the result of the audit\nand the commissioning company shall not provide any incentives or bonuses for\na positive audit result._\n\n_8\\. The attorney general may promulgate further rules to ensure (a) the\nindependence of auditors under this section, and (b) that teams conducting\naudits incorporate feedback from communities that may foreseeably be the\nsubject of algorithmic discrimination with respect to the AI system being\naudited._\n\n_Section 88. High-risk AI system reporting requirements. 1. Every developer\nand deployer of a high-risk AI system shall comply with the reporting\nrequirements of this section. Regardless of final findings, reports shall be\nfiled with the attorney general prior to deployment of a highrisk AI system\nand then annually, or after each substantial change to the system, whichever\ncomes first._\n\n_2\\. Together with each report required to be filed under this section,\ndevelopers and deployers shall file with the attorney general a copy of the\nlast completed independent audit required by this article and a legal\nattestation that the high-risk AI system: (a) does not violate any provision\nof this article; or (b) may violate or does violate one or more provisions of\nthis article, that there is a plan of remediation to bring the high-risk AI\nsystem into compliance with this article, and a summary of such plan of\nremediation._\n\n_3\\. Developers of high-risk AI systems shall file with the attorney general a\nreport containing the following:_\n\n_(a) a description of the system including:_\n\n_(i) a description of the system 's software stack;_\n\n_(ii) the purpose of the system;_\n\n_(iii) the system 's use to end users; and_\n\n_(iv) reasonably foreseeable uses outside of the current or intended uses;_\n\n_(v) how the system should be used or not used;_\n\n_(b) the intended outputs of the system and whether the outputs can be or are\notherwise appropriate to be used for any purpose not previously articulated;_\n\n_(c) the methods for training of their models including:_\n\n_(i) any pre-processing steps taken to prepare datasets for the training of a\nmodel underlying a high-risk AI system;_\n\n_(ii) datasheets comprehensively describing the datasets upon which models\nwere trained and evaluated, how and why datasets were collected, how that\ntraining data will be used and maintained going forward through the\ndevelopment cycle; and_\n\n_(iii) steps taken to ensure compliance with privacy, data privacy, data\nsecurity, and copyright laws;_\n\n_(d) detailed use and data management policies;_\n\n_(e) any other information necessary to allow the deployer to understand the\noutputs and monitor the system for compliance with this article;_\n\n_(f) any other information necessary to allow the deployer to comply with the\nrequirements of subdivision four of this section; and_\n\n_(g) for any high-risk AI system that is a substantial factor in making a\nconsequential decision:_\n\n_(i) a detailed description of the proposed uses of the system, including what\nconsequential decisions the system will support;_\n\n_(ii) a detailed description of the system 's capabilities and any developer-\nimposed limitations, including capabilities outside of its intended use, when\nthe system should not be used, any safeguards or guardrails in place to\nprotect against unintended, inappropriate, or disallowed uses, and testing of\nany such safeguards or guardrails;_\n\n_(iii) an internal risk assessment including documentation and results of\ntesting conducted to identify all reasonably foreseeable risks related to\nalgorithmic discrimination, accuracy and reliability, privacy and autonomy,\nand safety and security, as well as actions taken to address those risks, and\nsubsequent testing to assess the efficacy of actions taken to address risks;\nand_\n\n_(iv) whether the system should be monitored, and if so, how such system\nshould be monitored._\n\n_4\\. Deployers of high-risk AI systems shall file with the attorney general a\nreport containing the following:_\n\n_(a) a description of the system including:_\n\n_(i) a description of the system 's software stack;_\n\n_(ii) the purpose of the system;_\n\n_(iii) the system 's use to end users; and_\n\n_(iv) reasonably foreseeable uses outside of the current or intended uses;_\n\n_(b) the intended outputs of the system and whether the outputs can be or are\notherwise appropriate to be used for any purpose not previously articulated;_\n\n_(c) assessment of the relative benefits and costs to the consumer given the\nsystem 's purpose, capabilities, and probable use cases;_\n\n_(d) whether the deployer collects revenue or plans to collect revenue from\nuse of the high-risk AI system, and if so, how it monetizes or plans to\nmonetize use of the system; and_\n\n_(e) for any high-risk AI system that is a substantial factor in making a\nconsequential decision:_\n\n_(i) a detailed description of the proposed uses of the system, including what\nconsequential decisions the system will support;_\n\n_(ii) whether the system is designed to make consequential decisions itself or\nwhether and how it supports consequential decisions;_\n\n_(iii) a detailed description of the system 's capabilities and any deployer-\nimposed limitations, including capabilities outside of its intended use, when\nthe system should not be used, any safeguards or guardrails in place to\nprotect against unintended, inappropriate, or disallowed uses, and testing of\nany such safeguards or guardrails;_\n\n_(iv) an assessment of the relative benefits and costs to the consumer given\nthe system 's purpose, capabilities, and probable use cases;_\n\n_(v) an internal risk assessment including documentation and results of\ntesting conducted to identify all reasonably foreseeable risks related to\nalgorithmic discrimination, accuracy and reliability, privacy and autonomy,\nand safety and security, as well as actions taken to address those risks, and\nsubsequent testing to assess the efficacy of actions taken to address risks;\nand_\n\n_(vi) whether the system should be monitored, and if so, how such system\nshould be monitored._\n\n_5\\. The attorney general shall:_\n\n_(a) promulgate rules for a process whereby developers and deployers may\nrequest redaction of portions of reports required under this section to ensure\nthat they are not required to disclose sensitive and protected information;\nand_\n\n_(b) maintain an online database that is accessible to the general public with\nreports, redacted in accordance with this subdivision, and audits required by\nthis article which shall be updated biannually._\n\n_6\\. For high-risk AI systems which are already in deployment at the time of\nthe effective date of this article, developers and deployers shall have\neighteen months from such effective date to complete and file the reports and\nindependent audit required by this article._\n\n_Section 89. Risk management policy and program. 1. Each developer or deployer\nof high-risk AI systems shall plan, document, and implement a risk management\npolicy and program to govern development or deployment, as applicable, of such\nhigh-risk AI system. The risk management policy and program shall specify and\nincorporate the principles, processes, and personnel that the deployer uses to\nidentify, document, and mitigate known or reasonably foreseeable risks of\nalgorithmic discrimination covered under subdivision one of section eighty-six\nof this article. The risk management policy and program shall be an iterative\nprocess planned, implemented, and regularly and systematically reviewed and\nupdated over the life cycle of a high-risk AI system, requiring regular,\nsystematic review and updates, including updates to documentation. A risk\nmanagement policy and program implemented and maintained pursuant to this\nsection shall be reasonable considering:_\n\n_(a) The guidance and standards set forth in version 1.0 of the \"Artificial\nIntelligence Risk Management Framework\" published by the National Institute of\nStandards and Technology in the United States department of commerce, or the\nlatest version of the \"Artificial Intelligence Risk Management Framework\"\npublished by the National Institute of Standards and Technology if, in the\nattorney general's discretion, the latest version of the \"Artificial\nIntelligence Risk Management Framework\" published by the National Institute of\nStandards and Technology in the United States department of commerce is at\nleast as stringent as version 1.0;_\n\n_(b) The size and complexity of the developer or deployer;_\n\n_(c) The nature, scope, and intended uses of the high-risk AI system developed\nor deployed; and_\n\n_(d) The sensitivity and volume of data processed in connection with the high-\nrisk AI system._\n\n_2\\. A risk management policy and program implemented pursuant to subdivision\none of this section may cover multiple high-risk AI systems developed by the\nsame developer or deployed by the same deployer if sufficient._\n\n_3\\. The attorney general may require a developer or a deployer to disclose\nthe risk management policy and program implemented pursuant to subdivision one\nof this section in a form and manner prescribed by the attorney general. The\nattorney general may evaluate the risk management policy and program to ensure\ncompliance with this section._\n\n_Section 89-a. Social scoring AI systems prohibited. No person, partnership,\nassociation or corporation shall develop, deploy, use, or sell an AI system\nwhich evaluates or classifies the trustworthiness of natural persons over a\ncertain period of time based on their social behavior or known or predicted\npersonal or personality characteristics, with the social score leading to\neither or both of the following:_\n\n_1\\. differential treatment of certain natural persons or whole groups thereof\nin social contexts which are unrelated to the contexts in which the data was\noriginally generated or collected; or_\n\n_2\\. differential treatment of certain natural persons or whole groups thereof\nthat is unjustified or disproportionate to their social behavior or its\ngravity._\n\n_Section 89-b. Enforcement. 1. Whenever there shall be a violation of any\nprovision of this article, an application may be made by the attorney general\nin the name of the people of the state of New York, to the supreme court\nhaving jurisdiction by a special proceeding to issue an injunction, and upon\nnotice to the respondent of not less than ten days, to enjoin and restrain the\ncontinuance of such violation; and if it shall appear to the satisfaction of\nthe court that the respondent has, in fact, violated this article, an\ninjunction may be issued by the court, enjoining and restraining any further\nviolations, without requiring proof that any person has, in fact, been injured\nor damaged thereby. In any such proceeding, the court may make allowances to\nthe attorney general as provided in paragraph six of subdivision (a) of\nsection eighty-three hundred three of the civil practice law and rules, and\ndirect restitution. Whenever the court shall determine that a violation of\nthis article has occurred, the court may impose a civil penalty of not more\nthan twenty thousand dollars for each violation._\n\n_2\\. There shall be a private right of action by plenary proceeding for any\nperson harmed by any violation of this article by any natural person or\nentity. The court shall award compensatory damages and legal fees to the\nprevailing party._\n\n_3\\. In evaluating any motion to dismiss a plenary proceeding commenced\npursuant to subdivision two of this section, the court shall presume the\nspecified AI system was created and/or operated in violation of a specified\nlaw or laws and that such violation caused the harm or harms alleged._\n\n_(a) A defendant can rebut presumptions made pursuant to this subdivision\nthrough clear and convincing evidence that the specified AI system did not\ncause the harm or harms alleged and/or did not violate the alleged law or\nlaws. An algorithmic audit can be considered as evidence in rebutting such\npresumptions, but the mere existence of such an audit, without additional\nevidence, shall not be considered clear and convincing evidence._\n\n_(b) Where such presumptions are not rebutted pursuant to this subdivision,\nthe action shall not be dismissed._\n\n_(c) Where such presumptions are rebutted pursuant to this subdivision, a\nmotion to dismiss an action shall be adjudicated without any consideration of\nthis section._\n\n_4\\. The supreme court in the state shall have jurisdiction over any action,\nclaim, or lawsuit to enforce the provisions of this article._\n\nSection 4. Section 296 of the executive law is amended by adding a new\nsubdivision 23 to read as follows:\n\n_23\\. It shall be an unlawful discriminatory practice under this section for a\ndeployer or a developer, as such terms are defined in section eighty-five of\nthe civil rights law, to engage in an unlawful discriminatory practice under\nsection eighty-six of the civil rights law._\n\nSection 5. This act shall take effect immediately.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "06/09/2025",
      "label": "Amended",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:NY2025000S1169&verid=NY2025000S1169_20250609_0_A&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 NY S 1169</td> <td><table><tr><td class=\"label\">Author:</td> <td>Gonzalez</td></tr> <tr><td class=\"label\">Version:</td> <td>Amended</td></tr> <tr><td class=\"label\">Version Date:</td> <td>06/09/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">\n    <b>STATE OF NEW YORK</b>\n   </p>\n   <p class=\"center\">1169--A </p>\n   <p class=\"center\">2025-2026 Regular Sessions </p>\n   <p class=\"center\">\n    <b>IN SENATE</b>\n   </p>\n   <p class=\"center\">January 8, 2025 </p>\n   <p class=\"indent\">Introduced by Sens. GONZALEZ, BAILEY, BRISPORT, BYNOE, CLEARE, FERNANDEZ, HINCHEY, JACKSON, LIU, MAY, PERSAUD, RIVERA, SALAZAR, SANDERS, WEBB -- read twice and ordered printed, and when printed to be committed to the Committee on Internet and Technology -- committee discharged, bill amended, ordered reprinted as amended and recommitted to said committee </p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"indent\">AN ACT to amend the civil rights law and the executive law, in relation to the use of artificial intelligence systems </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">\n     <b>The People of the State of New York, represented in Senate and Assembly, do enact as follows:</b>\n    </p>\n   </span>\n   <p class=\"indent\">Section 1. This act shall be known and may be cited as the &quot;New York artificial intelligence act (New York AI act)&quot;. </p>\n   <p class=\"indent\">Section 2. Legislative findings and intent. The legislature finds and declares the following: </p>\n   <p class=\"indent\">(a) A revolution in artificial intelligence (AI) has advanced to the point that comprehensive regulations must be enacted to protect New Yorkers. </p>\n   <p class=\"indent\">(b) Artificial intelligence is already an integral part of New Yorkers&#39; daily lives. In the private sector, AI is currently in use in areas such as education, health care, employment, insurance, credit scoring, public safety, retail, banking and financial services, media, and more with little transparency or oversight. A growing body of research shows that AI systems that are deployed without adequate testing, sufficient oversight and robust guardrails can harm consumers and deny historically disadvantaged groups the full measure of their civil rights and liberties, thereby further entrenching inequalities. The legislature must act to ensure that all uses of AI, especially those that affect important life chances, are free from harmful biases, protect our privacy, and work for the public good. </p>\n   <p class=\"indent\">(c) Safe innovation must remain a priority for the state. New York state is home to thousands of technology start-ups, many of which experiment with new applications of AI and which have the potential to find new ways to employ technology at the service of New Yorkers. The goal of the legislature is to encourage safe innovation in the AI sector by providing clear guidance for AI development, testing, and validation both before a product is launched and throughout the product&#39;s life cycle. </p>\n   <p class=\"indent\">(d) New York must establish that the burden of responsibility of proving that AI products do not cause harm to New Yorkers will be shouldered by the developers and deployers of AI. While government and civil society must act to audit and enforce human rights laws around the use of AI, the companies employing and profiting from the use of AI must lead in ensuring that their products are free from algorithmic discrimination. </p>\n   <p class=\"indent\">(e) Close collaboration and communication between New York state and industry partners is key to ensuring that innovation can occur with safeguards to protect all New Yorkers. This legislation will ensure that lines of communication exist and that there is clear statutory authority to investigate and prosecute entities that break the law. </p>\n   <p class=\"indent\">(f) As new forms of AI are developed beyond what is currently technologically feasible, the goal of the legislature is to use this section as a guiding light for future regulations. </p>\n   <p class=\"indent\">(g) Lastly, it is in the interest of all New Yorkers that certain uses of AI that infringe on fundamental rights, deepen structural inequality, or that result in unequal access to services shall be banned. </p>\n   <p class=\"indent\">Section 3. The civil rights law is amended by adding a new article 8-A to read as follows: </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">ARTICLE 8-A</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">PROTECTIONS REGARDING USE OF ARTIFICIAL INTELLIGENCE</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 85. Definitions.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">86. Unlawful discriminatory practices.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">86-a. Deployer and developer obligations.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">86-b. Whistleblower protections.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">87. Audits.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">88. High-risk AI system reporting requirements.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89. Risk management policy and program.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89-a. Social scoring AI systems prohibited.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89-b. Developer safe harbor.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89-c. Enforcement.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">89-d. Severability.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 85. Definitions. The following terms shall have the following meanings:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. &quot;Algorithmic discrimination&quot; means any condition in which the use of an AI system contributes to unjustified differential treatment or impacts, disfavoring people based on their actual or perceived age, race, ethnicity, creed, religion, color, national origin, citizenship or immigration status, sexual orientation, gender identity, gender expression, military status, sex, disability, predisposing genetic characteristics, familial status, marital status, pregnancy, pregnancy outcomes, disability, height, weight, reproductive health care or autonomy, status as a victim of domestic violence or other classification protected under state or federal laws. Algorithmic discrimination shall not include:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) a developer&#39;s or deployer&#39;s testing of their own AI system to identify, mitigate, and prevent discriminatory bias;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) expanding an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) an act or omission by or on behalf of a private club or other establishment that is not in fact open to the public, as set forth in Title II of the federal Civil Rights Act of 1964, 42 U.S.C. section 2000a(e), as amended.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. &quot;Artificial intelligence system&quot; or &quot;AI system&quot; means a machinebased system or combination of systems, that for explicit and implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Artificial intelligence system shall not include:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) any system that (i) is used by a business entity solely for internal purposes and (ii) is not used as a substantial factor in a consequential decision; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) any software used primarily for basic computerized processes, such as anti-malware, anti-virus, auto-correct functions, calculators, databases, data storage, electronic communications, firewall, internet domain registration, internet website loading, networking, spam and robocall-filtering, spellcheck tools, spreadsheets, web caching, web hosting, or any tool that relates only to internal management affairs such as ordering office supplies or processing payments, and that do not materially affect the rights, liberties, benefits, safety or welfare of any individual within the state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. &quot;Auditor&quot; shall refer to an independent entity including but not limited to an individual, non-profit, firm, corporation, partnership, cooperative, association, academic institution, or group affiliated with an academic institution, commissioned to perform an audit.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. &quot;Consequential decision&quot; means a decision or judgment that has a material, legal or similarly significant effect on an individual&#39;s access to, or the cost, terms, or availability of, any of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) Employment, workers&#39; management, or self-employment, including, but not limited to, all of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) Pay or promotion; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) Hiring or termination.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) Education and vocational training, including, but not limited to, all of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) Accreditation;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) Certification;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) Admissions; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) Financial aid or scholarships.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) Housing or lodging, including rental or short-term housing or lodging.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) Family planning, including adoption services or reproductive services, as well as assessments related to child protective services.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(e) Health care or health insurance, including mental health care, dental, or vision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(f) Financial services, including a financial service provided by a mortgage company, mortgage broker, or creditor.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(g) Law enforcement activities, including the allocation of law enforcement personnel or assets, the enforcement of laws, maintaining public order, or managing public safety.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(h) Legal services.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. &quot;Deployer&quot; means any person, partnership, association or corporation that offers or uses an AI system for commerce in the state of New York, or provides an AI system for use by the general public in the state of New York. A deployer shall not include any natural person using an AI system for personal use. A developer may also be considered a deployer if its actions satisfy this definition.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. &quot;Developer&quot; means a person, partnership, or corporation that designs, codes, or produces an AI system, or creates a substantial change with respect to an AI system, whether for its own use in the state of New York or for use by a third party in the state of New York. A deployer may also be considered a developer if its actions satisfy this definition.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. &quot;Employee&quot; means an individual who performs services for and under the control and direction of an employer for wages or other remuneration, including former employees, or natural persons employed as independent contractors to carry out work in furtherance of an employer&#39;s business enterprise who are not themselves employers.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. &quot;Employer&quot; means any person, firm, partnership, institution, corporation, or association that employs one or more employees.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. &quot;End user&quot; means any individual or group of individuals that:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) is the subject of a consequential decision made entirely by or with the assistance of an AI system; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) interacts, directly or indirectly, with the relevant AI system on behalf of an individual or group that is the subject of a consequential decision made entirely by or with the assistance of an AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">10. &quot;High-risk AI system&quot; means any AI system that, when deployed: (a) is a substantial factor in making a consequential decision; or (b) will have a material impact on the statutory or constitutional rights, civil liberties, safety, or welfare of an individual in the state.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">11. &quot;Risk management policy and program&quot; means the risk management policy and program created pursuant to section eighty-nine of this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">12. &quot;Substantial change&quot; means any new version, new release, or any other update to an AI system that results in significant changes to such AI system&#39;s appropriate use cases, key functionality, or expected outcomes.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">13. &quot;Substantial factor&quot; means a factor that is (a) material in making a consequential decision, or (b) is capable of altering the outcome of a consequential decision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 86. Unlawful discriminatory practices. It shall be an unlawful discriminatory practice for a developer or deployer to fail to comply with the duties under this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. A developer or deployer shall take reasonable care to prevent foreseeable risk of algorithmic discrimination that is a consequence of the use, sale, or sharing of a high-risk AI system or a product featuring a high-risk AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Any developer or deployer that uses, sells, or shares a high-risk AI system shall have completed an independent audit, pursuant to section eighty-seven of this article, confirming that the developer or deployer has taken reasonable care to prevent foreseeable risk of algorithmic discrimination with respect to such high-risk AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 86-a. Deployer and developer obligations. 1. (a) Any deployer that employs a high-risk AI system for a consequential decision shall comply with the following requirements; provided, however, that where there is an urgent necessity for a decision to be made to confer a benefit to the end user, including, but not limited to, social benefits, housing access, or dispensing of emergency funds, and compliance with this section would cause imminent detriment to the welfare of the end user, such obligation shall be considered waived; provided further, that nothing in this section shall be construed to waive a natural person&#39;s option to request human review of the decision:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) inform the end user at least five business days prior to the use of such system for the making of a consequential decision in clear, conspicuous, and consumer-friendly terms, made available in each of the languages in which the company offers its end services, that AI systems will be used to make a decision or to assist in making a decision; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) allow sufficient time and opportunity in a clear, conspicuous, and consumer-friendly manner for the consumer to opt-out of the automated consequential decision process and for the decision to be made by a human representative. A consumer may not be punished or face any other adverse action for opting out of a decision by an AI system and the deployer shall render a decision to the consumer within forty-five days.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) If a deployer employs a high-risk AI system for a consequential decision to determine whether to or on what terms to confer a benefit on an end user, the deployer shall offer the end user the option to waive their right to advance notice of five business days under this subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) If the end user clearly and affirmatively waives their right to five business days&#39; notice, the deployer shall then inform the end user as early as practicable before the making of the consequential decision in clear, conspicuous, and consumer-friendly terms, made available in each of the languages in which the company offers its end services, that AI systems will be used to make a decision or to assist in making a decision. The deployer shall allow sufficient time and opportunity in a clear, conspicuous, and consumer-friendly manner for the consumer to opt-out of the automated process and for the decision to be made by a human representative. A consumer may not be punished or face any other adverse action for opting out of a decision by an AI system and the deployer shall render a decision to the consumer within forty-five days.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) An end user shall be entitled to no more than one opt-out with respect to the same consequential decision within a six-month period.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. (a) Any deployer that employs a high-risk AI system for a consequential decision shall inform the end user within five days in a clear, conspicuous and consumer-friendly manner if a high-risk AI system has been used to make a consequential decision. The deployer shall then provide and explain a process for the end user to appeal the decision, which shall at minimum allow the end user to (i) formally contest the decision, (ii) provide information to support their position, and (iii) obtain meaningful human review of the decision. A deployer shall respond to an end user&#39;s appeal within forty-five days of receipt of the appeal. That period may be extended once by forty-five additional days where reasonably necessary, taking into account the complexity and number of appeals. The deployer shall inform the end user of any such extension within forty-five days of receipt of the appeal, together with the reasons for the delay.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) An end user shall be entitled to no more than one appeal with respect to the same consequential decision in a six-month period.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The deployer or developer of a high-risk AI system is legally responsible for quality and accuracy of all consequential decisions made, including any bias or algorithmic discrimination resulting from the operation of the AI system on their behalf.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. The rights and obligations under this section may not be waived by any person, partnership, association or corporation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. With respect to a single consequential decision, an end user may not exercise both its right to opt-out of a consequential decision under subdivision one of this section and its right to appeal a consequential decision under subdivision two of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 86-b. Whistleblower protections. 1. Developers and/or deployers of high-risk AI systems shall not:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) prevent any of their employees from disclosing information to the attorney general, including through terms and conditions of employment or seeking to enforce terms and conditions of employment, if the employee has reasonable cause to believe the information indicates a violation of this article; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) retaliate against an employee for disclosing information to the attorney general pursuant to this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. An employee harmed by a violation of this article may petition a court for appropriate relief as provided in subdivision five of section seven hundred forty of the labor law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Developers and deployers of high-risk AI systems shall provide a clear notice to all of their employees working on such AI systems of their rights and responsibilities under this article, including the right of employees of contractors and subcontractors to use the developer&#39;s internal process for making protected disclosures pursuant to subdivision four of this section. A developer or deployer is presumed to be in compliance with the requirements of this subdivision if the developer or deployer does either of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) at all times post and display within all workplaces maintained by the developer or deployer a notice to all employees of their rights and responsibilities under this article, ensure that all new employees receive equivalent notice, and ensure that employees who work remotely periodically receive an equivalent notice; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) no less frequently than once every year, provide written notice to all employees of their rights and responsibilities under this article and ensure that the notice is received and acknowledged by all of those employees.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Each developer and deployer shall provide a reasonable internal process through which an employee may anonymously disclose information to the developer or deployer if the employee believes in good faith that the information indicates that the developer or deployer has violated any provision of this article or any other law, or has made false or materially misleading statements related to its risk management policy and program, or failed to disclose known risks to employees, including, at a minimum, a monthly update to the person who made the disclosure regarding the status of the developer&#39;s or deployer&#39;s investigation of the disclosure and the actions taken by the developer or deployer in response to the disclosure.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. This section does not limit protections provided to employees under section seven hundred forty of the labor law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 87. Audits. 1. Developers of high-risk AI systems shall cause to be conducted third-party audits in accordance with this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) A developer of a high-risk AI system shall complete at least:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a first audit within six months after completion of development of the high-risk AI system and the initial offering of the high-risk AI system to a deployer for deployment or, if the developer is first deployer to deploy the high-risk AI system, after initial deployment; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) one audit every one year following the submission of the first audit.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) A developer audit under this section shall include:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) an evaluation and determination of whether the developer has taken reasonable care to prevent foreseeable risk of algorithmic discrimination with respect to such high-risk AI system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) an evaluation of the developer&#39;s documented risk management policy and program required under section eighty-nine of this article for conformity with subdivision one of such section eighty-nine.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Deployers of high-risk AI systems shall cause to be conducted third-party audits in accordance with this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) A deployer of a high-risk AI system shall complete at least:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a first audit within six months after initial deployment;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) a second audit within one year following the submission of the first audit; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) one audit every two years following the submission of the second audit.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) A deployer audit under this section shall include:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) an evaluation and determination of whether the deployer has taken reasonable care to prevent foreseeable risk of algorithmic discrimination with respect to such high-risk AI system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) an evaluation of system accuracy and reliability with respect to such high-risk AI system&#39;s deployer-intended and actual use cases; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) an evaluation of the deployer&#39;s documented risk management policy and program required under section eighty-nine of this article for conformity with subdivision one of such section eighty-nine.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. A deployer or developer may hire more than one auditor to fulfill the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. At the attorney general&#39;s discretion, the attorney general may:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) promulgate further rules as necessary to ensure that audits under this section assess whether or not AI systems produce algorithmic discrimination and otherwise comply with the provisions of this article; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) recommend an updated AI system auditing framework to the legislature, where such recommendations are based on a standard or framework (i) designed to evaluate the risks of AI systems, and (ii) that is nationally or internationally recognized and consensus-driven, including but not limited to a relevant framework or standard created by the International Standards Organization.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. The independent auditor shall have complete and unredacted copies of all reports previously filed by the deployer or developer under section eighty-eight of this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. An audit conducted under this section may be completed in part, but shall not be completed entirely, with the assistance of an AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) Acceptable auditor uses of an AI system include, but are not limited to:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) use of an audited high-risk AI system in a controlled environment without impacts on end users for system testing purposes; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) detecting patterns in the behavior of an audited AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) An auditor shall not:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) use a different high-risk AI system that is not the subject of an audit to complete an audit; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) use an AI system to draft an audit under this section without meaningful human review and oversight.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">7. (a) An auditor shall be an independent entity including but not limited to an individual, non-profit, firm, corporation, partnership, cooperative, or association.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) For the purposes of this article, no auditor may be commissioned by a developer or deployer of a high-risk AI system if such entity:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) has already been commissioned to provide any auditing or non-auditing service, including but not limited to financial auditing, cybersecurity auditing, or consulting services of any type, to the commissioning company in the past twelve months; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) is, will be, or plans to be engaged in the business of developing or deploying an AI system that can compete commercially with such developer&#39;s or deployer&#39;s high-risk AI system in the five years following an audit.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) Fees paid to auditors may not be contingent on the result of the audit and the commissioning company shall not provide any incentives or bonuses for a positive audit result.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">8. The attorney general may promulgate further rules to ensure (a) the independence of auditors under this section, and (b) that teams conducting audits incorporate feedback from communities that may foreseeably be the subject of algorithmic discrimination with respect to the AI system being audited.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">9. If a developer or deployer has an audit completed for the purpose of complying with another applicable federal, state, or local law or regulation, and the audit otherwise satisfies all other requirements of this section, such audit shall be deemed to satisfy the requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 88. High-risk AI system reporting requirements. 1. Every developer and deployer of a high-risk AI system shall comply with the reporting requirements of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. Together with each report required to be filed under this section, every developer and deployer shall file with the attorney general a copy of the last completed independent audit required by this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. Developers of high-risk AI systems shall complete and file with the attorney general reports in accordance with this subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) A developer of a high-risk AI system shall complete and file with the attorney general at least:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a first report within six months after completion of development of the high-risk AI system and the initial offering of the high-risk AI system to a deployer for deployment or, if the developer is first deployer to deploy the high-risk AI system, after initial deployment;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) one report annually following the submission of the first report; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) one report within six months of any substantial change to the high-risk AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) A developer report under this section shall include:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a description of the system including:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(A) the uses of the high-risk AI system that the developer intends; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(B) any explicitly unintended or disallowed uses of the high-risk AI system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) an overview of how the high-risk AI system was developed;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) an overview of the high-risk AI system&#39;s training data; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) any other information necessary to allow a deployer to:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(A) understand the outputs and monitor the system for compliance with this article; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(B) fulfill its duties under this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. Deployers of high-risk AI systems shall complete and file with the attorney general reports in accordance with this subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) A deployer of a high-risk AI system shall complete and file with the attorney general at least:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a first report within six months after initial deployment;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) a second report within one year following the completion and filing of the first report;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iii) one report every two years following the completion and filing of the second report; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(iv) one report within six months of any substantial change to the high-risk AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) A deployer report under this section shall include:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) a description of the system including:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(A) the deployer&#39;s actual, intended, or planned uses of the high-risk AI system with respect to consequential decisions; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(B) whether the deployer is using the high-risk AI system for any developer unintended or disallowed uses; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) an impact assessment including:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(A) whether the high-risk AI system poses a risk of algorithmic discrimination and the steps taken to address the risk of algorithmic discrimination;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(B) if the high-risk AI system is or will be monetized, how it is or is planned to be monetized; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(C) an evaluation of the costs and benefits to consumers and other end users.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) A deployer that is also a developer and is required to submit reports under subdivision three of this section may submit a single joint report provided it contains the information required in this subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">5. The attorney general shall:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) promulgate rules for a process whereby developers and deployers may request redaction of portions of reports required under this section to ensure that they are not required to disclose sensitive and protected information; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) maintain an online database that is accessible to the general public with reports, redacted in accordance with this subdivision, and audits required by this article, which database shall be updated biannually.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">6. For high-risk AI systems which are already in deployment at the time of the effective date of this article, developers and deployers shall have eighteen months from such effective date to complete and file the first report and associated independent audit required by this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) Each developer of a high-risk AI system shall thereafter file at least one report annually following the submission of the first report under this subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) Each deployer of a high-risk AI system shall thereafter file at least one report every two years following the submission of the first report under this subdivision.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89. Risk management policy and program. 1. Each developer or deployer of high-risk AI systems shall plan, document, and implement a risk management policy and program to govern development or deployment, as applicable, of such high-risk AI system. The risk management policy and program shall specify and incorporate the principles, processes, and personnel that the deployer uses to identify, document, and mitigate known or reasonably foreseeable risks of algorithmic discrimination covered under subdivision one of section eighty-six of this article. The risk management policy and program shall be an iterative process planned, implemented, and regularly and systematically reviewed and updated over the life cycle of a high-risk AI system, requiring regular, systematic review and updates, including updates to documentation. A risk management policy and program implemented and maintained pursuant to this section shall be reasonable considering:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) The guidance and standards set forth in:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(i) version 1.0 of the &quot;Artificial Intelligence Risk Management Framework&quot; published by the National Institute of Standards and Technology in the United States department of commerce, or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(ii) another substantially equivalent framework selected at the discretion of the attorney general, if such framework was designed to manage risks associated with AI systems, is nationally or internationally recognized and consensus-driven, and is at least as stringent as version 1.0 of the &quot;Artificial Intelligence Risk Management Framework&quot; published by the National Institute of Standards and Technology;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) The size and complexity of the developer or deployer;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) The nature, scope, and intended uses of the high-risk AI system developed or deployed; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) The sensitivity and volume of data processed in connection with the high-risk AI system.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. A risk management policy and program implemented pursuant to subdivision one of this section may cover multiple high-risk AI systems developed by the same developer or deployed by the same deployer if sufficient.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. The attorney general may require a developer or a deployer to disclose the risk management policy and program implemented pursuant to subdivision one of this section in a form and manner prescribed by the attorney general. The attorney general may evaluate the risk management policy and program to ensure compliance with this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89-a. Social scoring AI systems prohibited. No person, partnership, association or corporation shall develop, deploy, use, or sell an AI system which evaluates or classifies the trustworthiness of natural persons over a certain period of time based on their social behavior or known or predicted personal or personality characteristics, with the social score leading to any of the following:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. differential treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. differential treatment of certain natural persons or whole groups thereof that is unjustified or disproportionate to their social behavior or its gravity; or</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. the infringement of any right guaranteed under the United States constitution, the New York constitution, or state or federal law.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89-b. Developer safe harbor. A developer may be exempt from its duties and obligations under sections eighty-six, eighty-six-a, eightysix-b, eighty-seven, eighty-eight, and eighty-nine of this article if such developer:</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">1. receives a written and signed contractual agreement from each deployer authorized to use the artificial intelligence system developed by such developer, including the developer if they are also a deployer, that such artificial intelligence system will not be used as a high-risk AI system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. implements reasonable technical safeguards designed to prevent or detect high-risk AI system use cases or otherwise demonstrates reasonable steps taken to ensure that any unauthorized deployments of its AI systems are not being used as a high-risk AI system;</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. prominently displays on its website, in marketing materials, and in all licensing agreements offered to prospective deployers of its AI system that the AI system cannot be used as a high-risk AI system; and</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. maintains records of deployer agreements for a period of not less than five years.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89-c. Enforcement. 1. Whenever there shall be a violation of section eighty-six-a, eighty-six-b, eighty-seven, eighty-eight, eighty-nine, or eighty-nine-a of this article, an application may be made by the attorney general in the name of the people of the state of New York, to the supreme court having jurisdiction to issue an injunction, and upon notice to the respondent of not less than ten days, to enjoin and restrain the continuance of such violation; and if it shall appear to the satisfaction of the court that the respondent has, in fact, violated this article, an injunction may be issued by the court, enjoining and restraining any further violations, without requiring proof that any person has, in fact, been injured or damaged thereby. In any such proceeding, the court may make allowances to the attorney general as provided in paragraph six of subdivision (a) of section eighty-three hundred three of the civil practice law and rules, and direct restitution. Whenever the court shall determine that a violation of this article has occurred, the court may impose a civil penalty of not more than twenty thousand dollars for each violation.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">2. There shall be a private right of action by plenary proceeding for any person harmed by any violation of section eighty-six-a, eighty-six-b, eighty-seven, eighty-eight, eighty-nine, or eighty-nine-a of this article by any natural person or entity. The court shall award compensatory damages and legal fees to the prevailing party.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">3. In evaluating any motion to dismiss a plenary proceeding commenced pursuant to subdivision two of this section, the court shall presume the specified AI system was created and/or operated in violation of a specified law or laws and that such violation caused the harm or harms alleged.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(a) A defendant can rebut presumptions made pursuant to this subdivision through clear and convincing evidence that the specified AI system did not cause the harm or harms alleged and/or did not violate the alleged law or laws. An algorithmic audit can be considered as evidence in rebutting such presumptions, but the mere existence of such an audit, without additional evidence, shall not be considered clear and convincing evidence.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(b) With respect to a violation of section eighty-six-a, eighty-six-b, eighty-seven, eighty-eight, or eighty-nine of this article, a developer can rebut presumptions made pursuant to this subdivision through clear and convincing evidence that it has complied with the duties under section eighty-nine-b of this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(c) Where such presumptions are not rebutted pursuant to this subdivision, the action shall not be dismissed.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">(d) Where such presumptions are rebutted pursuant to this subdivision, a motion to dismiss an action shall be adjudicated without any consideration of this section.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">4. The supreme court in the state shall have jurisdiction over any action, claim, or lawsuit to enforce the provisions of this article.</u>\n   </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">Section 89-d. Severability. If any clause, sentence, paragraph, subdivision, section or part of this article shall be adjudged by any court of competent jurisdiction to be invalid, such judgment shall not affect, impair, or invalidate the remainder thereof, but shall be confined in its operation to the clause, sentence, paragraph, subdivision, section, or part thereof directly involved in the controversy in which such judgment shall have been made.</u>\n   </p>\n   <p class=\"indent\">Section 4. Section 296 of the executive law is amended by adding a new subdivision 23 to read as follows: </p>\n   <p class=\"indent\">\n    <u class=\"amendmentInsertedText\">23. It shall be an unlawful discriminatory practice under this section for a deployer or a developer, as such terms are defined in section eighty-five of the civil rights law, to engage in an unlawful discriminatory practice under section eighty-six of the civil rights law.</u>\n   </p>\n   <p class=\"indent\">Section 5. This act shall take effect one year after it shall have become a law; provided, however, that section 87 of article 8-A of the civil rights law as added by section three of this act shall take effect two years after it shall have become a law.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 NY S 1169 | | Author: | Gonzalez  \n---|---  \nVersion: | Amended  \nVersion Date: | 06/09/2025  \n  \n**STATE OF NEW YORK**\n\n1169--A\n\n2025-2026 Regular Sessions\n\n**IN SENATE**\n\nJanuary 8, 2025\n\nIntroduced by Sens. GONZALEZ, BAILEY, BRISPORT, BYNOE, CLEARE, FERNANDEZ,\nHINCHEY, JACKSON, LIU, MAY, PERSAUD, RIVERA, SALAZAR, SANDERS, WEBB -- read\ntwice and ordered printed, and when printed to be committed to the Committee\non Internet and Technology -- committee discharged, bill amended, ordered\nreprinted as amended and recommitted to said committee\n\nAN ACT to amend the civil rights law and the executive law, in relation to the\nuse of artificial intelligence systems\n\n**The People of the State of New York, represented in Senate and Assembly, do\nenact as follows:**\n\nSection 1. This act shall be known and may be cited as the \"New York\nartificial intelligence act (New York AI act)\".\n\nSection 2. Legislative findings and intent. The legislature finds and declares\nthe following:\n\n(a) A revolution in artificial intelligence (AI) has advanced to the point\nthat comprehensive regulations must be enacted to protect New Yorkers.\n\n(b) Artificial intelligence is already an integral part of New Yorkers' daily\nlives. In the private sector, AI is currently in use in areas such as\neducation, health care, employment, insurance, credit scoring, public safety,\nretail, banking and financial services, media, and more with little\ntransparency or oversight. A growing body of research shows that AI systems\nthat are deployed without adequate testing, sufficient oversight and robust\nguardrails can harm consumers and deny historically disadvantaged groups the\nfull measure of their civil rights and liberties, thereby further entrenching\ninequalities. The legislature must act to ensure that all uses of AI,\nespecially those that affect important life chances, are free from harmful\nbiases, protect our privacy, and work for the public good.\n\n(c) Safe innovation must remain a priority for the state. New York state is\nhome to thousands of technology start-ups, many of which experiment with new\napplications of AI and which have the potential to find new ways to employ\ntechnology at the service of New Yorkers. The goal of the legislature is to\nencourage safe innovation in the AI sector by providing clear guidance for AI\ndevelopment, testing, and validation both before a product is launched and\nthroughout the product's life cycle.\n\n(d) New York must establish that the burden of responsibility of proving that\nAI products do not cause harm to New Yorkers will be shouldered by the\ndevelopers and deployers of AI. While government and civil society must act to\naudit and enforce human rights laws around the use of AI, the companies\nemploying and profiting from the use of AI must lead in ensuring that their\nproducts are free from algorithmic discrimination.\n\n(e) Close collaboration and communication between New York state and industry\npartners is key to ensuring that innovation can occur with safeguards to\nprotect all New Yorkers. This legislation will ensure that lines of\ncommunication exist and that there is clear statutory authority to investigate\nand prosecute entities that break the law.\n\n(f) As new forms of AI are developed beyond what is currently technologically\nfeasible, the goal of the legislature is to use this section as a guiding\nlight for future regulations.\n\n(g) Lastly, it is in the interest of all New Yorkers that certain uses of AI\nthat infringe on fundamental rights, deepen structural inequality, or that\nresult in unequal access to services shall be banned.\n\nSection 3. The civil rights law is amended by adding a new article 8-A to read\nas follows:\n\n_ARTICLE 8-A_\n\n_PROTECTIONS REGARDING USE OF ARTIFICIAL INTELLIGENCE_\n\n_Section 85. Definitions._\n\n_86\\. Unlawful discriminatory practices._\n\n_86-a. Deployer and developer obligations._\n\n_86-b. Whistleblower protections._\n\n_87\\. Audits._\n\n_88\\. High-risk AI system reporting requirements._\n\n_89\\. Risk management policy and program._\n\n_89-a. Social scoring AI systems prohibited._\n\n_89-b. Developer safe harbor._\n\n_89-c. Enforcement._\n\n_89-d. Severability._\n\n_Section 85. Definitions. The following terms shall have the following\nmeanings:_\n\n_1. \"Algorithmic discrimination\" means any condition in which the use of an AI\nsystem contributes to unjustified differential treatment or impacts,\ndisfavoring people based on their actual or perceived age, race, ethnicity,\ncreed, religion, color, national origin, citizenship or immigration status,\nsexual orientation, gender identity, gender expression, military status, sex,\ndisability, predisposing genetic characteristics, familial status, marital\nstatus, pregnancy, pregnancy outcomes, disability, height, weight,\nreproductive health care or autonomy, status as a victim of domestic violence\nor other classification protected under state or federal laws. Algorithmic\ndiscrimination shall not include:_\n\n_(a) a developer 's or deployer's testing of their own AI system to identify,\nmitigate, and prevent discriminatory bias;_\n\n_(b) expanding an applicant, customer, or participant pool to increase\ndiversity or redress historical discrimination; or_\n\n_(c) an act or omission by or on behalf of a private club or other\nestablishment that is not in fact open to the public, as set forth in Title II\nof the federal Civil Rights Act of 1964, 42 U.S.C. section 2000a(e), as\namended._\n\n_2. \"Artificial intelligence system\" or \"AI system\" means a machinebased\nsystem or combination of systems, that for explicit and implicit objectives,\ninfers, from the input it receives, how to generate outputs such as\npredictions, content, recommendations, or decisions that can influence\nphysical or virtual environments. Artificial intelligence system shall not\ninclude:_\n\n_(a) any system that (i) is used by a business entity solely for internal\npurposes and (ii) is not used as a substantial factor in a consequential\ndecision; or_\n\n_(b) any software used primarily for basic computerized processes, such as\nanti-malware, anti-virus, auto-correct functions, calculators, databases, data\nstorage, electronic communications, firewall, internet domain registration,\ninternet website loading, networking, spam and robocall-filtering, spellcheck\ntools, spreadsheets, web caching, web hosting, or any tool that relates only\nto internal management affairs such as ordering office supplies or processing\npayments, and that do not materially affect the rights, liberties, benefits,\nsafety or welfare of any individual within the state._\n\n_3. \"Auditor\" shall refer to an independent entity including but not limited\nto an individual, non-profit, firm, corporation, partnership, cooperative,\nassociation, academic institution, or group affiliated with an academic\ninstitution, commissioned to perform an audit._\n\n_4. \"Consequential decision\" means a decision or judgment that has a material,\nlegal or similarly significant effect on an individual's access to, or the\ncost, terms, or availability of, any of the following:_\n\n_(a) Employment, workers ' management, or self-employment, including, but not\nlimited to, all of the following:_\n\n_(i) Pay or promotion; and_\n\n_(ii) Hiring or termination._\n\n_(b) Education and vocational training, including, but not limited to, all of\nthe following:_\n\n_(i) Accreditation;_\n\n_(ii) Certification;_\n\n_(iii) Admissions; and_\n\n_(iv) Financial aid or scholarships._\n\n_(c) Housing or lodging, including rental or short-term housing or lodging._\n\n_(d) Family planning, including adoption services or reproductive services, as\nwell as assessments related to child protective services._\n\n_(e) Health care or health insurance, including mental health care, dental, or\nvision._\n\n_(f) Financial services, including a financial service provided by a mortgage\ncompany, mortgage broker, or creditor._\n\n_(g) Law enforcement activities, including the allocation of law enforcement\npersonnel or assets, the enforcement of laws, maintaining public order, or\nmanaging public safety._\n\n_(h) Legal services._\n\n_5. \"Deployer\" means any person, partnership, association or corporation that\noffers or uses an AI system for commerce in the state of New York, or provides\nan AI system for use by the general public in the state of New York. A\ndeployer shall not include any natural person using an AI system for personal\nuse. A developer may also be considered a deployer if its actions satisfy this\ndefinition._\n\n_6. \"Developer\" means a person, partnership, or corporation that designs,\ncodes, or produces an AI system, or creates a substantial change with respect\nto an AI system, whether for its own use in the state of New York or for use\nby a third party in the state of New York. A deployer may also be considered a\ndeveloper if its actions satisfy this definition._\n\n_7. \"Employee\" means an individual who performs services for and under the\ncontrol and direction of an employer for wages or other remuneration,\nincluding former employees, or natural persons employed as independent\ncontractors to carry out work in furtherance of an employer's business\nenterprise who are not themselves employers._\n\n_8. \"Employer\" means any person, firm, partnership, institution, corporation,\nor association that employs one or more employees._\n\n_9. \"End user\" means any individual or group of individuals that:_\n\n_(a) is the subject of a consequential decision made entirely by or with the\nassistance of an AI system; or_\n\n_(b) interacts, directly or indirectly, with the relevant AI system on behalf\nof an individual or group that is the subject of a consequential decision made\nentirely by or with the assistance of an AI system._\n\n_10. \"High-risk AI system\" means any AI system that, when deployed: (a) is a\nsubstantial factor in making a consequential decision; or (b) will have a\nmaterial impact on the statutory or constitutional rights, civil liberties,\nsafety, or welfare of an individual in the state._\n\n_11. \"Risk management policy and program\" means the risk management policy and\nprogram created pursuant to section eighty-nine of this article._\n\n_12. \"Substantial change\" means any new version, new release, or any other\nupdate to an AI system that results in significant changes to such AI system's\nappropriate use cases, key functionality, or expected outcomes._\n\n_13. \"Substantial factor\" means a factor that is (a) material in making a\nconsequential decision, or (b) is capable of altering the outcome of a\nconsequential decision._\n\n_Section 86. Unlawful discriminatory practices. It shall be an unlawful\ndiscriminatory practice for a developer or deployer to fail to comply with the\nduties under this section._\n\n_1\\. A developer or deployer shall take reasonable care to prevent foreseeable\nrisk of algorithmic discrimination that is a consequence of the use, sale, or\nsharing of a high-risk AI system or a product featuring a high-risk AI\nsystem._\n\n_2\\. Any developer or deployer that uses, sells, or shares a high-risk AI\nsystem shall have completed an independent audit, pursuant to section eighty-\nseven of this article, confirming that the developer or deployer has taken\nreasonable care to prevent foreseeable risk of algorithmic discrimination with\nrespect to such high-risk AI system._\n\n_Section 86-a. Deployer and developer obligations. 1. (a) Any deployer that\nemploys a high-risk AI system for a consequential decision shall comply with\nthe following requirements; provided, however, that where there is an urgent\nnecessity for a decision to be made to confer a benefit to the end user,\nincluding, but not limited to, social benefits, housing access, or dispensing\nof emergency funds, and compliance with this section would cause imminent\ndetriment to the welfare of the end user, such obligation shall be considered\nwaived; provided further, that nothing in this section shall be construed to\nwaive a natural person 's option to request human review of the decision:_\n\n_(i) inform the end user at least five business days prior to the use of such\nsystem for the making of a consequential decision in clear, conspicuous, and\nconsumer-friendly terms, made available in each of the languages in which the\ncompany offers its end services, that AI systems will be used to make a\ndecision or to assist in making a decision; and_\n\n_(ii) allow sufficient time and opportunity in a clear, conspicuous, and\nconsumer-friendly manner for the consumer to opt-out of the automated\nconsequential decision process and for the decision to be made by a human\nrepresentative. A consumer may not be punished or face any other adverse\naction for opting out of a decision by an AI system and the deployer shall\nrender a decision to the consumer within forty-five days._\n\n_(b) If a deployer employs a high-risk AI system for a consequential decision\nto determine whether to or on what terms to confer a benefit on an end user,\nthe deployer shall offer the end user the option to waive their right to\nadvance notice of five business days under this subdivision._\n\n_(c) If the end user clearly and affirmatively waives their right to five\nbusiness days ' notice, the deployer shall then inform the end user as early\nas practicable before the making of the consequential decision in clear,\nconspicuous, and consumer-friendly terms, made available in each of the\nlanguages in which the company offers its end services, that AI systems will\nbe used to make a decision or to assist in making a decision. The deployer\nshall allow sufficient time and opportunity in a clear, conspicuous, and\nconsumer-friendly manner for the consumer to opt-out of the automated process\nand for the decision to be made by a human representative. A consumer may not\nbe punished or face any other adverse action for opting out of a decision by\nan AI system and the deployer shall render a decision to the consumer within\nforty-five days._\n\n_(d) An end user shall be entitled to no more than one opt-out with respect to\nthe same consequential decision within a six-month period._\n\n_2\\. (a) Any deployer that employs a high-risk AI system for a consequential\ndecision shall inform the end user within five days in a clear, conspicuous\nand consumer-friendly manner if a high-risk AI system has been used to make a\nconsequential decision. The deployer shall then provide and explain a process\nfor the end user to appeal the decision, which shall at minimum allow the end\nuser to (i) formally contest the decision, (ii) provide information to support\ntheir position, and (iii) obtain meaningful human review of the decision. A\ndeployer shall respond to an end user 's appeal within forty-five days of\nreceipt of the appeal. That period may be extended once by forty-five\nadditional days where reasonably necessary, taking into account the complexity\nand number of appeals. The deployer shall inform the end user of any such\nextension within forty-five days of receipt of the appeal, together with the\nreasons for the delay._\n\n_(b) An end user shall be entitled to no more than one appeal with respect to\nthe same consequential decision in a six-month period._\n\n_3\\. The deployer or developer of a high-risk AI system is legally responsible\nfor quality and accuracy of all consequential decisions made, including any\nbias or algorithmic discrimination resulting from the operation of the AI\nsystem on their behalf._\n\n_4\\. The rights and obligations under this section may not be waived by any\nperson, partnership, association or corporation._\n\n_5\\. With respect to a single consequential decision, an end user may not\nexercise both its right to opt-out of a consequential decision under\nsubdivision one of this section and its right to appeal a consequential\ndecision under subdivision two of this section._\n\n_Section 86-b. Whistleblower protections. 1. Developers and/or deployers of\nhigh-risk AI systems shall not:_\n\n_(a) prevent any of their employees from disclosing information to the\nattorney general, including through terms and conditions of employment or\nseeking to enforce terms and conditions of employment, if the employee has\nreasonable cause to believe the information indicates a violation of this\narticle; or_\n\n_(b) retaliate against an employee for disclosing information to the attorney\ngeneral pursuant to this section._\n\n_2\\. An employee harmed by a violation of this article may petition a court\nfor appropriate relief as provided in subdivision five of section seven\nhundred forty of the labor law._\n\n_3\\. Developers and deployers of high-risk AI systems shall provide a clear\nnotice to all of their employees working on such AI systems of their rights\nand responsibilities under this article, including the right of employees of\ncontractors and subcontractors to use the developer 's internal process for\nmaking protected disclosures pursuant to subdivision four of this section. A\ndeveloper or deployer is presumed to be in compliance with the requirements of\nthis subdivision if the developer or deployer does either of the following:_\n\n_(a) at all times post and display within all workplaces maintained by the\ndeveloper or deployer a notice to all employees of their rights and\nresponsibilities under this article, ensure that all new employees receive\nequivalent notice, and ensure that employees who work remotely periodically\nreceive an equivalent notice; or_\n\n_(b) no less frequently than once every year, provide written notice to all\nemployees of their rights and responsibilities under this article and ensure\nthat the notice is received and acknowledged by all of those employees._\n\n_4\\. Each developer and deployer shall provide a reasonable internal process\nthrough which an employee may anonymously disclose information to the\ndeveloper or deployer if the employee believes in good faith that the\ninformation indicates that the developer or deployer has violated any\nprovision of this article or any other law, or has made false or materially\nmisleading statements related to its risk management policy and program, or\nfailed to disclose known risks to employees, including, at a minimum, a\nmonthly update to the person who made the disclosure regarding the status of\nthe developer 's or deployer's investigation of the disclosure and the actions\ntaken by the developer or deployer in response to the disclosure._\n\n_5\\. This section does not limit protections provided to employees under\nsection seven hundred forty of the labor law._\n\n_Section 87. Audits. 1. Developers of high-risk AI systems shall cause to be\nconducted third-party audits in accordance with this section._\n\n_(a) A developer of a high-risk AI system shall complete at least:_\n\n_(i) a first audit within six months after completion of development of the\nhigh-risk AI system and the initial offering of the high-risk AI system to a\ndeployer for deployment or, if the developer is first deployer to deploy the\nhigh-risk AI system, after initial deployment; and_\n\n_(ii) one audit every one year following the submission of the first audit._\n\n_(b) A developer audit under this section shall include:_\n\n_(i) an evaluation and determination of whether the developer has taken\nreasonable care to prevent foreseeable risk of algorithmic discrimination with\nrespect to such high-risk AI system; and_\n\n_(ii) an evaluation of the developer 's documented risk management policy and\nprogram required under section eighty-nine of this article for conformity with\nsubdivision one of such section eighty-nine._\n\n_2\\. Deployers of high-risk AI systems shall cause to be conducted third-party\naudits in accordance with this section._\n\n_(a) A deployer of a high-risk AI system shall complete at least:_\n\n_(i) a first audit within six months after initial deployment;_\n\n_(ii) a second audit within one year following the submission of the first\naudit; and_\n\n_(iii) one audit every two years following the submission of the second\naudit._\n\n_(b) A deployer audit under this section shall include:_\n\n_(i) an evaluation and determination of whether the deployer has taken\nreasonable care to prevent foreseeable risk of algorithmic discrimination with\nrespect to such high-risk AI system;_\n\n_(ii) an evaluation of system accuracy and reliability with respect to such\nhigh-risk AI system 's deployer-intended and actual use cases; and_\n\n_(iii) an evaluation of the deployer 's documented risk management policy and\nprogram required under section eighty-nine of this article for conformity with\nsubdivision one of such section eighty-nine._\n\n_3\\. A deployer or developer may hire more than one auditor to fulfill the\nrequirements of this section._\n\n_4\\. At the attorney general 's discretion, the attorney general may:_\n\n_(a) promulgate further rules as necessary to ensure that audits under this\nsection assess whether or not AI systems produce algorithmic discrimination\nand otherwise comply with the provisions of this article; and_\n\n_(b) recommend an updated AI system auditing framework to the legislature,\nwhere such recommendations are based on a standard or framework (i) designed\nto evaluate the risks of AI systems, and (ii) that is nationally or\ninternationally recognized and consensus-driven, including but not limited to\na relevant framework or standard created by the International Standards\nOrganization._\n\n_5\\. The independent auditor shall have complete and unredacted copies of all\nreports previously filed by the deployer or developer under section eighty-\neight of this article._\n\n_6\\. An audit conducted under this section may be completed in part, but shall\nnot be completed entirely, with the assistance of an AI system._\n\n_(a) Acceptable auditor uses of an AI system include, but are not limited to:_\n\n_(i) use of an audited high-risk AI system in a controlled environment without\nimpacts on end users for system testing purposes; or_\n\n_(ii) detecting patterns in the behavior of an audited AI system._\n\n_(b) An auditor shall not:_\n\n_(i) use a different high-risk AI system that is not the subject of an audit\nto complete an audit; or_\n\n_(ii) use an AI system to draft an audit under this section without meaningful\nhuman review and oversight._\n\n_7\\. (a) An auditor shall be an independent entity including but not limited\nto an individual, non-profit, firm, corporation, partnership, cooperative, or\nassociation._\n\n_(b) For the purposes of this article, no auditor may be commissioned by a\ndeveloper or deployer of a high-risk AI system if such entity:_\n\n_(i) has already been commissioned to provide any auditing or non-auditing\nservice, including but not limited to financial auditing, cybersecurity\nauditing, or consulting services of any type, to the commissioning company in\nthe past twelve months; or_\n\n_(ii) is, will be, or plans to be engaged in the business of developing or\ndeploying an AI system that can compete commercially with such developer 's or\ndeployer's high-risk AI system in the five years following an audit._\n\n_(c) Fees paid to auditors may not be contingent on the result of the audit\nand the commissioning company shall not provide any incentives or bonuses for\na positive audit result._\n\n_8\\. The attorney general may promulgate further rules to ensure (a) the\nindependence of auditors under this section, and (b) that teams conducting\naudits incorporate feedback from communities that may foreseeably be the\nsubject of algorithmic discrimination with respect to the AI system being\naudited._\n\n_9\\. If a developer or deployer has an audit completed for the purpose of\ncomplying with another applicable federal, state, or local law or regulation,\nand the audit otherwise satisfies all other requirements of this section, such\naudit shall be deemed to satisfy the requirements of this section._\n\n_Section 88. High-risk AI system reporting requirements. 1. Every developer\nand deployer of a high-risk AI system shall comply with the reporting\nrequirements of this section._\n\n_2\\. Together with each report required to be filed under this section, every\ndeveloper and deployer shall file with the attorney general a copy of the last\ncompleted independent audit required by this article._\n\n_3\\. Developers of high-risk AI systems shall complete and file with the\nattorney general reports in accordance with this subdivision._\n\n_(a) A developer of a high-risk AI system shall complete and file with the\nattorney general at least:_\n\n_(i) a first report within six months after completion of development of the\nhigh-risk AI system and the initial offering of the high-risk AI system to a\ndeployer for deployment or, if the developer is first deployer to deploy the\nhigh-risk AI system, after initial deployment;_\n\n_(ii) one report annually following the submission of the first report; and_\n\n_(iii) one report within six months of any substantial change to the high-risk\nAI system._\n\n_(b) A developer report under this section shall include:_\n\n_(i) a description of the system including:_\n\n_(A) the uses of the high-risk AI system that the developer intends; and_\n\n_(B) any explicitly unintended or disallowed uses of the high-risk AI system;_\n\n_(ii) an overview of how the high-risk AI system was developed;_\n\n_(iii) an overview of the high-risk AI system 's training data; and_\n\n_(iv) any other information necessary to allow a deployer to:_\n\n_(A) understand the outputs and monitor the system for compliance with this\narticle; and_\n\n_(B) fulfill its duties under this article._\n\n_4\\. Deployers of high-risk AI systems shall complete and file with the\nattorney general reports in accordance with this subdivision._\n\n_(a) A deployer of a high-risk AI system shall complete and file with the\nattorney general at least:_\n\n_(i) a first report within six months after initial deployment;_\n\n_(ii) a second report within one year following the completion and filing of\nthe first report;_\n\n_(iii) one report every two years following the completion and filing of the\nsecond report; and_\n\n_(iv) one report within six months of any substantial change to the high-risk\nAI system._\n\n_(b) A deployer report under this section shall include:_\n\n_(i) a description of the system including:_\n\n_(A) the deployer 's actual, intended, or planned uses of the high-risk AI\nsystem with respect to consequential decisions; and_\n\n_(B) whether the deployer is using the high-risk AI system for any developer\nunintended or disallowed uses; and_\n\n_(ii) an impact assessment including:_\n\n_(A) whether the high-risk AI system poses a risk of algorithmic\ndiscrimination and the steps taken to address the risk of algorithmic\ndiscrimination;_\n\n_(B) if the high-risk AI system is or will be monetized, how it is or is\nplanned to be monetized; and_\n\n_(C) an evaluation of the costs and benefits to consumers and other end\nusers._\n\n_(c) A deployer that is also a developer and is required to submit reports\nunder subdivision three of this section may submit a single joint report\nprovided it contains the information required in this subdivision._\n\n_5\\. The attorney general shall:_\n\n_(a) promulgate rules for a process whereby developers and deployers may\nrequest redaction of portions of reports required under this section to ensure\nthat they are not required to disclose sensitive and protected information;\nand_\n\n_(b) maintain an online database that is accessible to the general public with\nreports, redacted in accordance with this subdivision, and audits required by\nthis article, which database shall be updated biannually._\n\n_6\\. For high-risk AI systems which are already in deployment at the time of\nthe effective date of this article, developers and deployers shall have\neighteen months from such effective date to complete and file the first report\nand associated independent audit required by this article._\n\n_(a) Each developer of a high-risk AI system shall thereafter file at least\none report annually following the submission of the first report under this\nsubdivision._\n\n_(b) Each deployer of a high-risk AI system shall thereafter file at least one\nreport every two years following the submission of the first report under this\nsubdivision._\n\n_Section 89. Risk management policy and program. 1. Each developer or deployer\nof high-risk AI systems shall plan, document, and implement a risk management\npolicy and program to govern development or deployment, as applicable, of such\nhigh-risk AI system. The risk management policy and program shall specify and\nincorporate the principles, processes, and personnel that the deployer uses to\nidentify, document, and mitigate known or reasonably foreseeable risks of\nalgorithmic discrimination covered under subdivision one of section eighty-six\nof this article. The risk management policy and program shall be an iterative\nprocess planned, implemented, and regularly and systematically reviewed and\nupdated over the life cycle of a high-risk AI system, requiring regular,\nsystematic review and updates, including updates to documentation. A risk\nmanagement policy and program implemented and maintained pursuant to this\nsection shall be reasonable considering:_\n\n_(a) The guidance and standards set forth in:_\n\n_(i) version 1.0 of the \"Artificial Intelligence Risk Management Framework\"\npublished by the National Institute of Standards and Technology in the United\nStates department of commerce, or_\n\n_(ii) another substantially equivalent framework selected at the discretion of\nthe attorney general, if such framework was designed to manage risks\nassociated with AI systems, is nationally or internationally recognized and\nconsensus-driven, and is at least as stringent as version 1.0 of the\n\"Artificial Intelligence Risk Management Framework\" published by the National\nInstitute of Standards and Technology;_\n\n_(b) The size and complexity of the developer or deployer;_\n\n_(c) The nature, scope, and intended uses of the high-risk AI system developed\nor deployed; and_\n\n_(d) The sensitivity and volume of data processed in connection with the high-\nrisk AI system._\n\n_2\\. A risk management policy and program implemented pursuant to subdivision\none of this section may cover multiple high-risk AI systems developed by the\nsame developer or deployed by the same deployer if sufficient._\n\n_3\\. The attorney general may require a developer or a deployer to disclose\nthe risk management policy and program implemented pursuant to subdivision one\nof this section in a form and manner prescribed by the attorney general. The\nattorney general may evaluate the risk management policy and program to ensure\ncompliance with this section._\n\n_Section 89-a. Social scoring AI systems prohibited. No person, partnership,\nassociation or corporation shall develop, deploy, use, or sell an AI system\nwhich evaluates or classifies the trustworthiness of natural persons over a\ncertain period of time based on their social behavior or known or predicted\npersonal or personality characteristics, with the social score leading to any\nof the following:_\n\n_1\\. differential treatment of certain natural persons or whole groups thereof\nin social contexts which are unrelated to the contexts in which the data was\noriginally generated or collected;_\n\n_2\\. differential treatment of certain natural persons or whole groups thereof\nthat is unjustified or disproportionate to their social behavior or its\ngravity; or_\n\n_3\\. the infringement of any right guaranteed under the United States\nconstitution, the New York constitution, or state or federal law._\n\n_Section 89-b. Developer safe harbor. A developer may be exempt from its\nduties and obligations under sections eighty-six, eighty-six-a, eightysix-b,\neighty-seven, eighty-eight, and eighty-nine of this article if such\ndeveloper:_\n\n_1\\. receives a written and signed contractual agreement from each deployer\nauthorized to use the artificial intelligence system developed by such\ndeveloper, including the developer if they are also a deployer, that such\nartificial intelligence system will not be used as a high-risk AI system;_\n\n_2\\. implements reasonable technical safeguards designed to prevent or detect\nhigh-risk AI system use cases or otherwise demonstrates reasonable steps taken\nto ensure that any unauthorized deployments of its AI systems are not being\nused as a high-risk AI system;_\n\n_3\\. prominently displays on its website, in marketing materials, and in all\nlicensing agreements offered to prospective deployers of its AI system that\nthe AI system cannot be used as a high-risk AI system; and_\n\n_4\\. maintains records of deployer agreements for a period of not less than\nfive years._\n\n_Section 89-c. Enforcement. 1. Whenever there shall be a violation of section\neighty-six-a, eighty-six-b, eighty-seven, eighty-eight, eighty-nine, or\neighty-nine-a of this article, an application may be made by the attorney\ngeneral in the name of the people of the state of New York, to the supreme\ncourt having jurisdiction to issue an injunction, and upon notice to the\nrespondent of not less than ten days, to enjoin and restrain the continuance\nof such violation; and if it shall appear to the satisfaction of the court\nthat the respondent has, in fact, violated this article, an injunction may be\nissued by the court, enjoining and restraining any further violations, without\nrequiring proof that any person has, in fact, been injured or damaged thereby.\nIn any such proceeding, the court may make allowances to the attorney general\nas provided in paragraph six of subdivision (a) of section eighty-three\nhundred three of the civil practice law and rules, and direct restitution.\nWhenever the court shall determine that a violation of this article has\noccurred, the court may impose a civil penalty of not more than twenty\nthousand dollars for each violation._\n\n_2\\. There shall be a private right of action by plenary proceeding for any\nperson harmed by any violation of section eighty-six-a, eighty-six-b, eighty-\nseven, eighty-eight, eighty-nine, or eighty-nine-a of this article by any\nnatural person or entity. The court shall award compensatory damages and legal\nfees to the prevailing party._\n\n_3\\. In evaluating any motion to dismiss a plenary proceeding commenced\npursuant to subdivision two of this section, the court shall presume the\nspecified AI system was created and/or operated in violation of a specified\nlaw or laws and that such violation caused the harm or harms alleged._\n\n_(a) A defendant can rebut presumptions made pursuant to this subdivision\nthrough clear and convincing evidence that the specified AI system did not\ncause the harm or harms alleged and/or did not violate the alleged law or\nlaws. An algorithmic audit can be considered as evidence in rebutting such\npresumptions, but the mere existence of such an audit, without additional\nevidence, shall not be considered clear and convincing evidence._\n\n_(b) With respect to a violation of section eighty-six-a, eighty-six-b,\neighty-seven, eighty-eight, or eighty-nine of this article, a developer can\nrebut presumptions made pursuant to this subdivision through clear and\nconvincing evidence that it has complied with the duties under section eighty-\nnine-b of this article._\n\n_(c) Where such presumptions are not rebutted pursuant to this subdivision,\nthe action shall not be dismissed._\n\n_(d) Where such presumptions are rebutted pursuant to this subdivision, a\nmotion to dismiss an action shall be adjudicated without any consideration of\nthis section._\n\n_4\\. The supreme court in the state shall have jurisdiction over any action,\nclaim, or lawsuit to enforce the provisions of this article._\n\n_Section 89-d. Severability. If any clause, sentence, paragraph, subdivision,\nsection or part of this article shall be adjudged by any court of competent\njurisdiction to be invalid, such judgment shall not affect, impair, or\ninvalidate the remainder thereof, but shall be confined in its operation to\nthe clause, sentence, paragraph, subdivision, section, or part thereof\ndirectly involved in the controversy in which such judgment shall have been\nmade._\n\nSection 4. Section 296 of the executive law is amended by adding a new\nsubdivision 23 to read as follows:\n\n_23\\. It shall be an unlawful discriminatory practice under this section for a\ndeployer or a developer, as such terms are defined in section eighty-five of\nthe civil rights law, to engage in an unlawful discriminatory practice under\nsection eighty-six of the civil rights law._\n\nSection 5. This act shall take effect one year after it shall have become a\nlaw; provided, however, that section 87 of article 8-A of the civil rights law\nas added by section three of this act shall take effect two years after it\nshall have become a law.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": true
    }
  ]
}