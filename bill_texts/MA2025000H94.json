{
  "bill_id": "MA2025000H94",
  "source_url": "http://custom.statenet.com/public/resources.cgi?id=ID:bill:MA2025000H94&cuiq=93d84396-c63b-526a-b152-38b7f79b4cfd&client_md=e4f6fea4-27b4-5d41-b7d3-766fe52569f0",
  "versions": [
    {
      "date": "02/27/2025",
      "label": "Introduced",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:MA2025000H94&verid=MA2025000H94_20250227_0_I&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 MA H 94</td> <td><table><tr><td class=\"label\">Author:</td> <td>Paulino</td></tr> <tr><td class=\"label\">Version:</td> <td>Introduced</td></tr> <tr><td class=\"label\">Version Date:</td> <td>02/27/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"left\">HOUSE No. 94</p>\n   <p class=\"center\">The Commonwealth of Massachusetts</p>\n   <p class=\"center\">In the One Hundred and Ninety-Fourth General Court</p>\n   <p class=\"center\">(2025-2026)</p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"left\">AN ACT TO ENSURE ACCOUNTABILITY AND TRANSPARENCY IN ARTIFICIAL INTELLIGENCE SYSTEMS.</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">Be it enacted by the Senate and House of Representatives in General Court assembled, and by the authority of the same, as follows:</p>\n   </span>\n   <p class=\"indent\">SECTION 1. Chapter 93M of the General Laws is hereby established as follows:</p>\n   <p class=\"indent\">CHAPTER 93M: Artificial Intelligence Accountability and Consumer Protection</p>\n   <p class=\"indent\">Section 1. Definitions</p>\n   <p class=\"indent\">For the purposes of this Chapter:</p>\n   <p class=\"indent\">(1) Algorithmic Discrimination: Differential treatment or impact resulting from an artificial intelligence system that disadvantages individuals or groups based on actual or perceived age, race, ethnicity, gender, disability, national origin, religion, genetic information, reproductive health, veteran status, or any protected classification under Massachusetts or federal law.</p>\n   <p class=\"indent\">(2) Artificial Intelligence System: Any machine-based system that processes inputs to generate outputs, including content, decisions, predictions, or recommendations, that influence physical or virtual environments.</p>\n   <p class=\"indent\">(3) High-Risk Artificial Intelligence System: AI systems that materially influence consequential decisions, including but not limited to:</p>\n   <p class=\"indent\">(a) Education opportunities;</p>\n   <p class=\"indent\">(b) Employment decisions;</p>\n   <p class=\"indent\">(c) Financial or lending services;</p>\n   <p class=\"indent\">(d) Housing access;</p>\n   <p class=\"indent\">(e) Healthcare services;</p>\n   <p class=\"indent\">(f) Insurance decisions;</p>\n   <p class=\"indent\">(g) Legal or government services.</p>\n   <p class=\"indent\">(4) Consequential Decision: A decision with significant legal, financial, or personal implications for a consumer, such as denying housing, employment, or healthcare. For clarity, material influence refers to decisions where AI systems determine or heavily weigh inputs that directly affect such outcomes.</p>\n   <p class=\"indent\">(5) Developer: An entity or individual developing, modifying, or making AI systems available in Massachusetts.</p>\n   <p class=\"indent\">(6) Deployer: An entity using AI systems to make decisions impacting consumers in Massachusetts.</p>\n   <p class=\"indent\">(7) Consumer: A resident of the Commonwealth of Massachusetts.</p>\n   <p class=\"indent\">Section 2. Developer Responsibilities</p>\n   <p class=\"indent\">(a) Duty of Care: Developers must use reasonable care to identify, mitigate, and disclose risks of algorithmic discrimination.</p>\n   <p class=\"indent\">(b) Documentation Requirements: Developers must provide deployers with: (1) A summary of intended and foreseeable uses of the AI system; (2) Known limitations and risks, including algorithmic discrimination; (3) Information on the datasets used for training, including measures taken to mitigate biases.</p>\n   <p class=\"indent\">(c) Disclosure of Risks: Developers must notify the Attorney General and deployers of any known or foreseeable risks of discrimination within 90 days of discovery.</p>\n   <p class=\"indent\">(d) Public Statement: Developers must publish a plain-language summary on their website, detailing: (1) Types of AI systems they develop; (2) Measures to mitigate algorithmic discrimination; (3) Contact information for inquiries.</p>\n   <p class=\"indent\">Section 3. Deployer Responsibilities</p>\n   <p class=\"indent\">(a) Risk Management Policy: Deployers of high-risk AI systems must implement and maintain a risk management program that:</p>\n   <p class=\"indent\">(1) Identifies and mitigates known or foreseeable risks of algorithmic discrimination; (2) Aligns with industry standards, such as the National Institute of Standards and Technology (NIST) AI Risk Management Framework.</p>\n   <p class=\"indent\">(b) Impact Assessments:</p>\n   <p class=\"indent\">(1) Deployers must complete an annual impact assessment for each high-risk AI system, including: (i) The purpose and intended use of the system; (ii) Data categories used and outputs generated; (iii) Potential risks of discrimination and mitigation measures.</p>\n   <p class=\"indent\">(2) Impact assessments must be updated after any substantial modification to the system. State-provided templates for these assessments will be made available to reduce compliance burdens.</p>\n   <p class=\"indent\">(c) Consumer Protections: Deployers must:</p>\n   <p class=\"indent\">(1) Notify consumers when an AI system materially influences a consequential decision; (2) Provide consumers with: (i) The purpose of the system; (ii) An explanation of how the system influenced the decision; (iii) A process to appeal or correct adverse decisions.</p>\n   <p class=\"indent\">(d) Transparency: Deployers must publicly disclose the types of high-risk AI systems in use and their risk mitigation strategies.</p>\n   <p class=\"indent\">SECTION 4. Corporate Disclosure Requirements</p>\n   <p class=\"indent\">(a) Disclosure of AI Use: Any corporation operating in Massachusetts that uses artificial intelligence systems or related tools to target specific consumer groups or influence behavior must disclose:</p>\n   <p class=\"indent\">(1) Purpose of AI Use: The methods, purposes, and contexts in which AI systems are used to identify or target specific classes of individuals; (2) Behavioral Influence: The specific ways in which AI tools are designed to influence consumer behavior; (3) Third-Party Partnerships: Details of any third-party entities involved in the design, deployment, or operation of AI systems used for targeting or behavioral influence. Proprietary information will be safeguarded and exempt from public disclosure under state confidentiality laws.</p>\n   <p class=\"indent\">(b) Public Disclosure Requirements: Corporations must make these disclosures: (1) Publicly available on their website in a manner that is easily accessible and comprehensible; (2) Included in terms and conditions provided to consumers prior to significant interaction with an AI system.</p>\n   <p class=\"indent\">(c) Consumer Notification: Consumers must be notified when: (1) They are being targeted or influenced by AI systems in a way that materially impacts their decisions; (2) Algorithms are used to determine pricing, eligibility, or access to services.</p>\n   <p class=\"indent\">SECTION 5. Exemptions</p>\n   <p class=\"indent\">The following entities and circumstances are exempt from certain provisions of this Chapter:</p>\n   <p class=\"indent\">(1) Small Businesses: Businesses with fewer than 50 employees that do not use proprietary data to train AI systems.</p>\n   <p class=\"indent\">(2) Low-Risk Systems: AI systems performing procedural tasks (e.g., spell-checkers, calculators) or those not influencing consequential decisions.</p>\n   <p class=\"indent\">(3) Federal Compliance: Entities subject to equivalent or stricter federal AI regulations, such as those governed by the Federal Trade Commission or Department of Health and Human Services.</p>\n   <p class=\"indent\">SECTION 6. Enforcement</p>\n   <p class=\"indent\">(a) Attorney General Authority: The Attorney General has exclusive authority to enforce this Chapter. Violations are deemed unfair or deceptive trade practices under Chapter 93A.</p>\n   <p class=\"indent\">(b) Affirmative Defense: A developer or deployer may defend against enforcement if:</p>\n   <p class=\"indent\">(1) They identify and remedy violations through testing, internal review, or consumer feedback; (2) They demonstrate compliance with recognized AI risk management standards.</p>\n   <p class=\"indent\">(c) No Private Right of Action: This Chapter does not create a private right of action for consumers.</p>\n   <p class=\"indent\">SECTION 7. Rulemaking Authority</p>\n   <p class=\"indent\">The Attorney General may issue rules to: (1) Define documentation and impact assessment requirements (2) Set standards for risk management programs and consumer notifications; (3) Designate recognized AI risk management frameworks.</p>\n   <p class=\"indent\">SECTION 8. Public Education Campaign</p>\n   <p class=\"indent\">The Attorney General, in collaboration with relevant state agencies, shall establish a public education campaign to inform residents of their rights under this Chapter and to increase awareness of the role of AI in decision-making processes.</p>\n   <p class=\"indent\">SECTION 9. Sections 1, 4, 5, and 8 shall take effect 180 days after passage of this Act.</p>\n   <p class=\"indent\">SECTION 10. Sections 2, 3, 6, and 7 shall take effect 1 year after passage of this Act.</p>\n   <p class=\"indent\">SECTION 11. The Amendment to Chapter 93A shall take effect 180 days after passage of this Act.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 MA H 94 | | Author: | Paulino  \n---|---  \nVersion: | Introduced  \nVersion Date: | 02/27/2025  \n  \nHOUSE No. 94\n\nThe Commonwealth of Massachusetts\n\nIn the One Hundred and Ninety-Fourth General Court\n\n(2025-2026)\n\nAN ACT TO ENSURE ACCOUNTABILITY AND TRANSPARENCY IN ARTIFICIAL INTELLIGENCE\nSYSTEMS.\n\nBe it enacted by the Senate and House of Representatives in General Court\nassembled, and by the authority of the same, as follows:\n\nSECTION 1. Chapter 93M of the General Laws is hereby established as follows:\n\nCHAPTER 93M: Artificial Intelligence Accountability and Consumer Protection\n\nSection 1. Definitions\n\nFor the purposes of this Chapter:\n\n(1) Algorithmic Discrimination: Differential treatment or impact resulting\nfrom an artificial intelligence system that disadvantages individuals or\ngroups based on actual or perceived age, race, ethnicity, gender, disability,\nnational origin, religion, genetic information, reproductive health, veteran\nstatus, or any protected classification under Massachusetts or federal law.\n\n(2) Artificial Intelligence System: Any machine-based system that processes\ninputs to generate outputs, including content, decisions, predictions, or\nrecommendations, that influence physical or virtual environments.\n\n(3) High-Risk Artificial Intelligence System: AI systems that materially\ninfluence consequential decisions, including but not limited to:\n\n(a) Education opportunities;\n\n(b) Employment decisions;\n\n(c) Financial or lending services;\n\n(d) Housing access;\n\n(e) Healthcare services;\n\n(f) Insurance decisions;\n\n(g) Legal or government services.\n\n(4) Consequential Decision: A decision with significant legal, financial, or\npersonal implications for a consumer, such as denying housing, employment, or\nhealthcare. For clarity, material influence refers to decisions where AI\nsystems determine or heavily weigh inputs that directly affect such outcomes.\n\n(5) Developer: An entity or individual developing, modifying, or making AI\nsystems available in Massachusetts.\n\n(6) Deployer: An entity using AI systems to make decisions impacting consumers\nin Massachusetts.\n\n(7) Consumer: A resident of the Commonwealth of Massachusetts.\n\nSection 2. Developer Responsibilities\n\n(a) Duty of Care: Developers must use reasonable care to identify, mitigate,\nand disclose risks of algorithmic discrimination.\n\n(b) Documentation Requirements: Developers must provide deployers with: (1) A\nsummary of intended and foreseeable uses of the AI system; (2) Known\nlimitations and risks, including algorithmic discrimination; (3) Information\non the datasets used for training, including measures taken to mitigate\nbiases.\n\n(c) Disclosure of Risks: Developers must notify the Attorney General and\ndeployers of any known or foreseeable risks of discrimination within 90 days\nof discovery.\n\n(d) Public Statement: Developers must publish a plain-language summary on\ntheir website, detailing: (1) Types of AI systems they develop; (2) Measures\nto mitigate algorithmic discrimination; (3) Contact information for inquiries.\n\nSection 3. Deployer Responsibilities\n\n(a) Risk Management Policy: Deployers of high-risk AI systems must implement\nand maintain a risk management program that:\n\n(1) Identifies and mitigates known or foreseeable risks of algorithmic\ndiscrimination; (2) Aligns with industry standards, such as the National\nInstitute of Standards and Technology (NIST) AI Risk Management Framework.\n\n(b) Impact Assessments:\n\n(1) Deployers must complete an annual impact assessment for each high-risk AI\nsystem, including: (i) The purpose and intended use of the system; (ii) Data\ncategories used and outputs generated; (iii) Potential risks of discrimination\nand mitigation measures.\n\n(2) Impact assessments must be updated after any substantial modification to\nthe system. State-provided templates for these assessments will be made\navailable to reduce compliance burdens.\n\n(c) Consumer Protections: Deployers must:\n\n(1) Notify consumers when an AI system materially influences a consequential\ndecision; (2) Provide consumers with: (i) The purpose of the system; (ii) An\nexplanation of how the system influenced the decision; (iii) A process to\nappeal or correct adverse decisions.\n\n(d) Transparency: Deployers must publicly disclose the types of high-risk AI\nsystems in use and their risk mitigation strategies.\n\nSECTION 4. Corporate Disclosure Requirements\n\n(a) Disclosure of AI Use: Any corporation operating in Massachusetts that uses\nartificial intelligence systems or related tools to target specific consumer\ngroups or influence behavior must disclose:\n\n(1) Purpose of AI Use: The methods, purposes, and contexts in which AI systems\nare used to identify or target specific classes of individuals; (2) Behavioral\nInfluence: The specific ways in which AI tools are designed to influence\nconsumer behavior; (3) Third-Party Partnerships: Details of any third-party\nentities involved in the design, deployment, or operation of AI systems used\nfor targeting or behavioral influence. Proprietary information will be\nsafeguarded and exempt from public disclosure under state confidentiality\nlaws.\n\n(b) Public Disclosure Requirements: Corporations must make these disclosures:\n(1) Publicly available on their website in a manner that is easily accessible\nand comprehensible; (2) Included in terms and conditions provided to consumers\nprior to significant interaction with an AI system.\n\n(c) Consumer Notification: Consumers must be notified when: (1) They are being\ntargeted or influenced by AI systems in a way that materially impacts their\ndecisions; (2) Algorithms are used to determine pricing, eligibility, or\naccess to services.\n\nSECTION 5. Exemptions\n\nThe following entities and circumstances are exempt from certain provisions of\nthis Chapter:\n\n(1) Small Businesses: Businesses with fewer than 50 employees that do not use\nproprietary data to train AI systems.\n\n(2) Low-Risk Systems: AI systems performing procedural tasks (e.g., spell-\ncheckers, calculators) or those not influencing consequential decisions.\n\n(3) Federal Compliance: Entities subject to equivalent or stricter federal AI\nregulations, such as those governed by the Federal Trade Commission or\nDepartment of Health and Human Services.\n\nSECTION 6. Enforcement\n\n(a) Attorney General Authority: The Attorney General has exclusive authority\nto enforce this Chapter. Violations are deemed unfair or deceptive trade\npractices under Chapter 93A.\n\n(b) Affirmative Defense: A developer or deployer may defend against\nenforcement if:\n\n(1) They identify and remedy violations through testing, internal review, or\nconsumer feedback; (2) They demonstrate compliance with recognized AI risk\nmanagement standards.\n\n(c) No Private Right of Action: This Chapter does not create a private right\nof action for consumers.\n\nSECTION 7. Rulemaking Authority\n\nThe Attorney General may issue rules to: (1) Define documentation and impact\nassessment requirements (2) Set standards for risk management programs and\nconsumer notifications; (3) Designate recognized AI risk management\nframeworks.\n\nSECTION 8. Public Education Campaign\n\nThe Attorney General, in collaboration with relevant state agencies, shall\nestablish a public education campaign to inform residents of their rights\nunder this Chapter and to increase awareness of the role of AI in decision-\nmaking processes.\n\nSECTION 9. Sections 1, 4, 5, and 8 shall take effect 180 days after passage of\nthis Act.\n\nSECTION 10. Sections 2, 3, 6, and 7 shall take effect 1 year after passage of\nthis Act.\n\nSECTION 11. The Amendment to Chapter 93A shall take effect 180 days after\npassage of this Act.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": true
    }
  ]
}