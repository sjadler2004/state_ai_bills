{
  "bill_id": "IA2025000HSB294",
  "source_url": "http://custom.statenet.com/public/resources.cgi?id=ID:bill:IA2025000HSB294&cuiq=93d84396-c63b-526a-b152-38b7f79b4cfd&client_md=e4f6fea4-27b4-5d41-b7d3-766fe52569f0",
  "versions": [
    {
      "date": "03/04/2025",
      "label": "Introduced",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:IA2025000HSB294&verid=IA2025000HSB294_20250304_0_I&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 IA HSB 294</td> <td><table><tr><td class=\"label\">Author:</td> <td>Economic Growth and Technology</td></tr> <tr><td class=\"label\">Version:</td> <td>Introduced</td></tr> <tr><td class=\"label\">Version Date:</td> <td>03/04/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"indent\">HSB 294 - Introduced</p>\n   <p class=\"right\">HOUSE FILE _____ </p>\n   <p class=\"right\">BY (PROPOSED COMMITTEE ON ECONOMIC GROWTH AND TECHNOLOGY BILL BY CHAIRPERSON SORENSEN)</p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"center\">A BILL FOR</p>\n   <p class=\"indent\">An Act relating to artificial intelligence, including the use of artificial intelligence to create materials related to elections and protections in interactions with artificial intelligence systems, and making penalties applicable.</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">BE IT ENACTED BY THE GENERAL ASSEMBLY OF THE STATE OF IOWA:</p>\n   </span>\n   <p class=\"indent\">DIVISION I</p>\n   <p class=\"indent\">MATERIALS RELATED TO ELECTIONS</p>\n   <p class=\"indent\">Section 1. Section 68A.405, Code 2025, is amended by adding the following new subsection:</p>\n   <p class=\"indent\">\n    <u>NEW SUBSECTION.</u> 5. a. Published material generated through the use of artificial intelligence and designed to expressly advocate the nomination, election, or defeat of a candidate for public office or the passage or defeat of a ballot issue must contain a disclosure on the published material that the published material was generated using artificial intelligence. The disclosure must include the words &ldquo;this material was generated using artificial intelligence&rdquo;.</p>\n   <p class=\"indent\">b. For purposes of this subsection, &ldquo;artificial intelligence&rdquo; means a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments.</p>\n   <p class=\"indent\">c. The board shall adopt rules for the implementation of this subsection.</p>\n   <p class=\"indent\">d. A disclosure made in compliance with this subsection does not preclude a private right of action arising out of the publication of published material generated through the use of artificial intelligence.</p>\n   <p class=\"indent\">DIVISION II</p>\n   <p class=\"indent\">PROTECTIONS IN INTERACTIONS WITH ARTIFICIAL INTELLIGENCE SYSTEMS</p>\n   <p class=\"indent\">Sec. 2. <u>NEW SECTION.</u> 554I.1 Definitions.</p>\n   <p class=\"indent\">As used in this chapter:</p>\n   <p class=\"indent\">1. a. &ldquo;Algorithmic discrimination&rdquo; means any use of an artificial intelligence system that results in unfavorable treatment due to an individual or group of individuals&rsquo; actual or perceived age, race, creed, color, sex, sexual orientation, national origin, religion, or disability.</p>\n   <p class=\"indent\">b. &ldquo;Algorithmic discrimination&rdquo; does not include the offer, license, or use of an artificial intelligence system for the sole purpose of performing any of the following:</p>\n   <p class=\"indent\">(1) Testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state or federal law.</p>\n   <p class=\"indent\">(2) Expanding an applicant, customer, or participant pool to increase diversity or redress historic discrimination.</p>\n   <p class=\"indent\">(3) Any act or omission by or on behalf of a private club or other establishment not in fact open to the public, as established in the federal Civil Rights Act of 1964, Pub. L. No. 88-352, as amended.</p>\n   <p class=\"indent\">2. &ldquo;Artificial intelligence system&rdquo; means any machine-based system that, for any explicit or implicit objective, infers from the inputs the system receives to generate outputs, including content, decisions, predictions, or recommendations, that can influence physical or virtual environments.</p>\n   <p class=\"indent\">3. &ldquo;Consequential decision&rdquo; means any decision that has a material legal or similarly significant effect on the provision or denial of any of the following to an individual:</p>\n   <p class=\"indent\">a. A pardon, parole, probation, or release.</p>\n   <p class=\"indent\">b. Enrollment in education or an educational opportunity.</p>\n   <p class=\"indent\">c. Employment.</p>\n   <p class=\"indent\">d. A financial or lending service.</p>\n   <p class=\"indent\">e. An essential government service.</p>\n   <p class=\"indent\">f. A health care service, as health care is defined in section 144B.1.</p>\n   <p class=\"indent\">g. Insurance.</p>\n   <p class=\"indent\">h. A legal service.</p>\n   <p class=\"indent\">4. &ldquo;Deployer&rdquo; means a person doing business in this state that uses a high-risk artificial intelligence system.</p>\n   <p class=\"indent\">5. &ldquo;Developer&rdquo; means a person doing business in this state that develops or intentionally and substantially modifies a high-risk artificial intelligence system.</p>\n   <p class=\"indent\">6. a. &ldquo;High-risk artificial intelligence system&rdquo; means any artificial intelligence system that makes, or is a factor that would likely alter the outcome of, a consequential decision.</p>\n   <p class=\"indent\">b. &ldquo;High-risk artificial intelligence system&rdquo; does not include any of the following:</p>\n   <p class=\"indent\">(1) An artificial intelligence system that is only intended to do any of the following:</p>\n   <p class=\"indent\">(a) Perform a narrow procedural task.</p>\n   <p class=\"indent\">(b) Improve the result of a previously completed human activity.</p>\n   <p class=\"indent\">(c) Perform a preparatory task relevant to a consequential decision.</p>\n   <p class=\"indent\">(d) Detect any decision-making pattern or any deviation from a preexisting decision-making pattern.</p>\n   <p class=\"indent\">(2) Antifraud technology.</p>\n   <p class=\"indent\">(3) Antimalware technology.</p>\n   <p class=\"indent\">(4) Antivirus technology.</p>\n   <p class=\"indent\">(5) Calculators.</p>\n   <p class=\"indent\">(6) Cybersecurity technology.</p>\n   <p class=\"indent\">(7) Databases.</p>\n   <p class=\"indent\">(8) Data storage technology.</p>\n   <p class=\"indent\">(9) Firewalls.</p>\n   <p class=\"indent\">(10) Internet domain registration technology.</p>\n   <p class=\"indent\">(11) Internet website loading technology.</p>\n   <p class=\"indent\">(12) Networking.</p>\n   <p class=\"indent\">(13) Search engine or similar technology.</p>\n   <p class=\"indent\">(14) Spam and robocall filtering technology.</p>\n   <p class=\"indent\">(15) Spellchecking.</p>\n   <p class=\"indent\">(16) Spreadsheets.</p>\n   <p class=\"indent\">(17) Web caching technology.</p>\n   <p class=\"indent\">(18) Web hosting technology.</p>\n   <p class=\"indent\">(19) Any technology that communicates in natural language for the purpose of providing users with information, making referrals or recommendations, answering questions, or generating other content, and that is subject to an accepted use policy that prohibits generating content that is unlawful.</p>\n   <p class=\"indent\">7. &ldquo;Intentional and substantial modification&rdquo; or &ldquo;intentionally and substantially modifies&rdquo; means a deliberate change made to an artificial intelligence system that materially increases the risk of algorithmic discrimination.</p>\n   <p class=\"indent\">8. &ldquo;Trade secret&rdquo; means the same as defined in section 550.2.</p>\n   <p class=\"indent\">Sec. 3. <u>NEW SECTION.</u> 554I.2 Algorithmic discrimination prohibited.</p>\n   <p class=\"indent\">1. a. A developer shall use reasonable care to protect individuals from any known or reasonably foreseeable risks of algorithmic discrimination arising from the intended and contracted uses of the developer&rsquo;s high-risk artificial intelligence system.</p>\n   <p class=\"indent\">b. There is a rebuttable presumption that a developer used reasonable care as required under this section if the developer complied with subsections 2 through 8 and any additional requirements established in rules adopted by the attorney general pursuant to section 554I.7.</p>\n   <p class=\"indent\">2. A developer shall make all of the following available to a deployer or other developer that uses or intends to use the developer&rsquo;s high-risk artificial intelligence system:</p>\n   <p class=\"indent\">a. A general statement describing the reasonably foreseeable uses and known harmful or inappropriate uses of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">b. Documentation for all of the following:</p>\n   <p class=\"indent\">(1) Summaries of the types of data used to train the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(2) The known or reasonably foreseeable limitations of the high-risk artificial intelligence system including but not limited to the known or reasonably foreseeable risks of algorithmic discrimination arising from the intended use of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(3) The purpose and intended uses of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(4) The intended outputs of the high-risk artificial intelligence system and how to understand the outputs.</p>\n   <p class=\"indent\">(5) The intended benefits of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(6) How the high-risk artificial intelligence system was evaluated for performance and mitigation of algorithmic discrimination before the high-risk artificial intelligence system was sold, leased, licensed, given, or otherwise made available to the deployer or other developer.</p>\n   <p class=\"indent\">(7) The actions or processes the deployer implemented to ensure the quality and consistency of training datasets, the measures used to examine the suitability of data sources, the measures used to evaluate possible biases in the training datasets and data sources, and the measures used to mitigate possible biases in training datasets and data sources.</p>\n   <p class=\"indent\">(8) The measures the developer took to mitigate any known or reasonably foreseeable risks of algorithmic discrimination that may arise from using the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(9) How the high-risk artificial intelligence system should be used, should not be used, and should be monitored when the high-risk artificial intelligence system is used to make, or is a factor that would likely alter the outcome of, a consequential decision.</p>\n   <p class=\"indent\">(10) Instructions that are reasonably necessary to assist a deployer in monitoring the performance of the high-risk artificial intelligence system for any risk of algorithmic discrimination.</p>\n   <p class=\"indent\">3. a. A developer that offers, sells, leases, licenses, gives, or otherwise makes a high-risk artificial intelligence system available to a deployer shall provide documentation and information to the deployer necessary for the deployer to complete an impact assessment under section 554I.3. Materials and data include but are not limited to model cards, dataset cards, and other impact assessments.</p>\n   <p class=\"indent\">b. This subsection shall not apply if the deployer is affiliated with the developer that is providing the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">4. A developer shall make a statement available in a manner that is clear and readily available on the developer&rsquo;s internet site or in a public use case inventory that includes a summary of all of the following:</p>\n   <p class=\"indent\">a. The types of high-risk artificial intelligence systems the developer has developed or intentionally and substantially modified and is currently making generally available to deployers.</p>\n   <p class=\"indent\">b. How the developer manages known or reasonably foreseeable risks of algorithmic discrimination arising from development or intentional and substantial modification of the types of high-risk artificial intelligence systems described in paragraph &ldquo;a&rdquo;.</p>\n   <p class=\"indent\">5. Each developer shall update the statement described in subsection 4 as necessary to ensure that the statement remains accurate, but no later than ninety days after the developer develops or intentionally and substantially modifies a high-risk artificial intelligence system.</p>\n   <p class=\"indent\">6. a. A developer shall disclose to the attorney general and to all known deployers or other developers of the high-risk artificial intelligence system any known or foreseeable risks of algorithmic discrimination arising from the intended uses of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">b. A disclosure under paragraph &ldquo;a&rdquo; shall be given to the attorney general no later than ninety days after the earliest of any of the following occurs:</p>\n   <p class=\"indent\">(1) The developer discovers through the developer&rsquo;s ongoing testing and analysis that the developer&rsquo;s high-risk artificial intelligence system has been used and has caused or is reasonably likely to have caused algorithmic discrimination.</p>\n   <p class=\"indent\">(2) The developer receives from a deployer a credible report that the high-risk artificial intelligence system has been used and has caused algorithmic discrimination.</p>\n   <p class=\"indent\">7. Subsections 2 through 6 do not require a developer to disclose a trade secret, information protected under state or federal law, or other confidential or proprietary information.</p>\n   <p class=\"indent\">8. a. The attorney general may require that a developer disclose to the attorney general any statement or documentation described in subsection 2 if the statement or documentation is relevant to an investigation conducted by the attorney general regarding a violation of this chapter.</p>\n   <p class=\"indent\">b. To the extent that a statement or documentation requested by the attorney general pursuant to paragraph &ldquo;a&rdquo; includes proprietary information or a trade secret, the statement or documentation is exempt from disclosure. The developer may designate the statement or documentation as including proprietary information or a trade secret.</p>\n   <p class=\"indent\">c. To the extent that a statement or documentation requested by the attorney general pursuant to paragraph &ldquo;a&rdquo; includes information subject to attorney-client privilege or work-product protection, the disclosure does not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">Sec. 4. <u>NEW SECTION.</u> 554I.3 Impact assessments.</p>\n   <p class=\"indent\">1. A deployer shall use reasonable care to protect individuals from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the attorney general, there is a rebuttable presumption that a deployer used reasonable care as required under this section if the deployer complied with subsections 2 through 7 and any additional requirements established in rules adopted by the attorney general pursuant to section 554I.7.</p>\n   <p class=\"indent\">2. a. A deployer shall implement a risk management policy and program to govern the deployer&rsquo;s use of high-risk artificial intelligence systems.</p>\n   <p class=\"indent\">b. The risk management policy and program shall specify and incorporate the principles, processes, and personnel that the deployer uses to identify, document, and mitigate known or reasonably foreseeable risks of algorithmic discrimination.</p>\n   <p class=\"indent\">c. The risk management program shall be an iterative process that is planned, implemented, and regularly and systematically reviewed and updated for the duration of the deployer&rsquo;s use of high-risk artificial intelligence systems.</p>\n   <p class=\"indent\">d. Each risk management policy and program shall consider all of the following:</p>\n   <p class=\"indent\">(1) The guidance and standards established by all of the following:</p>\n   <p class=\"indent\">(a) Standards established in the most recent version of the artificial intelligence risk management framework published by the national institute of standards and technology of the United States department of commerce.</p>\n   <p class=\"indent\">(b) Standard ISO/IEC 42001 of the international organization for standardization.</p>\n   <p class=\"indent\">(c) A nationally or internationally recognized risk management framework for artificial intelligence systems.</p>\n   <p class=\"indent\">(d) Standards designated by the attorney general.</p>\n   <p class=\"indent\">(2) The size and complexity of the deployer.</p>\n   <p class=\"indent\">(3) The nature and scope of the high-risk artificial intelligence system used by the deployer, including but not limited to the intended uses of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(4) The sensitivity and volume of data processed in connection with the high-risk artificial intelligence systems used by the deployer.</p>\n   <p class=\"indent\">3. A risk management policy and program implemented pursuant to this section may cover more than one high-risk artificial intelligence system.</p>\n   <p class=\"indent\">4. a. A deployer, or a third party contracted by the deployer to use a high-risk artificial intelligence system, shall complete an impact assessment for the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">b. The impact assessment shall be completed no later than ninety days after a high-risk artificial intelligence system or an intentional and substantial modification of a high-risk artificial intelligence system is available for use.</p>\n   <p class=\"indent\">c. Each impact assessment shall, at a minimum, include all of the following to the extent reasonably known by or available to the deployer:</p>\n   <p class=\"indent\">(1) A statement disclosing all of the following:</p>\n   <p class=\"indent\">(a) The purpose for using the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(b) The context for using the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(c) The benefits afforded by using the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(2) An analysis of whether the use of the high-risk artificial intelligence system poses any known or foreseeable risk of algorithmic discrimination and, if so, the nature of the algorithmic discrimination and the steps that have been taken to mitigate the risks.</p>\n   <p class=\"indent\">(3) A description of the categories of data the high-risk artificial intelligence system processes as inputs and the outputs the high-risk artificial intelligence system produces.</p>\n   <p class=\"indent\">(4) Any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(5) A description of any transparency measures taken concerning the high-risk artificial intelligence system, such as measures taken to disclose to individuals that the high-risk artificial intelligence system is in use.</p>\n   <p class=\"indent\">(6) A description of the post-use monitoring and user safeguards provided concerning the high-risk artificial intelligence system, such as the oversight process established by the deployer to address issues arising from using the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(7) If the impact statement is being made subsequent to an intentional and substantial modification to a high-risk artificial intelligence system, a statement disclosing the extent to which the high-risk artificial intelligence system was used in a manner that was consistent with or varied from the developer&rsquo;s intended uses of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">d. A single impact statement may address a comparable set of high-risk artificial intelligence systems used by a deployer.</p>\n   <p class=\"indent\">5. An impact assessment completed for the purpose of complying with another applicable law or regulation shall satisfy the requirements of this section if the impact assessment is reasonably similar in scope and effect to an impact assessment that would otherwise be completed pursuant to this section.</p>\n   <p class=\"indent\">6. A deployer shall maintain the most recently completed impact assessment for a high-risk artificial intelligence system as required under this section and relevant records supporting the impact assessment for a period of at least three years following the final use of the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">7. A deployer shall review, at least annually, the deployment of each high-risk artificial intelligence system used by the deployer to ensure that the high-risk artificial intelligence system is not causing algorithmic discrimination.</p>\n   <p class=\"indent\">Sec. 5. <u>NEW SECTION.</u> 554I.4 Disclosures of artificial intelligence system.</p>\n   <p class=\"indent\">1. A deployer shall disclose to each individual that interacts with the artificial intelligence system that the individual is interacting with an artificial intelligence system. Developers who make a high-risk artificial intelligence system available in this state shall cooperate with deployers to allow deployers to fulfill the requirements of this subsection.</p>\n   <p class=\"indent\">2. A disclosure under subsection 1 is not required in circumstances in which it would be obvious to a reasonable individual that the individual is interacting with an artificial intelligence system.</p>\n   <p class=\"indent\">Sec. 6. <u>NEW SECTION.</u> 554I.5 Exclusions.</p>\n   <p class=\"indent\">1. This chapter shall not be construed to restrict a developer&rsquo;s, deployer&rsquo;s, or other person&rsquo;s ability to do any of the following:</p>\n   <p class=\"indent\">a. Comply with federal, state, or municipal law.</p>\n   <p class=\"indent\">b. Comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by a federal, state, municipal, or other governmental authority.</p>\n   <p class=\"indent\">c. Cooperate with a law enforcement agency concerning conduct or activity that a developer, deployer, or other person reasonably and in good faith believes may violate federal, state, or municipal law.</p>\n   <p class=\"indent\">d. Investigate, establish, exercise, prepare for, or defend legal claims.</p>\n   <p class=\"indent\">e. Take immediate steps to protect an interest that is essential for the life or physical safety of an individual.</p>\n   <p class=\"indent\">f. Engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is conducted in accordance with 45 C.F.R. pt. 46, as amended, or other relevant requirements established by the federal food and drug administration.</p>\n   <p class=\"indent\">g. Conduct research, testing, and development activities regarding an artificial intelligence system, other than testing conducted under real-world conditions, before the artificial intelligence system is placed on the market, used, or otherwise put into service.</p>\n   <p class=\"indent\">h. Effectuate a product recall.</p>\n   <p class=\"indent\">i. Identify and repair technical errors that impair existing or intended functionality in a computer system or an artificial intelligence system.</p>\n   <p class=\"indent\">j. Assist another developer, deployer, or person with any of the requirements of this chapter.</p>\n   <p class=\"indent\">2. This chapter does not apply to a developer, deployer, or other person if the circumstances in which the high-risk artificial intelligence system was developed, used, or intentionally and substantially modified are described by any of the following:</p>\n   <p class=\"indent\">a. The high-risk artificial intelligence system has been approved, authorized, certified, cleared, developed, or granted by a federal agency and the deployer, developer, or other person is described by any of the following:</p>\n   <p class=\"indent\">(1) The person is acting under the authority of the federal agency that approved, authorized, certified, cleared, developed, or granted the high-risk artificial intelligence system.</p>\n   <p class=\"indent\">(2) The person is in compliance with standards established by a federal agency for use of high-risk artificial intelligence systems if the requirements imposed by those standards are substantially similar or more restrictive than the requirements of this chapter.</p>\n   <p class=\"indent\">b. The developer, deployer, or other person is conducting research to support an application for approval or certification from a federal agency.</p>\n   <p class=\"indent\">c. The developer, deployer, or other person is performing work under or in connection with a contract with the United States department of commerce, the United States department of defense, or the national aeronautics and space administration, unless the developer, deployer, or other person is performing work on a high-risk artificial intelligence system that is used to make, or is a factor that would likely alter the outcome of, a decision concerning employment or housing.</p>\n   <p class=\"indent\">d. The developer, deployer, or other person is a covered entity within the meaning of the Health Insurance Portability and Accountability Act of 1996, Pub. L. No. 104-191, as amended, and is providing health care recommendations that are generated by an artificial intelligence system, require a health care provider to take action to implement the recommendations, and are not likely to alter the outcome of a consequential decision.</p>\n   <p class=\"indent\">3. This chapter does not apply to any artificial intelligence system that is acquired by or for the federal government, a federal agency, or federal department unless the artificial intelligence system is a high-risk artificial intelligence system used to make, or is a factor that would likely alter the outcome of, a decision concerning employment or housing.</p>\n   <p class=\"indent\">4. A developer, deployer, or other person shall have the burden to prove an action qualifies for an exclusion under this section.</p>\n   <p class=\"indent\">Sec. 7. <u>NEW SECTION.</u> 554I.6 Enforcement.</p>\n   <p class=\"indent\">1. The attorney general shall have exclusive authority to enforce this chapter.</p>\n   <p class=\"indent\">2. Prior to initiating any action for a violation of this chapter, the attorney general shall issue a notice of violation to the developer, deployer, or other person who allegedly violated the chapter. The notice shall contain all of the following:</p>\n   <p class=\"indent\">a. A specific description of the alleged violation.</p>\n   <p class=\"indent\">b. The actions the developer, deployer, or other person must take to cure the violation.</p>\n   <p class=\"indent\">3. The attorney general may bring an action for the alleged violation if the alleged violation is not cured within ninety days of the date the developer, deployer, or other person received the notice of violation.</p>\n   <p class=\"indent\">4. A violation of the requirements established in this chapter is an unlawful practice under section 714.16.</p>\n   <p class=\"indent\">5. In any action commenced by the attorney general to enforce this chapter, it is an affirmative defense that the developer, deployer, or other person is described by all of the following:</p>\n   <p class=\"indent\">a. The developer, deployer, or other person discovered and cured a violation of this chapter as a result of any of the following:</p>\n   <p class=\"indent\">(1) Feedback that the developer, deployer, or other person encourages deployers or other users to provide to the developer, deployer, or other person.</p>\n   <p class=\"indent\">(2) Adversarial testing or red teaming, as those terms are defined or used by the national institute of standards and technology.</p>\n   <p class=\"indent\">(3) An internal review process.</p>\n   <p class=\"indent\">b. The developer, deployer, or other person is otherwise in compliance with any of the following:</p>\n   <p class=\"indent\">(1) The latest version of the artificial intelligence risk management framework as published by the national institute of standards and technology and standard ISO/IEC 42001 of the international organization for standardization.</p>\n   <p class=\"indent\">(2) A nationally or internationally recognized risk management framework for artificial intelligence systems, if the requirements imposed by those standards are substantially similar or more restrictive than the requirements of this chapter.</p>\n   <p class=\"indent\">(3) A risk management framework for artificial intelligence systems that the attorney general designated.</p>\n   <p class=\"indent\">6. A developer, deployer, or other person bears the burden of demonstrating to the attorney general that the requirements of subsection 5 have been satisfied.</p>\n   <p class=\"indent\">7. a. Prior to initiating any action under this chapter, the attorney general shall consult with the department of health and human services to determine whether any complaint has been filed that is founded on the same act or omission that constitutes a violation of this chapter.</p>\n   <p class=\"indent\">b. The attorney general shall not initiate any action to enforce the provisions of this chapter if a complaint has been filed with the department of health and human services relating to the act or omission that constitutes a violation of this chapter unless the complaint has been fully adjudicated or resolved.</p>\n   <p class=\"indent\">8. This chapter shall not preempt or otherwise affect any right, claim, remedy, presumption, or defense available at law or in equity. Any rebuttable presumption or affirmative defense established under this chapter shall apply only to an enforcement action brought by the attorney general pursuant to this chapter and shall not apply to any right, claim, remedy, presumption, or defense available at law or in equity.</p>\n   <p class=\"indent\">9. The attorney general shall post on its internet site how to properly file a complaint for a violation under this chapter.</p>\n   <p class=\"indent\">10. This section does not provide a basis for a private right of action for violations of this chapter or any other law.</p>\n   <p class=\"indent\">Sec. 8. <u>NEW SECTION.</u> 554I.7 Attorney general rulemaking authority.</p>\n   <p class=\"indent\">The attorney general shall adopt rules pursuant to chapter 17A to implement this chapter.</p>\n   <p class=\"indent\">Sec. 9. Section 714.16, subsection 2, Code 2025, is amended by adding the following new paragraph:</p>\n   <p class=\"indent\">\n    <u>NEW PARAGRAPH.</u> r. It is an unlawful practice for a person to violate any of the provisions of chapter 554I.</p>\n  </div>\n  <a name=\"digest_document_section\"></a><div class=\"digest\">\n   <p class=\"center\">EXPLANATION</p>\n   <p class=\"center\">The inclusion of this explanation does not constitute agreement withthe explanation&rsquo;s substance by the members of the general assembly.</p>\n   <p class=\"indent\">This bill relates to artificial intelligence (AI), including the use of artificial intelligence to create materials related to elections and protections in interactions with AI systems.</p>\n   <p class=\"indent\">DIVISION I &mdash; MATERIALS RELATED TO ELECTIONS. The bill requires that published material generated through the use of AI, defined in this division, and designed to expressly advocate the nomination, election, or defeat of a candidate for public office or the passage or defeat of a ballot issue to include a disclosure that the published material was generated using AI. The disclosure must include the words &ldquo;this material was generated using artificial intelligence&rdquo;. A disclosure made in compliance with the bill does not preclude a private right of action arising out of the publication of published material generated through the use of AI.</p>\n   <p class=\"indent\">By operation of law, a person who willfully violates the division of the bill is guilty of a serious misdemeanor. A serious misdemeanor is punishable by confinement for no more than one year and a fine of at least $430 but not more than $2,560.</p>\n   <p class=\"indent\">DIVISION II &mdash; PROTECTIONS IN INTERACTIONS WITH ARTIFICIAL INTELLIGENCE SYSTEMS. The bill defines &ldquo;algorithmic discrimination&rdquo; as any use of an AI system that results in unfavorable treatment due to an individual or group of individuals&rsquo; actual or perceived age, race, creed, color, sex, sexual orientation, national origin, religion, or disability. The bill lists several circumstances which do not constitute &ldquo;algorithmic discrimination&rdquo;.</p>\n   <p class=\"indent\">The bill defines &ldquo;artificial intelligence system&rdquo; as any machine-based system that, for any explicit or implicit objective, infers from the inputs the system receives to generate outputs, including but not limited to content, decisions, predictions, or recommendations, that can influence physical or virtual environments.</p>\n   <p class=\"indent\">The bill defines &ldquo;consequential decision&rdquo; as any decision that has a material legal or similarly significant effect on the provision or denial of a pardon, parole, probation, or release; an education enrollment or educational opportunity; employment; a financial or lending service; an essential government service; a health care service; insurance; or a legal service.</p>\n   <p class=\"indent\">The bill defines &ldquo;deployer&rdquo; as a person doing business in this state that uses a high-risk AI system.</p>\n   <p class=\"indent\">The bill defines &ldquo;developer&rdquo; as a person doing business in this state that develops or intentionally and substantially modifies a high-risk AI system.</p>\n   <p class=\"indent\">The bill defines &ldquo;high-risk artificial intelligence system&rdquo; as any AI system that makes, or is a factor that would likely alter the outcome of, a consequential decision. The bill lists several systems that do not constitute a &ldquo;high-risk artificial intelligence system&rdquo;.</p>\n   <p class=\"indent\">The bill defines &ldquo;intentional and substantial modification&rdquo; or &ldquo;intentionally and substantially modifies&rdquo; as a deliberate change made to an AI system that materially increases the risk of algorithmic discrimination.</p>\n   <p class=\"indent\">The bill defines &ldquo;trade secret&rdquo; as information, including but not limited to a formula, pattern, compilation, program, device, method, technique, or process that derives independent economic value, actual or potential, from not being generally known to, and not being readily ascertainable by proper means by a person able to obtain economic value from its disclosure or use; and is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</p>\n   <p class=\"indent\">The bill requires developers to use reasonable care to protect individuals from any known or reasonably foreseeable risks of algorithmic discrimination arising from the intended and contracted uses of the developer&rsquo;s high-risk AI system. The bill creates a rebuttable presumption that a developer used reasonable care if the developer complied with certain requirements detailed in the bill and any additional requirements established in rules adopted by the attorney general.</p>\n   <p class=\"indent\">The bill requires a developer to make certain statements, documentation, and summaries related to the high-risk AI system, as detailed in the bill, available to a deployer or other developer that uses or intends to use the developer&rsquo;s high-risk AI system.</p>\n   <p class=\"indent\">The bill requires a developer, if the developer offers, sells, leases, licenses, gives, or otherwise makes a high-risk AI system available to a deployer, to provide documentation and information to the deployer necessary for the deployer to complete an impact assessment. Documentation and information includes but is not limited to model cards, dataset cards, and other impact assessments. This requirement does not apply if the deployer is affiliated with the developer that is providing the high-risk AI system.</p>\n   <p class=\"indent\">The bill requires a developer to make a statement available in a manner that is clear and readily available on the developer&rsquo;s internet site or in a public use case inventory that includes a summary of the types of high-risk AI systems the developer has developed or intentionally and substantially modified and is currently making generally available to deployers, and how the developer manages known or reasonably foreseeable risks of algorithmic discrimination arising from development or intentional and substantial modification of the types of high-risk AI systems. The bill requires developers to update the statement as necessary to ensure that the statement remains accurate, but no later than 90 days after the developer develops or intentionally and substantially modifies an AI system.</p>\n   <p class=\"indent\">The bill requires developers to disclose to the attorney general and to all known deployers or other developers of the high-risk AI system any known or foreseeable risks of algorithmic discrimination arising from the intended uses of the high-risk AI system. The disclosure must be given to the attorney general no later than 90 days after the developer discovers through the developer&rsquo;s ongoing testing and analysis that the developer&rsquo;s high-risk AI system has been used and has caused or is reasonably likely to have caused algorithmic discrimination; or the developer receives from a deployer a credible report that the high-risk AI system has been used and has caused algorithmic discrimination, whichever is earlier. The bill does not require a developer to disclose a trade secret, information protected under state or federal law, or other confidential or proprietary information.</p>\n   <p class=\"indent\">The bill authorizes the attorney general to require a developer to disclose to the attorney general any statement or documentation the bill requires the developer to make available to deployers and other developers if the statement or documentation is relevant to an investigation conducted by the attorney general regarding a violation of the bill. The bill makes statements and documentation provided to the attorney general exempt from disclosure to the extent the statement or documentation includes any proprietary information or any trade secret. The developer may designate the statement or documentation as including proprietary information or a trade secret. Disclosures of statements and documents to the attorney general that include information subject to attorney-client privilege or work-product protection do not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">The bill requires deployers to use reasonable care to protect individuals from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought by the attorney general, there is a rebuttable presumption that a deployer used reasonable care if the deployer implemented a risk management policy and program as detailed in the bill and complied with the bill&rsquo;s requirements regarding impact statements.</p>\n   <p class=\"indent\">The bill requires deployers, or a third party contracted by a deployer to use a high-risk AI system, to complete an impact assessment for the high-risk AI system. Requirements for when the impact assessment must be completed and the contents of the impact assessment are detailed in the bill.</p>\n   <p class=\"indent\">The bill requires deployers to maintain the most recently completed impact assessment for a high-risk AI system and relevant records supporting the impact assessment for a period of at least three years following the final use of the high-risk AI system.</p>\n   <p class=\"indent\">The bill requires deployers to, at least annually, review the deployment of each high-risk AI system used by the deployer to ensure that the high-risk AI system is not causing algorithmic discrimination.</p>\n   <p class=\"indent\">The bill requires deployers to disclose to each individual that interacts with the AI system that the individual is interacting with an AI system. Developers who make a high-risk AI system available in this state must cooperate with deployers to allow deployers to fulfill the bill&rsquo;s requirements. A disclosure that an individual is interacting with an AI system is not required in circumstances in which it would be obvious to a reasonable individual that the individual is interacting with an AI system.</p>\n   <p class=\"indent\">The bill clarifies that its provisions do not restrict a developer&rsquo;s, deployer&rsquo;s, or other person&rsquo;s abilities to perform certain actions as detailed in the bill.</p>\n   <p class=\"indent\">The bill clarifies that its provisions do not apply to a developer, deployer, or other person that develops, uses, or intentionally and substantially modifies a high-risk AI system in certain circumstances detailed in the bill.</p>\n   <p class=\"indent\">The bill does not apply to an AI system that is acquired by or for the federal government, a federal agency, or a federal department unless the AI system is a high-risk AI system used to make, or is a factor that would likely alter the outcome of, a decision concerning employment or housing.</p>\n   <p class=\"indent\">The bill makes a developer, deployer, or other person responsible for proving an action qualifies for an exclusion from the bill&rsquo;s provisions.</p>\n   <p class=\"indent\">The bill provides the attorney general with exclusive authority to enforce the bill&rsquo;s provisions.</p>\n   <p class=\"indent\">The bill requires, prior to initiating any action for a violation of the bill&rsquo;s provisions, the attorney general to issue a notice of violation to the developer, deployer, or other person who allegedly violated the bill&rsquo;s provisions. The notice must contain a specific description of the alleged violation and the actions the developer, deployer, or other person must take to cure the violation. The bill authorizes the attorney general to bring an action for the alleged violation if the alleged violation is not cured within 90 days of the date the developer, deployer, or other person received the notice of violation.</p>\n   <p class=\"indent\">The bill details under what circumstances a developer, deployer, or other person has an affirmative defense to an alleged violation. The developer, deployer, or other person bears the burden of demonstrating to the attorney general that the person meets the requirements for an affirmative defense.</p>\n   <p class=\"indent\">The bill requires, prior to initiating any action under the bill&rsquo;s provisions, the attorney general to consult with the department of health and human services (HHS) to determine whether any complaint has been filed that is founded on the same act or omission that constitutes a violation of the bill&rsquo;s provisions. The attorney general is prohibited from initiating an action to enforce the bill&rsquo;s provisions if a complaint has been filed with HHS relating to the same act or omission that constitutes a violation of the bill&rsquo;s provisions unless the complaint has been fully adjudicated or resolved.</p>\n   <p class=\"indent\">The bill does not preempt or otherwise affect any right, claim, remedy, presumption, or defense available at law or in equity. Any rebuttable presumption or affirmative defense established under the bill&rsquo;s provisions applies only to an enforcement action brought by the attorney general and does not apply to any right, claim, remedy, presumption, or defense available at law or in equity.</p>\n   <p class=\"indent\">The bill requires the attorney general to post on its internet site how to properly file a complaint for a violation of the bill&rsquo;s provisions.</p>\n   <p class=\"indent\">The bill does not provide a basis for a private right of action for violations of the bill&rsquo;s provisions or any other law.</p>\n   <p class=\"indent\">The bill requires the attorney general to adopt rules to implement the bill&rsquo;s provisions.</p>\n   <p class=\"indent\">A violation of this division of the bill is an unlawful practice under Code section 714.16. Several types of remedies are available if a court finds that a person has committed an unlawful practice, including injunctive relief, disgorgement of moneys or property, and a civil penalty not to exceed $40,000 per violation.</p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 IA HSB 294 | | Author: | Economic Growth and Technology  \n---|---  \nVersion: | Introduced  \nVersion Date: | 03/04/2025  \n  \nHSB 294 - Introduced\n\nHOUSE FILE _____\n\nBY (PROPOSED COMMITTEE ON ECONOMIC GROWTH AND TECHNOLOGY BILL BY CHAIRPERSON\nSORENSEN)\n\nA BILL FOR\n\nAn Act relating to artificial intelligence, including the use of artificial\nintelligence to create materials related to elections and protections in\ninteractions with artificial intelligence systems, and making penalties\napplicable.\n\nBE IT ENACTED BY THE GENERAL ASSEMBLY OF THE STATE OF IOWA:\n\nDIVISION I\n\nMATERIALS RELATED TO ELECTIONS\n\nSection 1. Section 68A.405, Code 2025, is amended by adding the following new\nsubsection:\n\n_NEW SUBSECTION._ 5\\. a. Published material generated through the use of\nartificial intelligence and designed to expressly advocate the nomination,\nelection, or defeat of a candidate for public office or the passage or defeat\nof a ballot issue must contain a disclosure on the published material that the\npublished material was generated using artificial intelligence. The disclosure\nmust include the words \"this material was generated using artificial\nintelligence\".\n\nb. For purposes of this subsection, \"artificial intelligence\" means a machine-\nbased system that can, for a given set of human-defined objectives, make\npredictions, recommendations, or decisions influencing real or virtual\nenvironments.\n\nc. The board shall adopt rules for the implementation of this subsection.\n\nd. A disclosure made in compliance with this subsection does not preclude a\nprivate right of action arising out of the publication of published material\ngenerated through the use of artificial intelligence.\n\nDIVISION II\n\nPROTECTIONS IN INTERACTIONS WITH ARTIFICIAL INTELLIGENCE SYSTEMS\n\nSec. 2. _NEW SECTION._ 554I.1 Definitions.\n\nAs used in this chapter:\n\n1\\. a. \"Algorithmic discrimination\" means any use of an artificial\nintelligence system that results in unfavorable treatment due to an individual\nor group of individuals' actual or perceived age, race, creed, color, sex,\nsexual orientation, national origin, religion, or disability.\n\nb. \"Algorithmic discrimination\" does not include the offer, license, or use of\nan artificial intelligence system for the sole purpose of performing any of\nthe following:\n\n(1) Testing to identify, mitigate, or prevent discrimination or otherwise\nensure compliance with state or federal law.\n\n(2) Expanding an applicant, customer, or participant pool to increase\ndiversity or redress historic discrimination.\n\n(3) Any act or omission by or on behalf of a private club or other\nestablishment not in fact open to the public, as established in the federal\nCivil Rights Act of 1964, Pub. L. No. 88-352, as amended.\n\n2\\. \"Artificial intelligence system\" means any machine-based system that, for\nany explicit or implicit objective, infers from the inputs the system receives\nto generate outputs, including content, decisions, predictions, or\nrecommendations, that can influence physical or virtual environments.\n\n3\\. \"Consequential decision\" means any decision that has a material legal or\nsimilarly significant effect on the provision or denial of any of the\nfollowing to an individual:\n\na. A pardon, parole, probation, or release.\n\nb. Enrollment in education or an educational opportunity.\n\nc. Employment.\n\nd. A financial or lending service.\n\ne. An essential government service.\n\nf. A health care service, as health care is defined in section 144B.1.\n\ng. Insurance.\n\nh. A legal service.\n\n4\\. \"Deployer\" means a person doing business in this state that uses a high-\nrisk artificial intelligence system.\n\n5\\. \"Developer\" means a person doing business in this state that develops or\nintentionally and substantially modifies a high-risk artificial intelligence\nsystem.\n\n6\\. a. \"High-risk artificial intelligence system\" means any artificial\nintelligence system that makes, or is a factor that would likely alter the\noutcome of, a consequential decision.\n\nb. \"High-risk artificial intelligence system\" does not include any of the\nfollowing:\n\n(1) An artificial intelligence system that is only intended to do any of the\nfollowing:\n\n(a) Perform a narrow procedural task.\n\n(b) Improve the result of a previously completed human activity.\n\n(c) Perform a preparatory task relevant to a consequential decision.\n\n(d) Detect any decision-making pattern or any deviation from a preexisting\ndecision-making pattern.\n\n(2) Antifraud technology.\n\n(3) Antimalware technology.\n\n(4) Antivirus technology.\n\n(5) Calculators.\n\n(6) Cybersecurity technology.\n\n(7) Databases.\n\n(8) Data storage technology.\n\n(9) Firewalls.\n\n(10) Internet domain registration technology.\n\n(11) Internet website loading technology.\n\n(12) Networking.\n\n(13) Search engine or similar technology.\n\n(14) Spam and robocall filtering technology.\n\n(15) Spellchecking.\n\n(16) Spreadsheets.\n\n(17) Web caching technology.\n\n(18) Web hosting technology.\n\n(19) Any technology that communicates in natural language for the purpose of\nproviding users with information, making referrals or recommendations,\nanswering questions, or generating other content, and that is subject to an\naccepted use policy that prohibits generating content that is unlawful.\n\n7\\. \"Intentional and substantial modification\" or \"intentionally and\nsubstantially modifies\" means a deliberate change made to an artificial\nintelligence system that materially increases the risk of algorithmic\ndiscrimination.\n\n8\\. \"Trade secret\" means the same as defined in section 550.2.\n\nSec. 3. _NEW SECTION._ 554I.2 Algorithmic discrimination prohibited.\n\n1\\. a. A developer shall use reasonable care to protect individuals from any\nknown or reasonably foreseeable risks of algorithmic discrimination arising\nfrom the intended and contracted uses of the developer's high-risk artificial\nintelligence system.\n\nb. There is a rebuttable presumption that a developer used reasonable care as\nrequired under this section if the developer complied with subsections 2\nthrough 8 and any additional requirements established in rules adopted by the\nattorney general pursuant to section 554I.7.\n\n2\\. A developer shall make all of the following available to a deployer or\nother developer that uses or intends to use the developer's high-risk\nartificial intelligence system:\n\na. A general statement describing the reasonably foreseeable uses and known\nharmful or inappropriate uses of the high-risk artificial intelligence system.\n\nb. Documentation for all of the following:\n\n(1) Summaries of the types of data used to train the high-risk artificial\nintelligence system.\n\n(2) The known or reasonably foreseeable limitations of the high-risk\nartificial intelligence system including but not limited to the known or\nreasonably foreseeable risks of algorithmic discrimination arising from the\nintended use of the high-risk artificial intelligence system.\n\n(3) The purpose and intended uses of the high-risk artificial intelligence\nsystem.\n\n(4) The intended outputs of the high-risk artificial intelligence system and\nhow to understand the outputs.\n\n(5) The intended benefits of the high-risk artificial intelligence system.\n\n(6) How the high-risk artificial intelligence system was evaluated for\nperformance and mitigation of algorithmic discrimination before the high-risk\nartificial intelligence system was sold, leased, licensed, given, or otherwise\nmade available to the deployer or other developer.\n\n(7) The actions or processes the deployer implemented to ensure the quality\nand consistency of training datasets, the measures used to examine the\nsuitability of data sources, the measures used to evaluate possible biases in\nthe training datasets and data sources, and the measures used to mitigate\npossible biases in training datasets and data sources.\n\n(8) The measures the developer took to mitigate any known or reasonably\nforeseeable risks of algorithmic discrimination that may arise from using the\nhigh-risk artificial intelligence system.\n\n(9) How the high-risk artificial intelligence system should be used, should\nnot be used, and should be monitored when the high-risk artificial\nintelligence system is used to make, or is a factor that would likely alter\nthe outcome of, a consequential decision.\n\n(10) Instructions that are reasonably necessary to assist a deployer in\nmonitoring the performance of the high-risk artificial intelligence system for\nany risk of algorithmic discrimination.\n\n3\\. a. A developer that offers, sells, leases, licenses, gives, or otherwise\nmakes a high-risk artificial intelligence system available to a deployer shall\nprovide documentation and information to the deployer necessary for the\ndeployer to complete an impact assessment under section 554I.3. Materials and\ndata include but are not limited to model cards, dataset cards, and other\nimpact assessments.\n\nb. This subsection shall not apply if the deployer is affiliated with the\ndeveloper that is providing the high-risk artificial intelligence system.\n\n4\\. A developer shall make a statement available in a manner that is clear and\nreadily available on the developer's internet site or in a public use case\ninventory that includes a summary of all of the following:\n\na. The types of high-risk artificial intelligence systems the developer has\ndeveloped or intentionally and substantially modified and is currently making\ngenerally available to deployers.\n\nb. How the developer manages known or reasonably foreseeable risks of\nalgorithmic discrimination arising from development or intentional and\nsubstantial modification of the types of high-risk artificial intelligence\nsystems described in paragraph \"a\".\n\n5\\. Each developer shall update the statement described in subsection 4 as\nnecessary to ensure that the statement remains accurate, but no later than\nninety days after the developer develops or intentionally and substantially\nmodifies a high-risk artificial intelligence system.\n\n6\\. a. A developer shall disclose to the attorney general and to all known\ndeployers or other developers of the high-risk artificial intelligence system\nany known or foreseeable risks of algorithmic discrimination arising from the\nintended uses of the high-risk artificial intelligence system.\n\nb. A disclosure under paragraph \"a\" shall be given to the attorney general no\nlater than ninety days after the earliest of any of the following occurs:\n\n(1) The developer discovers through the developer's ongoing testing and\nanalysis that the developer's high-risk artificial intelligence system has\nbeen used and has caused or is reasonably likely to have caused algorithmic\ndiscrimination.\n\n(2) The developer receives from a deployer a credible report that the high-\nrisk artificial intelligence system has been used and has caused algorithmic\ndiscrimination.\n\n7\\. Subsections 2 through 6 do not require a developer to disclose a trade\nsecret, information protected under state or federal law, or other\nconfidential or proprietary information.\n\n8\\. a. The attorney general may require that a developer disclose to the\nattorney general any statement or documentation described in subsection 2 if\nthe statement or documentation is relevant to an investigation conducted by\nthe attorney general regarding a violation of this chapter.\n\nb. To the extent that a statement or documentation requested by the attorney\ngeneral pursuant to paragraph \"a\" includes proprietary information or a trade\nsecret, the statement or documentation is exempt from disclosure. The\ndeveloper may designate the statement or documentation as including\nproprietary information or a trade secret.\n\nc. To the extent that a statement or documentation requested by the attorney\ngeneral pursuant to paragraph \"a\" includes information subject to attorney-\nclient privilege or work-product protection, the disclosure does not\nconstitute a waiver of the privilege or protection.\n\nSec. 4. _NEW SECTION._ 554I.3 Impact assessments.\n\n1\\. A deployer shall use reasonable care to protect individuals from any known\nor reasonably foreseeable risks of algorithmic discrimination. In any\nenforcement action brought by the attorney general, there is a rebuttable\npresumption that a deployer used reasonable care as required under this\nsection if the deployer complied with subsections 2 through 7 and any\nadditional requirements established in rules adopted by the attorney general\npursuant to section 554I.7.\n\n2\\. a. A deployer shall implement a risk management policy and program to\ngovern the deployer's use of high-risk artificial intelligence systems.\n\nb. The risk management policy and program shall specify and incorporate the\nprinciples, processes, and personnel that the deployer uses to identify,\ndocument, and mitigate known or reasonably foreseeable risks of algorithmic\ndiscrimination.\n\nc. The risk management program shall be an iterative process that is planned,\nimplemented, and regularly and systematically reviewed and updated for the\nduration of the deployer's use of high-risk artificial intelligence systems.\n\nd. Each risk management policy and program shall consider all of the\nfollowing:\n\n(1) The guidance and standards established by all of the following:\n\n(a) Standards established in the most recent version of the artificial\nintelligence risk management framework published by the national institute of\nstandards and technology of the United States department of commerce.\n\n(b) Standard ISO/IEC 42001 of the international organization for\nstandardization.\n\n(c) A nationally or internationally recognized risk management framework for\nartificial intelligence systems.\n\n(d) Standards designated by the attorney general.\n\n(2) The size and complexity of the deployer.\n\n(3) The nature and scope of the high-risk artificial intelligence system used\nby the deployer, including but not limited to the intended uses of the high-\nrisk artificial intelligence system.\n\n(4) The sensitivity and volume of data processed in connection with the high-\nrisk artificial intelligence systems used by the deployer.\n\n3\\. A risk management policy and program implemented pursuant to this section\nmay cover more than one high-risk artificial intelligence system.\n\n4\\. a. A deployer, or a third party contracted by the deployer to use a high-\nrisk artificial intelligence system, shall complete an impact assessment for\nthe high-risk artificial intelligence system.\n\nb. The impact assessment shall be completed no later than ninety days after a\nhigh-risk artificial intelligence system or an intentional and substantial\nmodification of a high-risk artificial intelligence system is available for\nuse.\n\nc. Each impact assessment shall, at a minimum, include all of the following to\nthe extent reasonably known by or available to the deployer:\n\n(1) A statement disclosing all of the following:\n\n(a) The purpose for using the high-risk artificial intelligence system.\n\n(b) The context for using the high-risk artificial intelligence system.\n\n(c) The benefits afforded by using the high-risk artificial intelligence\nsystem.\n\n(2) An analysis of whether the use of the high-risk artificial intelligence\nsystem poses any known or foreseeable risk of algorithmic discrimination and,\nif so, the nature of the algorithmic discrimination and the steps that have\nbeen taken to mitigate the risks.\n\n(3) A description of the categories of data the high-risk artificial\nintelligence system processes as inputs and the outputs the high-risk\nartificial intelligence system produces.\n\n(4) Any metrics used to evaluate the performance and known limitations of the\nhigh-risk artificial intelligence system.\n\n(5) A description of any transparency measures taken concerning the high-risk\nartificial intelligence system, such as measures taken to disclose to\nindividuals that the high-risk artificial intelligence system is in use.\n\n(6) A description of the post-use monitoring and user safeguards provided\nconcerning the high-risk artificial intelligence system, such as the oversight\nprocess established by the deployer to address issues arising from using the\nhigh-risk artificial intelligence system.\n\n(7) If the impact statement is being made subsequent to an intentional and\nsubstantial modification to a high-risk artificial intelligence system, a\nstatement disclosing the extent to which the high-risk artificial intelligence\nsystem was used in a manner that was consistent with or varied from the\ndeveloper's intended uses of the high-risk artificial intelligence system.\n\nd. A single impact statement may address a comparable set of high-risk\nartificial intelligence systems used by a deployer.\n\n5\\. An impact assessment completed for the purpose of complying with another\napplicable law or regulation shall satisfy the requirements of this section if\nthe impact assessment is reasonably similar in scope and effect to an impact\nassessment that would otherwise be completed pursuant to this section.\n\n6\\. A deployer shall maintain the most recently completed impact assessment\nfor a high-risk artificial intelligence system as required under this section\nand relevant records supporting the impact assessment for a period of at least\nthree years following the final use of the high-risk artificial intelligence\nsystem.\n\n7\\. A deployer shall review, at least annually, the deployment of each high-\nrisk artificial intelligence system used by the deployer to ensure that the\nhigh-risk artificial intelligence system is not causing algorithmic\ndiscrimination.\n\nSec. 5. _NEW SECTION._ 554I.4 Disclosures of artificial intelligence system.\n\n1\\. A deployer shall disclose to each individual that interacts with the\nartificial intelligence system that the individual is interacting with an\nartificial intelligence system. Developers who make a high-risk artificial\nintelligence system available in this state shall cooperate with deployers to\nallow deployers to fulfill the requirements of this subsection.\n\n2\\. A disclosure under subsection 1 is not required in circumstances in which\nit would be obvious to a reasonable individual that the individual is\ninteracting with an artificial intelligence system.\n\nSec. 6. _NEW SECTION._ 554I.5 Exclusions.\n\n1\\. This chapter shall not be construed to restrict a developer's, deployer's,\nor other person's ability to do any of the following:\n\na. Comply with federal, state, or municipal law.\n\nb. Comply with a civil, criminal, or regulatory inquiry, investigation,\nsubpoena, or summons by a federal, state, municipal, or other governmental\nauthority.\n\nc. Cooperate with a law enforcement agency concerning conduct or activity that\na developer, deployer, or other person reasonably and in good faith believes\nmay violate federal, state, or municipal law.\n\nd. Investigate, establish, exercise, prepare for, or defend legal claims.\n\ne. Take immediate steps to protect an interest that is essential for the life\nor physical safety of an individual.\n\nf. Engage in public or peer-reviewed scientific or statistical research in the\npublic interest that adheres to all other applicable ethics and privacy laws\nand is conducted in accordance with 45 C.F.R. pt. 46, as amended, or other\nrelevant requirements established by the federal food and drug administration.\n\ng. Conduct research, testing, and development activities regarding an\nartificial intelligence system, other than testing conducted under real-world\nconditions, before the artificial intelligence system is placed on the market,\nused, or otherwise put into service.\n\nh. Effectuate a product recall.\n\ni. Identify and repair technical errors that impair existing or intended\nfunctionality in a computer system or an artificial intelligence system.\n\nj. Assist another developer, deployer, or person with any of the requirements\nof this chapter.\n\n2\\. This chapter does not apply to a developer, deployer, or other person if\nthe circumstances in which the high-risk artificial intelligence system was\ndeveloped, used, or intentionally and substantially modified are described by\nany of the following:\n\na. The high-risk artificial intelligence system has been approved, authorized,\ncertified, cleared, developed, or granted by a federal agency and the\ndeployer, developer, or other person is described by any of the following:\n\n(1) The person is acting under the authority of the federal agency that\napproved, authorized, certified, cleared, developed, or granted the high-risk\nartificial intelligence system.\n\n(2) The person is in compliance with standards established by a federal agency\nfor use of high-risk artificial intelligence systems if the requirements\nimposed by those standards are substantially similar or more restrictive than\nthe requirements of this chapter.\n\nb. The developer, deployer, or other person is conducting research to support\nan application for approval or certification from a federal agency.\n\nc. The developer, deployer, or other person is performing work under or in\nconnection with a contract with the United States department of commerce, the\nUnited States department of defense, or the national aeronautics and space\nadministration, unless the developer, deployer, or other person is performing\nwork on a high-risk artificial intelligence system that is used to make, or is\na factor that would likely alter the outcome of, a decision concerning\nemployment or housing.\n\nd. The developer, deployer, or other person is a covered entity within the\nmeaning of the Health Insurance Portability and Accountability Act of 1996,\nPub. L. No. 104-191, as amended, and is providing health care recommendations\nthat are generated by an artificial intelligence system, require a health care\nprovider to take action to implement the recommendations, and are not likely\nto alter the outcome of a consequential decision.\n\n3\\. This chapter does not apply to any artificial intelligence system that is\nacquired by or for the federal government, a federal agency, or federal\ndepartment unless the artificial intelligence system is a high-risk artificial\nintelligence system used to make, or is a factor that would likely alter the\noutcome of, a decision concerning employment or housing.\n\n4\\. A developer, deployer, or other person shall have the burden to prove an\naction qualifies for an exclusion under this section.\n\nSec. 7. _NEW SECTION._ 554I.6 Enforcement.\n\n1\\. The attorney general shall have exclusive authority to enforce this\nchapter.\n\n2\\. Prior to initiating any action for a violation of this chapter, the\nattorney general shall issue a notice of violation to the developer, deployer,\nor other person who allegedly violated the chapter. The notice shall contain\nall of the following:\n\na. A specific description of the alleged violation.\n\nb. The actions the developer, deployer, or other person must take to cure the\nviolation.\n\n3\\. The attorney general may bring an action for the alleged violation if the\nalleged violation is not cured within ninety days of the date the developer,\ndeployer, or other person received the notice of violation.\n\n4\\. A violation of the requirements established in this chapter is an unlawful\npractice under section 714.16.\n\n5\\. In any action commenced by the attorney general to enforce this chapter,\nit is an affirmative defense that the developer, deployer, or other person is\ndescribed by all of the following:\n\na. The developer, deployer, or other person discovered and cured a violation\nof this chapter as a result of any of the following:\n\n(1) Feedback that the developer, deployer, or other person encourages\ndeployers or other users to provide to the developer, deployer, or other\nperson.\n\n(2) Adversarial testing or red teaming, as those terms are defined or used by\nthe national institute of standards and technology.\n\n(3) An internal review process.\n\nb. The developer, deployer, or other person is otherwise in compliance with\nany of the following:\n\n(1) The latest version of the artificial intelligence risk management\nframework as published by the national institute of standards and technology\nand standard ISO/IEC 42001 of the international organization for\nstandardization.\n\n(2) A nationally or internationally recognized risk management framework for\nartificial intelligence systems, if the requirements imposed by those\nstandards are substantially similar or more restrictive than the requirements\nof this chapter.\n\n(3) A risk management framework for artificial intelligence systems that the\nattorney general designated.\n\n6\\. A developer, deployer, or other person bears the burden of demonstrating\nto the attorney general that the requirements of subsection 5 have been\nsatisfied.\n\n7\\. a. Prior to initiating any action under this chapter, the attorney general\nshall consult with the department of health and human services to determine\nwhether any complaint has been filed that is founded on the same act or\nomission that constitutes a violation of this chapter.\n\nb. The attorney general shall not initiate any action to enforce the\nprovisions of this chapter if a complaint has been filed with the department\nof health and human services relating to the act or omission that constitutes\na violation of this chapter unless the complaint has been fully adjudicated or\nresolved.\n\n8\\. This chapter shall not preempt or otherwise affect any right, claim,\nremedy, presumption, or defense available at law or in equity. Any rebuttable\npresumption or affirmative defense established under this chapter shall apply\nonly to an enforcement action brought by the attorney general pursuant to this\nchapter and shall not apply to any right, claim, remedy, presumption, or\ndefense available at law or in equity.\n\n9\\. The attorney general shall post on its internet site how to properly file\na complaint for a violation under this chapter.\n\n10\\. This section does not provide a basis for a private right of action for\nviolations of this chapter or any other law.\n\nSec. 8. _NEW SECTION._ 554I.7 Attorney general rulemaking authority.\n\nThe attorney general shall adopt rules pursuant to chapter 17A to implement\nthis chapter.\n\nSec. 9. Section 714.16, subsection 2, Code 2025, is amended by adding the\nfollowing new paragraph:\n\n_NEW PARAGRAPH._ r. It is an unlawful practice for a person to violate any of\nthe provisions of chapter 554I.\n\nEXPLANATION\n\nThe inclusion of this explanation does not constitute agreement withthe\nexplanation's substance by the members of the general assembly.\n\nThis bill relates to artificial intelligence (AI), including the use of\nartificial intelligence to create materials related to elections and\nprotections in interactions with AI systems.\n\nDIVISION I -- MATERIALS RELATED TO ELECTIONS. The bill requires that published\nmaterial generated through the use of AI, defined in this division, and\ndesigned to expressly advocate the nomination, election, or defeat of a\ncandidate for public office or the passage or defeat of a ballot issue to\ninclude a disclosure that the published material was generated using AI. The\ndisclosure must include the words \"this material was generated using\nartificial intelligence\". A disclosure made in compliance with the bill does\nnot preclude a private right of action arising out of the publication of\npublished material generated through the use of AI.\n\nBy operation of law, a person who willfully violates the division of the bill\nis guilty of a serious misdemeanor. A serious misdemeanor is punishable by\nconfinement for no more than one year and a fine of at least $430 but not more\nthan $2,560.\n\nDIVISION II -- PROTECTIONS IN INTERACTIONS WITH ARTIFICIAL INTELLIGENCE\nSYSTEMS. The bill defines \"algorithmic discrimination\" as any use of an AI\nsystem that results in unfavorable treatment due to an individual or group of\nindividuals' actual or perceived age, race, creed, color, sex, sexual\norientation, national origin, religion, or disability. The bill lists several\ncircumstances which do not constitute \"algorithmic discrimination\".\n\nThe bill defines \"artificial intelligence system\" as any machine-based system\nthat, for any explicit or implicit objective, infers from the inputs the\nsystem receives to generate outputs, including but not limited to content,\ndecisions, predictions, or recommendations, that can influence physical or\nvirtual environments.\n\nThe bill defines \"consequential decision\" as any decision that has a material\nlegal or similarly significant effect on the provision or denial of a pardon,\nparole, probation, or release; an education enrollment or educational\nopportunity; employment; a financial or lending service; an essential\ngovernment service; a health care service; insurance; or a legal service.\n\nThe bill defines \"deployer\" as a person doing business in this state that uses\na high-risk AI system.\n\nThe bill defines \"developer\" as a person doing business in this state that\ndevelops or intentionally and substantially modifies a high-risk AI system.\n\nThe bill defines \"high-risk artificial intelligence system\" as any AI system\nthat makes, or is a factor that would likely alter the outcome of, a\nconsequential decision. The bill lists several systems that do not constitute\na \"high-risk artificial intelligence system\".\n\nThe bill defines \"intentional and substantial modification\" or \"intentionally\nand substantially modifies\" as a deliberate change made to an AI system that\nmaterially increases the risk of algorithmic discrimination.\n\nThe bill defines \"trade secret\" as information, including but not limited to a\nformula, pattern, compilation, program, device, method, technique, or process\nthat derives independent economic value, actual or potential, from not being\ngenerally known to, and not being readily ascertainable by proper means by a\nperson able to obtain economic value from its disclosure or use; and is the\nsubject of efforts that are reasonable under the circumstances to maintain its\nsecrecy.\n\nThe bill requires developers to use reasonable care to protect individuals\nfrom any known or reasonably foreseeable risks of algorithmic discrimination\narising from the intended and contracted uses of the developer's high-risk AI\nsystem. The bill creates a rebuttable presumption that a developer used\nreasonable care if the developer complied with certain requirements detailed\nin the bill and any additional requirements established in rules adopted by\nthe attorney general.\n\nThe bill requires a developer to make certain statements, documentation, and\nsummaries related to the high-risk AI system, as detailed in the bill,\navailable to a deployer or other developer that uses or intends to use the\ndeveloper's high-risk AI system.\n\nThe bill requires a developer, if the developer offers, sells, leases,\nlicenses, gives, or otherwise makes a high-risk AI system available to a\ndeployer, to provide documentation and information to the deployer necessary\nfor the deployer to complete an impact assessment. Documentation and\ninformation includes but is not limited to model cards, dataset cards, and\nother impact assessments. This requirement does not apply if the deployer is\naffiliated with the developer that is providing the high-risk AI system.\n\nThe bill requires a developer to make a statement available in a manner that\nis clear and readily available on the developer's internet site or in a public\nuse case inventory that includes a summary of the types of high-risk AI\nsystems the developer has developed or intentionally and substantially\nmodified and is currently making generally available to deployers, and how the\ndeveloper manages known or reasonably foreseeable risks of algorithmic\ndiscrimination arising from development or intentional and substantial\nmodification of the types of high-risk AI systems. The bill requires\ndevelopers to update the statement as necessary to ensure that the statement\nremains accurate, but no later than 90 days after the developer develops or\nintentionally and substantially modifies an AI system.\n\nThe bill requires developers to disclose to the attorney general and to all\nknown deployers or other developers of the high-risk AI system any known or\nforeseeable risks of algorithmic discrimination arising from the intended uses\nof the high-risk AI system. The disclosure must be given to the attorney\ngeneral no later than 90 days after the developer discovers through the\ndeveloper's ongoing testing and analysis that the developer's high-risk AI\nsystem has been used and has caused or is reasonably likely to have caused\nalgorithmic discrimination; or the developer receives from a deployer a\ncredible report that the high-risk AI system has been used and has caused\nalgorithmic discrimination, whichever is earlier. The bill does not require a\ndeveloper to disclose a trade secret, information protected under state or\nfederal law, or other confidential or proprietary information.\n\nThe bill authorizes the attorney general to require a developer to disclose to\nthe attorney general any statement or documentation the bill requires the\ndeveloper to make available to deployers and other developers if the statement\nor documentation is relevant to an investigation conducted by the attorney\ngeneral regarding a violation of the bill. The bill makes statements and\ndocumentation provided to the attorney general exempt from disclosure to the\nextent the statement or documentation includes any proprietary information or\nany trade secret. The developer may designate the statement or documentation\nas including proprietary information or a trade secret. Disclosures of\nstatements and documents to the attorney general that include information\nsubject to attorney-client privilege or work-product protection do not\nconstitute a waiver of the privilege or protection.\n\nThe bill requires deployers to use reasonable care to protect individuals from\nany known or reasonably foreseeable risks of algorithmic discrimination. In\nany enforcement action brought by the attorney general, there is a rebuttable\npresumption that a deployer used reasonable care if the deployer implemented a\nrisk management policy and program as detailed in the bill and complied with\nthe bill's requirements regarding impact statements.\n\nThe bill requires deployers, or a third party contracted by a deployer to use\na high-risk AI system, to complete an impact assessment for the high-risk AI\nsystem. Requirements for when the impact assessment must be completed and the\ncontents of the impact assessment are detailed in the bill.\n\nThe bill requires deployers to maintain the most recently completed impact\nassessment for a high-risk AI system and relevant records supporting the\nimpact assessment for a period of at least three years following the final use\nof the high-risk AI system.\n\nThe bill requires deployers to, at least annually, review the deployment of\neach high-risk AI system used by the deployer to ensure that the high-risk AI\nsystem is not causing algorithmic discrimination.\n\nThe bill requires deployers to disclose to each individual that interacts with\nthe AI system that the individual is interacting with an AI system. Developers\nwho make a high-risk AI system available in this state must cooperate with\ndeployers to allow deployers to fulfill the bill's requirements. A disclosure\nthat an individual is interacting with an AI system is not required in\ncircumstances in which it would be obvious to a reasonable individual that the\nindividual is interacting with an AI system.\n\nThe bill clarifies that its provisions do not restrict a developer's,\ndeployer's, or other person's abilities to perform certain actions as detailed\nin the bill.\n\nThe bill clarifies that its provisions do not apply to a developer, deployer,\nor other person that develops, uses, or intentionally and substantially\nmodifies a high-risk AI system in certain circumstances detailed in the bill.\n\nThe bill does not apply to an AI system that is acquired by or for the federal\ngovernment, a federal agency, or a federal department unless the AI system is\na high-risk AI system used to make, or is a factor that would likely alter the\noutcome of, a decision concerning employment or housing.\n\nThe bill makes a developer, deployer, or other person responsible for proving\nan action qualifies for an exclusion from the bill's provisions.\n\nThe bill provides the attorney general with exclusive authority to enforce the\nbill's provisions.\n\nThe bill requires, prior to initiating any action for a violation of the\nbill's provisions, the attorney general to issue a notice of violation to the\ndeveloper, deployer, or other person who allegedly violated the bill's\nprovisions. The notice must contain a specific description of the alleged\nviolation and the actions the developer, deployer, or other person must take\nto cure the violation. The bill authorizes the attorney general to bring an\naction for the alleged violation if the alleged violation is not cured within\n90 days of the date the developer, deployer, or other person received the\nnotice of violation.\n\nThe bill details under what circumstances a developer, deployer, or other\nperson has an affirmative defense to an alleged violation. The developer,\ndeployer, or other person bears the burden of demonstrating to the attorney\ngeneral that the person meets the requirements for an affirmative defense.\n\nThe bill requires, prior to initiating any action under the bill's provisions,\nthe attorney general to consult with the department of health and human\nservices (HHS) to determine whether any complaint has been filed that is\nfounded on the same act or omission that constitutes a violation of the bill's\nprovisions. The attorney general is prohibited from initiating an action to\nenforce the bill's provisions if a complaint has been filed with HHS relating\nto the same act or omission that constitutes a violation of the bill's\nprovisions unless the complaint has been fully adjudicated or resolved.\n\nThe bill does not preempt or otherwise affect any right, claim, remedy,\npresumption, or defense available at law or in equity. Any rebuttable\npresumption or affirmative defense established under the bill's provisions\napplies only to an enforcement action brought by the attorney general and does\nnot apply to any right, claim, remedy, presumption, or defense available at\nlaw or in equity.\n\nThe bill requires the attorney general to post on its internet site how to\nproperly file a complaint for a violation of the bill's provisions.\n\nThe bill does not provide a basis for a private right of action for violations\nof the bill's provisions or any other law.\n\nThe bill requires the attorney general to adopt rules to implement the bill's\nprovisions.\n\nA violation of this division of the bill is an unlawful practice under Code\nsection 714.16. Several types of remedies are available if a court finds that\na person has committed an unlawful practice, including injunctive relief,\ndisgorgement of moneys or property, and a civil penalty not to exceed $40,000\nper violation.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": true
    }
  ]
}