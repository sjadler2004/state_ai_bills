{
  "bill_id": "NM2025000H60",
  "source_url": "http://custom.statenet.com/public/resources.cgi?id=ID:bill:NM2025000H60&cuiq=93d84396-c63b-526a-b152-38b7f79b4cfd&client_md=e4f6fea4-27b4-5d41-b7d3-766fe52569f0",
  "versions": [
    {
      "date": "01/09/2025",
      "label": "Prefiled",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:NM2025000H60&verid=NM2025000H60_20250109_0_F&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 NM H 60</td> <td><table><tr><td class=\"label\">Author:</td> <td>Chandler</td></tr> <tr><td class=\"label\">Version:</td> <td>Prefiled</td></tr> <tr><td class=\"label\">Version Date:</td> <td>01/09/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">HOUSE BILL 60</p>\n   <p class=\"center\">57th legislature - STATE OF NEW MEXICO - FIRST SESSION, 2025</p>\n   <p class=\"center\">INTRODUCED BY</p>\n   <p class=\"center\">Christine Chandler</p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"center\">AN ACT</p>\n   <p class=\"left\">RELATING TO ARTIFICIAL INTELLIGENCE; ENACTING THE ARTIFICIAL INTELLIGENCE ACT; REQUIRING NOTICE OF USE, DOCUMENTATION OF SYSTEMS, DISCLOSURE OF ALGORITHMIC DISCRIMINATION RISK AND RISK INCIDENTS; REQUIRING RISK MANAGEMENT POLICIES AND IMPACT ASSESSMENTS; PROVIDING FOR ENFORCEMENT BY THE STATE DEPARTMENT OF JUSTICE AND FOR CIVIL ACTIONS BY CONSUMERS FOR INJUNCTIVE OR DECLARATORY RELIEF. </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"left\">BE IT ENACTED BY THE LEGISLATURE OF THE STATE OF NEW MEXICO:</p>\n   </span>\n   <p class=\"indent\">SECTION 1. [<u>NEW MATERIAL</u>] SHORT TITLE.--This act may be cited as the &quot;Artificial Intelligence Act&quot;.</p>\n   <p class=\"indent\">SECTION 2. [<u>NEW MATERIAL</u>] DEFINITIONS.--As used in the Artificial Intelligence Act:</p>\n   <p class=\"indent\">A. &quot;algorithmic discrimination&quot; means any condition in which the use of an artificial intelligence system results in an unlawful differential treatment or impact that disfavors a person on the basis of the person&#39;s actual or perceived age, color, disability, ethnicity, gender, genetic information, proficiency in the English language, national origin, race, religion, reproductive health, veteran status or other status protected by state or federal law, but does not include:</p>\n   <p class=\"indent\">(1) the offer, license or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of:</p>\n   <p class=\"indent\">(a) the developer&#39;s or deployer&#39;s self-testing to identify, mitigate or ensure compliance with state and federal law; or</p>\n   <p class=\"indent\">(b) expanding an applicant, customer or participant pool to increase diversity or redress historical discrimination; or</p>\n   <p class=\"indent\">(2) an act or omission by or on behalf of a private club or other entity that is not open to the public pursuant to federal law;</p>\n   <p class=\"indent\">B. &quot;artificial intelligence system&quot; means any machine-based system that for an explicit or implicit objective infers from the inputs the system receives how to generate outputs, including content, decisions, predictions or recommendations, that can influence physical or virtual environments;</p>\n   <p class=\"indent\">C. &quot;consequential decision&quot; means a decision that has a material legal or similarly significant effect on the provision or denial to a consumer of or the cost or terms of:</p>\n   <p class=\"indent\">(1) education enrollment or an educational opportunity;</p>\n   <p class=\"indent\">(2) employment or an employment opportunity;</p>\n   <p class=\"indent\">(3) a financial or lending service;</p>\n   <p class=\"indent\">(4) health care service;</p>\n   <p class=\"indent\">(5) housing;</p>\n   <p class=\"indent\">(6) insurance; or</p>\n   <p class=\"indent\">(7) legal service;</p>\n   <p class=\"indent\">D. &quot;consumer&quot; means a resident of New Mexico;</p>\n   <p class=\"indent\">E. &quot;deploy&quot; means to use an artificial intelligence system;</p>\n   <p class=\"indent\">F. &quot;deployer&quot; means a person who deploys an artificial intelligence system;</p>\n   <p class=\"indent\">G. &quot;developer&quot; means a person who develops or intentionally and substantially modifies an artificial intelligence system;</p>\n   <p class=\"indent\">H. &quot;health care services&quot; means treatment, services or research designed to promote the improved health of a person, including primary care, prenatal care, dental care, behavioral health care, alcohol or drug detoxification and rehabilitation, hospital care, the provision of prescription drugs, preventive care or health outreach;</p>\n   <p class=\"indent\">I. &quot;high-level summary&quot; means information about the data and data sets used to train the high-risk artificial intelligence system, including:</p>\n   <p class=\"indent\">(1) the sources or owners of the data sets and whether the data sets were purchased or licensed by the developer;</p>\n   <p class=\"indent\">(2) the factors in the data, including attributes or other information about a consumer, that the system uses to produce its outputs, scores or recommendations;</p>\n   <p class=\"indent\">(3) the demographic groups represented in the data sets and the proportion of each age, ethnic, gender or racial group in each dataset;</p>\n   <p class=\"indent\">(4) a description of the types of data points within the data sets, including, for data sets that include labels, a description of the types of labels used;</p>\n   <p class=\"indent\">(5) whether the data sets include any data protected by copyright, trademark or patent or whether the data sets are entirely in the public domain;</p>\n   <p class=\"indent\">(6) whether there was any cleaning, processing or other modification to the data sets by the developer, including the intended purpose of those efforts in relation to the high-risk artificial intelligence system;</p>\n   <p class=\"indent\">(7) the time period during which the data in the data sets were collected, including a notice when data collection is ongoing;</p>\n   <p class=\"indent\">(8) the geographical regions or jurisdictions in which the data sets were collected, including whether the data sets were collected solely in New Mexico, solely in other states or in New Mexico in combination with other states; and</p>\n   <p class=\"indent\">(9) other information as required by the state department of justice by rule;</p>\n   <p class=\"indent\">J. &quot;high-risk artificial intelligence system&quot; means any artificial intelligence system that when deployed makes or is a substantial factor in making a consequential decision, but does not include:</p>\n   <p class=\"indent\">(1) an artificial intelligence system intended to:</p>\n   <p class=\"indent\">(a) perform a narrow procedural task; or</p>\n   <p class=\"indent\">(b) detect decision-making patterns or deviations from prior decision-making patterns and is not intended to replace or influence a previously completed human assessment without sufficient human review; or</p>\n   <p class=\"indent\">(2) the following technologies, unless the technologies make or are a substantial factor in making a consequential decision when the technologies are deployed:</p>\n   <p class=\"indent\">(a) anti-fraud technology that does not use facial recognition technology;</p>\n   <p class=\"indent\">(b) anti-malware;</p>\n   <p class=\"indent\">(c) antivirus;</p>\n   <p class=\"indent\">(d) artificial-intelligence-enabled video games;</p>\n   <p class=\"indent\">(e) calculators;</p>\n   <p class=\"indent\">(f) cybersecurity;</p>\n   <p class=\"indent\">(g) databases;</p>\n   <p class=\"indent\">(h) data storage;</p>\n   <p class=\"indent\">(i) firewalls;</p>\n   <p class=\"indent\">(j) internet domain registration;</p>\n   <p class=\"indent\">(k) internet website loading;</p>\n   <p class=\"indent\">(l) networking;</p>\n   <p class=\"indent\">(m) spam and robocall filtering;</p>\n   <p class=\"indent\">(n) spell checking;</p>\n   <p class=\"indent\">(o) spreadsheets;</p>\n   <p class=\"indent\">(p) web caching;</p>\n   <p class=\"indent\">(q) web hosting or similar technology; or</p>\n   <p class=\"indent\">(r) technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations and answering questions and is subject to an accepted use policy that prohibits generating content that is discriminatory or harmful;</p>\n   <p class=\"indent\">K. &quot;intentional and substantial modification&quot; and &quot;intentionally and substantially modifies&quot; means a deliberate change made to an artificial intelligence system that results in a new reasonably foreseeable risk of algorithmic discrimination, but does not include a change made to a high-risk artificial intelligence system or the performance of a high-risk artificial intelligence system when:</p>\n   <p class=\"indent\">(1) the high-risk artificial intelligence system continues to learn after the system is:</p>\n   <p class=\"indent\">(a) offered, sold, leased, licensed, given or otherwise made available to a deployer; or</p>\n   <p class=\"indent\">(b) deployed;</p>\n   <p class=\"indent\">(2) the change is made as a result of system learning after being made available to a deployer or being deployed;</p>\n   <p class=\"indent\">(3) the change was predetermined by the deployer or a third party contracted by the deployer when the deployer or third party completed an impact assessment of the high-risk artificial intelligence system pursuant to Section 6 of the Artificial Intelligence Act; or</p>\n   <p class=\"indent\">(4) the change is included in technical documentation for the high-risk artificial intelligence system;</p>\n   <p class=\"indent\">L. &quot;offered or made available&quot; includes a gift, lease, sale or other conveyance of an artificial intelligence system to a recipient deployer or a developer other than the original system developer;</p>\n   <p class=\"indent\">M. &quot;recipient&quot; means a deployer who has received an artificial intelligence system from a developer or a developer who has received an artificial intelligence system from another developer;</p>\n   <p class=\"indent\">N. &quot;risk incident&quot; means an incident when a developer discovers or receives a credible report from a deployer that a high-risk artificial intelligence system offered or made available by the developer has caused or is reasonably likely to have caused algorithmic discrimination;</p>\n   <p class=\"indent\">O. &quot;substantial factor&quot; means:</p>\n   <p class=\"indent\">(1) a factor that:</p>\n   <p class=\"indent\">(a) assists in making a consequential decision;</p>\n   <p class=\"indent\">(b) is capable of altering, advising or influencing the outcome of a consequential decision; and</p>\n   <p class=\"indent\">(c) is generated by an artificial intelligence system; or</p>\n   <p class=\"indent\">(2) content, decisions, labels, predictions, recommendations or scores generated by an artificial intelligence system concerning a consumer that are used as a basis, partial basis or recommendation to make a consequential decision concerning the consumer; and</p>\n   <p class=\"indent\">P. &quot;trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique or process, that:</p>\n   <p class=\"indent\">(1) derives independent economic value, actual or potential, from not being generally known to and not being readily ascertainable by proper means by other persons who could obtain economic value from the information&#39;s disclosure or use; and</p>\n   <p class=\"indent\">(2) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</p>\n   <p class=\"indent\">SECTION 3. [<u>NEW MATERIAL</u>] DUTY OF CARE--DISCLOSURE OF RISK POTENTIAL--PROVISION OF DOCUMENTATION.--A developer shall:</p>\n   <p class=\"indent\">A. use reasonable care to protect consumers from known or foreseeable risks of algorithmic discrimination arising from intended and contracted uses of a high-risk artificial intelligence system;</p>\n   <p class=\"indent\">B. except for information excluded pursuant to Subsection C of Section 4 of the Artificial Intelligence Act, make the following available to a recipient of the developer&#39;s high-risk artificial intelligence system:</p>\n   <p class=\"indent\">(1) a general summary describing the reasonably foreseeable uses and known harmful or inappropriate uses of the system; and</p>\n   <p class=\"indent\">(2) documentation disclosing:</p>\n   <p class=\"indent\">(a) the purpose, intended uses and benefits of the system;</p>\n   <p class=\"indent\">(b) a high-level summary of the type of data used to train the system;</p>\n   <p class=\"indent\">(c) known or reasonable foreseeable limitations of the system, including the risk of algorithmic discrimination arising from the intended use of the system;</p>\n   <p class=\"indent\">(d) how the system was evaluated for performance and mitigation of algorithmic discrimination prior to being offered or made available to the deployer, including: 1) the metrics of performance and bias that were used; 2) how the metrics were measured; 3) any independent studies carried out to evaluate the system for performance and risk of discrimination; and 4) whether the studies are publicly available or peer-reviewed;</p>\n   <p class=\"indent\">(e) the measures governing the data sets used to train the system, the suitability of data sources, possible biases and bias mitigation;</p>\n   <p class=\"indent\">(f) the intended outputs of the system;</p>\n   <p class=\"indent\">(g) the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that are reasonably foreseeable from the use of the system;</p>\n   <p class=\"indent\">(h) how the system should be used and monitored by the deployer;</p>\n   <p class=\"indent\">(i) any additional information that is reasonably necessary to assist the deployer in understanding the outputs and monitoring the performance of the system for risks of algorithmic discrimination; and</p>\n   <p class=\"indent\">(j) any other information necessary to allow the deployer to comply with the requirements of this section;</p>\n   <p class=\"indent\">C. except for information excluded pursuant to Subsection C of Section 4 of the Artificial Intelligence Act, to the extent feasible make available to the recipient the necessary information to conduct an impact assessment as required pursuant to Section 6 of the Artificial Intelligence Act. Such information shall include model cards, dataset cards or previous impact assessments relevant to the system, its development or use;</p>\n   <p class=\"indent\">D. post on the developer&#39;s website in a clear and readily available manner a statement or public-use case inventory that summarizes:</p>\n   <p class=\"indent\">(1) the types of high-risk artificial intelligence systems that the developer has developed or intentionally and substantially modified and currently offers or makes available to recipients; and</p>\n   <p class=\"indent\">(2) how the developer manages known or reasonably foreseeable risks of algorithmic discrimination that may arise from the use or intentional and substantial modification of the systems listed on the developer&#39;s website pursuant to this subsection; and</p>\n   <p class=\"indent\">E. ensure that the statement or public-use case inventory posted pursuant to this section remains accurate and is updated within ninety days of an intentional and substantial modification of a high-risk artificial intelligence system offered or made available by the developer to recipients.</p>\n   <p class=\"indent\">SECTION 4. [<u>NEW MATERIAL</u>] RISK INCIDENTS--REQUIRED DISCLOSURE AND SUBMISSION--EXCEPTIONS.--</p>\n   <p class=\"indent\">A. Within ninety days of a risk incident and in a form and manner prescribed by the state department of justice, a developer shall disclose to the department and all known recipients of the high-risk artificial intelligence system that is the basis of the risk incident the known and foreseeable risks of algorithmic discrimination that may arise from the intended uses of the system.</p>\n   <p class=\"indent\">B. Within ninety days of a request by the state department of justice, a developer shall submit to the department a copy of the summary and documentation the developer has made available to recipients pursuant to Section 3 of the Artificial Intelligence Act. A developer may designate the summary or documentation as including proprietary information or a trade secret. To the extent that information contained in the summary or documentation includes information subject to attorney-client privilege or work-product protection, compliance with this section does not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">C. As part of a disclosure, notice or submission pursuant to the Artificial Intelligence Act, a developer shall not be required to disclose a trade secret, information protected from disclosure by state or federal law or information that would create a security risk to the developer. Such disclosure, notice or submission shall be exempt from disclosure pursuant to the Inspection of Public Records Act.</p>\n   <p class=\"indent\">SECTION 5. [<u>NEW MATERIAL</u>] DEPLOYER RISK-MANAGEMENT POLICY REQUIRED.--</p>\n   <p class=\"indent\">A. A deployer shall use reasonable care to protect consumers from known or reasonably foreseeable risks of algorithmic discrimination.</p>\n   <p class=\"indent\">B. A deployer shall implement a risk management policy and program to govern the deployer&#39;s deployment of a high-risk artificial intelligence system. The risk management policy and program shall:</p>\n   <p class=\"indent\">(1) specify and incorporate the principles, processes and personnel that the deployer uses to identify, document and mitigate known or reasonably foreseeable risks of algorithmic discrimination; and</p>\n   <p class=\"indent\">(2) be an iterative process planned, implemented and regularly and systematically updated over the life cycle of a high-risk artificial intelligence system and include regular systematic review and updates.</p>\n   <p class=\"indent\">C. A risk management policy shall meet standards established by the state department of justice by rule.</p>\n   <p class=\"indent\">SECTION 6. [<u>NEW MATERIAL</u>] DEPLOYER IMPACT ASSESSMENTS.--</p>\n   <p class=\"indent\">A. Except as provided in Subsections D, E and H of this section, a deployer shall conduct an impact assessment for any high-risk artificial intelligence system deployed by the deployer:</p>\n   <p class=\"indent\">(1) annually; and</p>\n   <p class=\"indent\">(2) within ninety days of an intentional and substantial modification to the system.</p>\n   <p class=\"indent\">B. An impact assessment of a high-risk artificial intelligence system completed pursuant to this section shall include, to the extent reasonably known by or available to the deployer:</p>\n   <p class=\"indent\">(1) a statement of the intended uses, deployment contexts and benefits of the system;</p>\n   <p class=\"indent\">(2) an analysis of any known or reasonably foreseeable risks of algorithmic discrimination posed by the system, and when a risk exists, the nature of the algorithmic discrimination and the steps that have been taken to mitigate the risk;</p>\n   <p class=\"indent\">(3) a description of the categories of data the system processes as inputs and the outputs the system produces;</p>\n   <p class=\"indent\">(4) a summary of categories of any data used to customize the system;</p>\n   <p class=\"indent\">(5) the metrics used to evaluate the performance and known limitations of the system, including:</p>\n   <p class=\"indent\">(a) whether the evaluation was carried out using test data;</p>\n   <p class=\"indent\">(b) whether the test data sets were collected solely in New Mexico, solely in other states or in New Mexico in combination with other states;</p>\n   <p class=\"indent\">(c) the demographic groups represented in the test data sets and the proportion of each age, ethnic, gender or racial group in each data set; and</p>\n   <p class=\"indent\">(d) any independent studies carried out to evaluate the system for performance and risk of discrimination and whether the studies are publicly available or peer-reviewed;</p>\n   <p class=\"indent\">(6) a description of any transparency measures taken concerning the system, including measures taken to disclose to a consumer when the system is in use; and</p>\n   <p class=\"indent\">(7) a description of the post-deployment monitoring and user safeguards provided for the system, including oversight, use and learning processes used by the deployer to address issues arising from deployment of the system.</p>\n   <p class=\"indent\">C. An impact assessment conducted due to an intentional and substantial modification of a high-risk artificial intelligence system shall include a disclosure of the extent to which the system was used in a manner consistent with, or that varied from, the developer&#39;s intended uses of the system.</p>\n   <p class=\"indent\">D. A deployer may use a single impact assessment to address a set of comparable high-risk artificial intelligence systems.</p>\n   <p class=\"indent\">E. An impact assessment conducted for the purpose of complying with another applicable law or rule shall satisfy the requirement of this section when the assessment:</p>\n   <p class=\"indent\">(1) meets the requirements of this section; and</p>\n   <p class=\"indent\">(2) is reasonably similar in scope and effect to an assessment that would otherwise be conducted pursuant to this section.</p>\n   <p class=\"indent\">F. For at least three years following the final deployment of a high-risk artificial intelligence system, a deployer shall maintain records of the most recently conducted impact assessment for the system, including all records concerning the assessment and all prior assessments for the system.</p>\n   <p class=\"indent\">G. On or before March 1, 2027, a deployer shall review each high-risk artificial intelligence system that the deployer has deployed to ensure that the system is not causing algorithmic discrimination.</p>\n   <p class=\"indent\">H. This section is not applicable when:</p>\n   <p class=\"indent\">(1) a deployer using a high-risk artificial intelligence system:</p>\n   <p class=\"indent\">(a) employs fewer than fifty full-time employees;</p>\n   <p class=\"indent\">(b) does not use the deployer&#39;s own data to train the system;</p>\n   <p class=\"indent\">(c) uses the system solely for the system&#39;s intended uses as disclosed by a developer pursuant to the Artificial Intelligence Act; and</p>\n   <p class=\"indent\">(d) makes any impact assessment of the system that has been provided by the developer pursuant to the Artificial Intelligence Act available to consumers; and</p>\n   <p class=\"indent\">(2) the system continues learning based on data derived from sources other than the deployer&#39;s own data.</p>\n   <p class=\"indent\">SECTION 7. [<u>NEW MATERIAL</u>] DEPLOYER GENERAL NOTICE TO CONSUMERS.--</p>\n   <p class=\"indent\">A. A deployer shall make readily available to its consumers and on its website:</p>\n   <p class=\"indent\">(1) a summary of the types of high-risk artificial intelligence systems that the deployer currently deploys and how known or reasonably foreseeable risks of algorithmic discrimination from the deployment of each system are managed; and</p>\n   <p class=\"indent\">(2) a detailed explanation of the nature, source and extent of the information collected and used by the deployer.</p>\n   <p class=\"indent\">B. At a minimum, a deployer shall update the information posted on its website pursuant to this section annually and when the deployer deploys a new high-risk artificial intelligence system.</p>\n   <p class=\"indent\">SECTION 8. [<u>NEW MATERIAL</u>] USE OF ARTIFICIAL INTELLIGENCE SYSTEMS WHEN MAKING CONSEQUENTIAL DECISIONS--DIRECT NOTICE TO AFFECTED CONSUMERS--ADVERSE DECISIONS--OPPORTUNITY FOR APPEAL.--</p>\n   <p class=\"indent\">A. Except as provided in Subsection E of this section, before a high-risk artificial intelligence system is used to make or is a substantial factor in making a consequential decision concerning a consumer, a deployer shall provide directly to the consumer:</p>\n   <p class=\"indent\">(1) notice that the system will be used to make or be a substantial factor in making the decision; and</p>\n   <p class=\"indent\">(2) information describing:</p>\n   <p class=\"indent\">(a) the system and how to access the deployer&#39;s notice required pursuant to Section 7 of the Artificial Intelligence Act;</p>\n   <p class=\"indent\">(b) the purpose of the system and the nature of the consequential decision being made; and</p>\n   <p class=\"indent\">(c) the deployer&#39;s contact information.</p>\n   <p class=\"indent\">B. Except as provided in Subsection E of this section, when a high-risk artificial intelligence system has been used to make or has been a substantial factor in making a consequential decision concerning a consumer that is adverse to the consumer, the deployer shall provide directly to the consumer:</p>\n   <p class=\"indent\">(1) a statement explaining:</p>\n   <p class=\"indent\">(a) the principal reason or reasons for the decision;</p>\n   <p class=\"indent\">(b) the degree and manner in which the system contributed to the decision; and</p>\n   <p class=\"indent\">(c) the source and type of data that was processed by the system to make or that was a substantial factor in making the decision;</p>\n   <p class=\"indent\">(2) an opportunity to correct any incorrect personal data that the system processed to make or that was a substantial factor in making the decision; and</p>\n   <p class=\"indent\">(3) an opportunity to appeal the adverse decision except in instances where an appeal is not in the best interest of the consumer, such as creating a delay that may pose a risk of life or safety to the consumer.</p>\n   <p class=\"indent\">C. If technically feasible, an appeal of an adverse decision pursuant to this section shall allow for human review.</p>\n   <p class=\"indent\">D. All information, notices and statements to a consumer as required by this section shall be provided:</p>\n   <p class=\"indent\">(1) in plain language and in all languages in which the deployer in the ordinary course of business provides contracts, disclaimers, sale announcements and other information to consumers; and</p>\n   <p class=\"indent\">(2) in a format that is accessible to consumers with disabilities.</p>\n   <p class=\"indent\">E. When a deployer is unable to provide information, notice or a statement required pursuant to this section directly to a consumer, the deployer shall make such information, notices or statements available in a manner that is reasonably calculated to ensure that the consumer receives the information, notice or statement.</p>\n   <p class=\"indent\">SECTION 9. [<u>NEW MATERIAL</u>] USE OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM--NOTICE AND DISCLOSURE TO THE STATE DEPARTMENT OF JUSTICE--INSPECTION OF PUBLIC RECORDS ACT EXEMPTION.--</p>\n   <p class=\"indent\">A. When a deployer discovers that a high-risk artificial intelligence system that has been used has caused algorithmic discrimination, the deployer shall as expeditiously as possible but at a maximum within ninety days notify the state department of justice of the discovery. The notice shall be in a form and manner prescribed by the department.</p>\n   <p class=\"indent\">B. Upon request by the state department of justice, a deployer shall within ninety days submit to the state department of justice any risk management policy, impact assessment or records conducted, implemented, maintained or received pursuant to the Artificial Intelligence Act. The submission shall be in a form and manner prescribed by the department.</p>\n   <p class=\"indent\">C. The state department of justice may evaluate risk management policies, impact assessments or records submitted pursuant to this section for compliance with the Artificial Intelligence Act.</p>\n   <p class=\"indent\">D. A risk management policy, impact assessment or record submitted to the state department of justice pursuant to this section is exempt from disclosure pursuant to the Inspection of Public Records Act.</p>\n   <p class=\"indent\">E. In a submission pursuant to this section, a deployer may designate a portion of the submission as including proprietary information or a trade secret and to the extent that a submission contains information subject to attorney-client privilege or work-product protection, the submission does not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">SECTION 10. [<u>NEW MATERIAL</u>] INTERACTION OF ARTIFICIAL INTELLIGENCE SYSTEM WITH CONSUMERS--REQUIRED DISCLOSURE.--A developer that offers or makes available an artificial intelligence system intended to interact with consumers shall ensure that a consumer is informed that the consumer is interacting with an artificial intelligence system. This section does not apply when it would be obvious to a reasonable person that the consumer is interacting with an artificial intelligence system.</p>\n   <p class=\"indent\">SECTION 11. [<u>NEW MATERIAL</u>] EXEMPTION FROM DISCLOSURE--TRADE SECRETS AND OTHER INFORMATION PROTECTED BY LAW--NOTICE TO CONSUMER.--</p>\n   <p class=\"indent\">A. Nothing in the Artificial Intelligence Act shall require a deployer or developer to disclose a trade secret or other information protected from disclosure by state or federal law.</p>\n   <p class=\"indent\">B. To the extent that a deployer or developer withholds information pursuant to this section that would otherwise be part of a disclosure pursuant to the Artificial Intelligence Act, the deployer or developer shall notify a consumer and provide a basis for the withholding.</p>\n   <p class=\"indent\">SECTION 12. [<u>NEW MATERIAL</u>] APPLICABILITY EXEMPTIONS-- OTHER LAW--SECURITY AND TESTING--FEDERAL USE--INSURANCE PROVIDERS.--</p>\n   <p class=\"indent\">A. No provision of the Artificial Intelligence Act shall be construed to restrict a person&#39;s ability to:</p>\n   <p class=\"indent\">(1) comply with federal, state or municipal laws or regulations;</p>\n   <p class=\"indent\">(2) comply with a civil, criminal or regulatory inquiry, investigation, subpoena or summons by a governmental authority;</p>\n   <p class=\"indent\">(3) cooperate with a law enforcement agency concerning activity that the person reasonably and in good faith believes may violate other laws or regulations;</p>\n   <p class=\"indent\">(4) defend, exercise or investigate legal claims;</p>\n   <p class=\"indent\">(5) act to protect an interest that is essential for the life or physical safety of a person;</p>\n   <p class=\"indent\">(6) by means other than the use of facial recognition technology:</p>\n   <p class=\"indent\">(a) detect, prevent, protect against or respond to deceptive, illegal or malicious activity, fraud, identity theft, harassment or security incidents; or</p>\n   <p class=\"indent\">(b) investigate, prosecute or report persons responsible for the actions listed in Subparagraph (a) of this paragraph;</p>\n   <p class=\"indent\">(7) preserve the integrity or security of artificial intelligence, computer, electronic or internet connection systems;</p>\n   <p class=\"indent\">(8) engage in public or peer-reviewed scientific or statistical research that adheres to and is conducted in accordance with applicable federal and state law;</p>\n   <p class=\"indent\">(9) engage in pre-market testing other than testing conducted under real-world conditions, including development, research and testing of artificial intelligence systems; or</p>\n   <p class=\"indent\">(10) assist another person with compliance with the Artificial Intelligence Use Act.</p>\n   <p class=\"indent\">B. No provision of the Artificial Intelligence Act shall be construed to restrict:</p>\n   <p class=\"indent\">(1) a product recall; or</p>\n   <p class=\"indent\">(2) identification or repair of technical errors that impair the functionality of an artificial intelligence system.</p>\n   <p class=\"indent\">C. The Artificial Intelligence Act shall not apply in circumstances where compliance would violate an evidentiary privilege pursuant to law.</p>\n   <p class=\"indent\">D. No provision of the Artificial Intelligence Act shall be construed so as to limit a person&#39;s rights to free speech or freedom of the press pursuant to the first amendment to the United States constitution or Article 2, Section 17 of the constitution of New Mexico.</p>\n   <p class=\"indent\">E. The Artificial Intelligence Act shall not apply to a developer, deployer or other person who:</p>\n   <p class=\"indent\">(1) uses or intentionally and substantially modifies a high-risk artificial intelligence system that:</p>\n   <p class=\"indent\">(a) has been authorized by a federal agency in accordance with federal law; and</p>\n   <p class=\"indent\">(b) is in compliance with standards established by a federal agency in accordance with federal law when such standards are substantially equivalent or more stringent than the requirements of the Artificial Intelligence Act;</p>\n   <p class=\"indent\">(2) conducts research to support an application for certification or review by a federal agency pursuant to federal law;</p>\n   <p class=\"indent\">(3) performs work under or in connection with a contract with a federal agency, unless the work is on a high- risk artificial intelligence system used to make or as a substantial factor in making a decision concerning employment or housing; or</p>\n   <p class=\"indent\">(4) is a covered entity pursuant to federal health insurance law and is providing health care recommendations:</p>\n   <p class=\"indent\">(a) generated by an artificial intelligence system;</p>\n   <p class=\"indent\">(b) that require a health care provider to take action to implement the recommendations; and</p>\n   <p class=\"indent\">(c) that are not considered to be high risk.</p>\n   <p class=\"indent\">F. The Artificial Intelligence Act shall not apply to an artificial intelligence system acquired by the federal government, except for a high-risk artificial intelligence system used to make or as a substantial factor in making a decision concerning employment or housing.</p>\n   <p class=\"indent\">G. A financial institution or affiliate or subsidiary of a financial institution that is subject to prudential regulation by another state or by the federal government pursuant to laws that apply to the use of high-risk artificial intelligence systems shall be deemed to be in compliance with the Artificial Intelligence Act when the applicable laws:</p>\n   <p class=\"indent\">(1) impose requirements that are substantially equivalent to or more stringent than the requirements imposed by the Artificial Intelligence Act; and</p>\n   <p class=\"indent\">(2) at a minimum, require the financial institution to:</p>\n   <p class=\"indent\">(a) regularly audit the institution&#39;s use of high-risk artificial intelligence systems for compliance with state and federal antidiscrimination laws; and</p>\n   <p class=\"indent\">(b) mitigate any algorithmic discrimination caused by the use of a high-risk artificial intelligence system.</p>\n   <p class=\"indent\">H. As used in this section, &quot;financial institution&quot; means an insured state or national bank, a state or federal savings and loan association or savings bank, a state or federal credit union or authorized branches of each of the foregoing.</p>\n   <p class=\"indent\">I. A developer, deployer or other person who engages in an action pursuant to an exemption set forth in this section shall bear the burden of demonstrating that the action qualifies for the exemption.</p>\n   <p class=\"indent\">SECTION 13. [<u>NEW MATERIAL</u>] ENFORCEMENT--STATE DEPARTMENT OF JUSTICE--CONSUMER CIVIL ACTIONS.--</p>\n   <p class=\"indent\">A. Upon the promulgation of rules pursuant to Section 14 of the Artificial Intelligence Act:</p>\n   <p class=\"indent\">(1) the state department of justice shall have authority to enforce that act; and</p>\n   <p class=\"indent\">(2) a consumer may bring a civil action in district court against a developer or deployer for declaratory or injunctive relief and attorney fees for a violation of that act.</p>\n   <p class=\"indent\">B. In an action by the state department of justice to enforce the Artificial Intelligence Act, it is an affirmative defense when:</p>\n   <p class=\"indent\">(1) the developer, deployer or other person discovers and cures a violation of the Artificial Intelligence Act as a result of:</p>\n   <p class=\"indent\">(a) feedback that the developer, deployer or other person encourages the deployer or users to provide; or</p>\n   <p class=\"indent\">(b) adversarial testing, red teaming or an internal review process; and</p>\n   <p class=\"indent\">(2) the developer, deployer or other person is in compliance with a risk management framework for artificial intelligence systems designated by the state department of justice by rule.</p>\n   <p class=\"indent\">C. In an action by the state department of justice to enforce the Artificial Intelligence Act, the developer, deployer or other person who is the subject of the enforcement shall bear the burden of demonstrating that the requirements for an affirmative defense pursuant to this section have been met.</p>\n   <p class=\"indent\">D. Nothing within the Artificial Intelligence Act, including the enforcement authority granted to the state department of justice pursuant to this section, preempts or otherwise affects any right, claim, remedy, presumption or defense available in law or equity.</p>\n   <p class=\"indent\">E. An affirmative defense or rebuttable presumption established by the Artificial Intelligence Act applies only to an enforcement action by the state department of justice and does not apply to any right, claim, remedy, presumption or defense available in law or equity.</p>\n   <p class=\"indent\">F. A violation of the Artificial Intelligence Act is an unfair practice and may be enforced pursuant to the Unfair Practices Act.</p>\n   <p class=\"indent\">G. As used in this section:</p>\n   <p class=\"indent\">(1) &quot;adversarial testing&quot; means to proactively try to break an application by providing it with data most likely to elicit problematic output, or as defined by the state department of justice by rule; and</p>\n   <p class=\"indent\">(2) &quot;red teaming&quot; means the practice of simulating attack scenarios on an artificial intelligence application to pinpoint weaknesses and plan preventive measures or as defined by the state department of justice by rule.</p>\n   <p class=\"indent\">SECTION 14. [<u>NEW MATERIAL</u>] RULEMAKING.--On or before January 1, 2027, the state department of justice shall promulgate rules to implement the Artificial Intelligence Act and shall post them prominently on the state department of justice&#39;s website.</p>\n   <effective_clause>\n    <p class=\"indent\">SECTION 15. EFFECTIVE DATE.--The effective date of the provisions of this act is July 1, 2026.</p>\n   </effective_clause>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 NM H 60 | | Author: | Chandler  \n---|---  \nVersion: | Prefiled  \nVersion Date: | 01/09/2025  \n  \nHOUSE BILL 60\n\n57th legislature - STATE OF NEW MEXICO - FIRST SESSION, 2025\n\nINTRODUCED BY\n\nChristine Chandler\n\nAN ACT\n\nRELATING TO ARTIFICIAL INTELLIGENCE; ENACTING THE ARTIFICIAL INTELLIGENCE ACT;\nREQUIRING NOTICE OF USE, DOCUMENTATION OF SYSTEMS, DISCLOSURE OF ALGORITHMIC\nDISCRIMINATION RISK AND RISK INCIDENTS; REQUIRING RISK MANAGEMENT POLICIES AND\nIMPACT ASSESSMENTS; PROVIDING FOR ENFORCEMENT BY THE STATE DEPARTMENT OF\nJUSTICE AND FOR CIVIL ACTIONS BY CONSUMERS FOR INJUNCTIVE OR DECLARATORY\nRELIEF.\n\nBE IT ENACTED BY THE LEGISLATURE OF THE STATE OF NEW MEXICO:\n\nSECTION 1. [_NEW MATERIAL_] SHORT TITLE.--This act may be cited as the\n\"Artificial Intelligence Act\".\n\nSECTION 2. [_NEW MATERIAL_] DEFINITIONS.--As used in the Artificial\nIntelligence Act:\n\nA. \"algorithmic discrimination\" means any condition in which the use of an\nartificial intelligence system results in an unlawful differential treatment\nor impact that disfavors a person on the basis of the person's actual or\nperceived age, color, disability, ethnicity, gender, genetic information,\nproficiency in the English language, national origin, race, religion,\nreproductive health, veteran status or other status protected by state or\nfederal law, but does not include:\n\n(1) the offer, license or use of a high-risk artificial intelligence system by\na developer or deployer for the sole purpose of:\n\n(a) the developer's or deployer's self-testing to identify, mitigate or ensure\ncompliance with state and federal law; or\n\n(b) expanding an applicant, customer or participant pool to increase diversity\nor redress historical discrimination; or\n\n(2) an act or omission by or on behalf of a private club or other entity that\nis not open to the public pursuant to federal law;\n\nB. \"artificial intelligence system\" means any machine-based system that for an\nexplicit or implicit objective infers from the inputs the system receives how\nto generate outputs, including content, decisions, predictions or\nrecommendations, that can influence physical or virtual environments;\n\nC. \"consequential decision\" means a decision that has a material legal or\nsimilarly significant effect on the provision or denial to a consumer of or\nthe cost or terms of:\n\n(1) education enrollment or an educational opportunity;\n\n(2) employment or an employment opportunity;\n\n(3) a financial or lending service;\n\n(4) health care service;\n\n(5) housing;\n\n(6) insurance; or\n\n(7) legal service;\n\nD. \"consumer\" means a resident of New Mexico;\n\nE. \"deploy\" means to use an artificial intelligence system;\n\nF. \"deployer\" means a person who deploys an artificial intelligence system;\n\nG. \"developer\" means a person who develops or intentionally and substantially\nmodifies an artificial intelligence system;\n\nH. \"health care services\" means treatment, services or research designed to\npromote the improved health of a person, including primary care, prenatal\ncare, dental care, behavioral health care, alcohol or drug detoxification and\nrehabilitation, hospital care, the provision of prescription drugs, preventive\ncare or health outreach;\n\nI. \"high-level summary\" means information about the data and data sets used to\ntrain the high-risk artificial intelligence system, including:\n\n(1) the sources or owners of the data sets and whether the data sets were\npurchased or licensed by the developer;\n\n(2) the factors in the data, including attributes or other information about a\nconsumer, that the system uses to produce its outputs, scores or\nrecommendations;\n\n(3) the demographic groups represented in the data sets and the proportion of\neach age, ethnic, gender or racial group in each dataset;\n\n(4) a description of the types of data points within the data sets, including,\nfor data sets that include labels, a description of the types of labels used;\n\n(5) whether the data sets include any data protected by copyright, trademark\nor patent or whether the data sets are entirely in the public domain;\n\n(6) whether there was any cleaning, processing or other modification to the\ndata sets by the developer, including the intended purpose of those efforts in\nrelation to the high-risk artificial intelligence system;\n\n(7) the time period during which the data in the data sets were collected,\nincluding a notice when data collection is ongoing;\n\n(8) the geographical regions or jurisdictions in which the data sets were\ncollected, including whether the data sets were collected solely in New\nMexico, solely in other states or in New Mexico in combination with other\nstates; and\n\n(9) other information as required by the state department of justice by rule;\n\nJ. \"high-risk artificial intelligence system\" means any artificial\nintelligence system that when deployed makes or is a substantial factor in\nmaking a consequential decision, but does not include:\n\n(1) an artificial intelligence system intended to:\n\n(a) perform a narrow procedural task; or\n\n(b) detect decision-making patterns or deviations from prior decision-making\npatterns and is not intended to replace or influence a previously completed\nhuman assessment without sufficient human review; or\n\n(2) the following technologies, unless the technologies make or are a\nsubstantial factor in making a consequential decision when the technologies\nare deployed:\n\n(a) anti-fraud technology that does not use facial recognition technology;\n\n(b) anti-malware;\n\n(c) antivirus;\n\n(d) artificial-intelligence-enabled video games;\n\n(e) calculators;\n\n(f) cybersecurity;\n\n(g) databases;\n\n(h) data storage;\n\n(i) firewalls;\n\n(j) internet domain registration;\n\n(k) internet website loading;\n\n(l) networking;\n\n(m) spam and robocall filtering;\n\n(n) spell checking;\n\n(o) spreadsheets;\n\n(p) web caching;\n\n(q) web hosting or similar technology; or\n\n(r) technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations and answering questions and is subject to an accepted use\npolicy that prohibits generating content that is discriminatory or harmful;\n\nK. \"intentional and substantial modification\" and \"intentionally and\nsubstantially modifies\" means a deliberate change made to an artificial\nintelligence system that results in a new reasonably foreseeable risk of\nalgorithmic discrimination, but does not include a change made to a high-risk\nartificial intelligence system or the performance of a high-risk artificial\nintelligence system when:\n\n(1) the high-risk artificial intelligence system continues to learn after the\nsystem is:\n\n(a) offered, sold, leased, licensed, given or otherwise made available to a\ndeployer; or\n\n(b) deployed;\n\n(2) the change is made as a result of system learning after being made\navailable to a deployer or being deployed;\n\n(3) the change was predetermined by the deployer or a third party contracted\nby the deployer when the deployer or third party completed an impact\nassessment of the high-risk artificial intelligence system pursuant to Section\n6 of the Artificial Intelligence Act; or\n\n(4) the change is included in technical documentation for the high-risk\nartificial intelligence system;\n\nL. \"offered or made available\" includes a gift, lease, sale or other\nconveyance of an artificial intelligence system to a recipient deployer or a\ndeveloper other than the original system developer;\n\nM. \"recipient\" means a deployer who has received an artificial intelligence\nsystem from a developer or a developer who has received an artificial\nintelligence system from another developer;\n\nN. \"risk incident\" means an incident when a developer discovers or receives a\ncredible report from a deployer that a high-risk artificial intelligence\nsystem offered or made available by the developer has caused or is reasonably\nlikely to have caused algorithmic discrimination;\n\nO. \"substantial factor\" means:\n\n(1) a factor that:\n\n(a) assists in making a consequential decision;\n\n(b) is capable of altering, advising or influencing the outcome of a\nconsequential decision; and\n\n(c) is generated by an artificial intelligence system; or\n\n(2) content, decisions, labels, predictions, recommendations or scores\ngenerated by an artificial intelligence system concerning a consumer that are\nused as a basis, partial basis or recommendation to make a consequential\ndecision concerning the consumer; and\n\nP. \"trade secret\" means information, including a formula, pattern,\ncompilation, program, device, method, technique or process, that:\n\n(1) derives independent economic value, actual or potential, from not being\ngenerally known to and not being readily ascertainable by proper means by\nother persons who could obtain economic value from the information's\ndisclosure or use; and\n\n(2) is the subject of efforts that are reasonable under the circumstances to\nmaintain its secrecy.\n\nSECTION 3. [_NEW MATERIAL_] DUTY OF CARE--DISCLOSURE OF RISK POTENTIAL--\nPROVISION OF DOCUMENTATION.--A developer shall:\n\nA. use reasonable care to protect consumers from known or foreseeable risks of\nalgorithmic discrimination arising from intended and contracted uses of a\nhigh-risk artificial intelligence system;\n\nB. except for information excluded pursuant to Subsection C of Section 4 of\nthe Artificial Intelligence Act, make the following available to a recipient\nof the developer's high-risk artificial intelligence system:\n\n(1) a general summary describing the reasonably foreseeable uses and known\nharmful or inappropriate uses of the system; and\n\n(2) documentation disclosing:\n\n(a) the purpose, intended uses and benefits of the system;\n\n(b) a high-level summary of the type of data used to train the system;\n\n(c) known or reasonable foreseeable limitations of the system, including the\nrisk of algorithmic discrimination arising from the intended use of the\nsystem;\n\n(d) how the system was evaluated for performance and mitigation of algorithmic\ndiscrimination prior to being offered or made available to the deployer,\nincluding: 1) the metrics of performance and bias that were used; 2) how the\nmetrics were measured; 3) any independent studies carried out to evaluate the\nsystem for performance and risk of discrimination; and 4) whether the studies\nare publicly available or peer-reviewed;\n\n(e) the measures governing the data sets used to train the system, the\nsuitability of data sources, possible biases and bias mitigation;\n\n(f) the intended outputs of the system;\n\n(g) the measures the developer has taken to mitigate known or reasonably\nforeseeable risks of algorithmic discrimination that are reasonably\nforeseeable from the use of the system;\n\n(h) how the system should be used and monitored by the deployer;\n\n(i) any additional information that is reasonably necessary to assist the\ndeployer in understanding the outputs and monitoring the performance of the\nsystem for risks of algorithmic discrimination; and\n\n(j) any other information necessary to allow the deployer to comply with the\nrequirements of this section;\n\nC. except for information excluded pursuant to Subsection C of Section 4 of\nthe Artificial Intelligence Act, to the extent feasible make available to the\nrecipient the necessary information to conduct an impact assessment as\nrequired pursuant to Section 6 of the Artificial Intelligence Act. Such\ninformation shall include model cards, dataset cards or previous impact\nassessments relevant to the system, its development or use;\n\nD. post on the developer's website in a clear and readily available manner a\nstatement or public-use case inventory that summarizes:\n\n(1) the types of high-risk artificial intelligence systems that the developer\nhas developed or intentionally and substantially modified and currently offers\nor makes available to recipients; and\n\n(2) how the developer manages known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the use or intentional and\nsubstantial modification of the systems listed on the developer's website\npursuant to this subsection; and\n\nE. ensure that the statement or public-use case inventory posted pursuant to\nthis section remains accurate and is updated within ninety days of an\nintentional and substantial modification of a high-risk artificial\nintelligence system offered or made available by the developer to recipients.\n\nSECTION 4. [_NEW MATERIAL_] RISK INCIDENTS--REQUIRED DISCLOSURE AND SUBMISSION\n--EXCEPTIONS.--\n\nA. Within ninety days of a risk incident and in a form and manner prescribed\nby the state department of justice, a developer shall disclose to the\ndepartment and all known recipients of the high-risk artificial intelligence\nsystem that is the basis of the risk incident the known and foreseeable risks\nof algorithmic discrimination that may arise from the intended uses of the\nsystem.\n\nB. Within ninety days of a request by the state department of justice, a\ndeveloper shall submit to the department a copy of the summary and\ndocumentation the developer has made available to recipients pursuant to\nSection 3 of the Artificial Intelligence Act. A developer may designate the\nsummary or documentation as including proprietary information or a trade\nsecret. To the extent that information contained in the summary or\ndocumentation includes information subject to attorney-client privilege or\nwork-product protection, compliance with this section does not constitute a\nwaiver of the privilege or protection.\n\nC. As part of a disclosure, notice or submission pursuant to the Artificial\nIntelligence Act, a developer shall not be required to disclose a trade\nsecret, information protected from disclosure by state or federal law or\ninformation that would create a security risk to the developer. Such\ndisclosure, notice or submission shall be exempt from disclosure pursuant to\nthe Inspection of Public Records Act.\n\nSECTION 5. [_NEW MATERIAL_] DEPLOYER RISK-MANAGEMENT POLICY REQUIRED.--\n\nA. A deployer shall use reasonable care to protect consumers from known or\nreasonably foreseeable risks of algorithmic discrimination.\n\nB. A deployer shall implement a risk management policy and program to govern\nthe deployer's deployment of a high-risk artificial intelligence system. The\nrisk management policy and program shall:\n\n(1) specify and incorporate the principles, processes and personnel that the\ndeployer uses to identify, document and mitigate known or reasonably\nforeseeable risks of algorithmic discrimination; and\n\n(2) be an iterative process planned, implemented and regularly and\nsystematically updated over the life cycle of a high-risk artificial\nintelligence system and include regular systematic review and updates.\n\nC. A risk management policy shall meet standards established by the state\ndepartment of justice by rule.\n\nSECTION 6. [_NEW MATERIAL_] DEPLOYER IMPACT ASSESSMENTS.--\n\nA. Except as provided in Subsections D, E and H of this section, a deployer\nshall conduct an impact assessment for any high-risk artificial intelligence\nsystem deployed by the deployer:\n\n(1) annually; and\n\n(2) within ninety days of an intentional and substantial modification to the\nsystem.\n\nB. An impact assessment of a high-risk artificial intelligence system\ncompleted pursuant to this section shall include, to the extent reasonably\nknown by or available to the deployer:\n\n(1) a statement of the intended uses, deployment contexts and benefits of the\nsystem;\n\n(2) an analysis of any known or reasonably foreseeable risks of algorithmic\ndiscrimination posed by the system, and when a risk exists, the nature of the\nalgorithmic discrimination and the steps that have been taken to mitigate the\nrisk;\n\n(3) a description of the categories of data the system processes as inputs and\nthe outputs the system produces;\n\n(4) a summary of categories of any data used to customize the system;\n\n(5) the metrics used to evaluate the performance and known limitations of the\nsystem, including:\n\n(a) whether the evaluation was carried out using test data;\n\n(b) whether the test data sets were collected solely in New Mexico, solely in\nother states or in New Mexico in combination with other states;\n\n(c) the demographic groups represented in the test data sets and the\nproportion of each age, ethnic, gender or racial group in each data set; and\n\n(d) any independent studies carried out to evaluate the system for performance\nand risk of discrimination and whether the studies are publicly available or\npeer-reviewed;\n\n(6) a description of any transparency measures taken concerning the system,\nincluding measures taken to disclose to a consumer when the system is in use;\nand\n\n(7) a description of the post-deployment monitoring and user safeguards\nprovided for the system, including oversight, use and learning processes used\nby the deployer to address issues arising from deployment of the system.\n\nC. An impact assessment conducted due to an intentional and substantial\nmodification of a high-risk artificial intelligence system shall include a\ndisclosure of the extent to which the system was used in a manner consistent\nwith, or that varied from, the developer's intended uses of the system.\n\nD. A deployer may use a single impact assessment to address a set of\ncomparable high-risk artificial intelligence systems.\n\nE. An impact assessment conducted for the purpose of complying with another\napplicable law or rule shall satisfy the requirement of this section when the\nassessment:\n\n(1) meets the requirements of this section; and\n\n(2) is reasonably similar in scope and effect to an assessment that would\notherwise be conducted pursuant to this section.\n\nF. For at least three years following the final deployment of a high-risk\nartificial intelligence system, a deployer shall maintain records of the most\nrecently conducted impact assessment for the system, including all records\nconcerning the assessment and all prior assessments for the system.\n\nG. On or before March 1, 2027, a deployer shall review each high-risk\nartificial intelligence system that the deployer has deployed to ensure that\nthe system is not causing algorithmic discrimination.\n\nH. This section is not applicable when:\n\n(1) a deployer using a high-risk artificial intelligence system:\n\n(a) employs fewer than fifty full-time employees;\n\n(b) does not use the deployer's own data to train the system;\n\n(c) uses the system solely for the system's intended uses as disclosed by a\ndeveloper pursuant to the Artificial Intelligence Act; and\n\n(d) makes any impact assessment of the system that has been provided by the\ndeveloper pursuant to the Artificial Intelligence Act available to consumers;\nand\n\n(2) the system continues learning based on data derived from sources other\nthan the deployer's own data.\n\nSECTION 7. [_NEW MATERIAL_] DEPLOYER GENERAL NOTICE TO CONSUMERS.--\n\nA. A deployer shall make readily available to its consumers and on its\nwebsite:\n\n(1) a summary of the types of high-risk artificial intelligence systems that\nthe deployer currently deploys and how known or reasonably foreseeable risks\nof algorithmic discrimination from the deployment of each system are managed;\nand\n\n(2) a detailed explanation of the nature, source and extent of the information\ncollected and used by the deployer.\n\nB. At a minimum, a deployer shall update the information posted on its website\npursuant to this section annually and when the deployer deploys a new high-\nrisk artificial intelligence system.\n\nSECTION 8. [_NEW MATERIAL_] USE OF ARTIFICIAL INTELLIGENCE SYSTEMS WHEN MAKING\nCONSEQUENTIAL DECISIONS--DIRECT NOTICE TO AFFECTED CONSUMERS--ADVERSE\nDECISIONS--OPPORTUNITY FOR APPEAL.--\n\nA. Except as provided in Subsection E of this section, before a high-risk\nartificial intelligence system is used to make or is a substantial factor in\nmaking a consequential decision concerning a consumer, a deployer shall\nprovide directly to the consumer:\n\n(1) notice that the system will be used to make or be a substantial factor in\nmaking the decision; and\n\n(2) information describing:\n\n(a) the system and how to access the deployer's notice required pursuant to\nSection 7 of the Artificial Intelligence Act;\n\n(b) the purpose of the system and the nature of the consequential decision\nbeing made; and\n\n(c) the deployer's contact information.\n\nB. Except as provided in Subsection E of this section, when a high-risk\nartificial intelligence system has been used to make or has been a substantial\nfactor in making a consequential decision concerning a consumer that is\nadverse to the consumer, the deployer shall provide directly to the consumer:\n\n(1) a statement explaining:\n\n(a) the principal reason or reasons for the decision;\n\n(b) the degree and manner in which the system contributed to the decision; and\n\n(c) the source and type of data that was processed by the system to make or\nthat was a substantial factor in making the decision;\n\n(2) an opportunity to correct any incorrect personal data that the system\nprocessed to make or that was a substantial factor in making the decision; and\n\n(3) an opportunity to appeal the adverse decision except in instances where an\nappeal is not in the best interest of the consumer, such as creating a delay\nthat may pose a risk of life or safety to the consumer.\n\nC. If technically feasible, an appeal of an adverse decision pursuant to this\nsection shall allow for human review.\n\nD. All information, notices and statements to a consumer as required by this\nsection shall be provided:\n\n(1) in plain language and in all languages in which the deployer in the\nordinary course of business provides contracts, disclaimers, sale\nannouncements and other information to consumers; and\n\n(2) in a format that is accessible to consumers with disabilities.\n\nE. When a deployer is unable to provide information, notice or a statement\nrequired pursuant to this section directly to a consumer, the deployer shall\nmake such information, notices or statements available in a manner that is\nreasonably calculated to ensure that the consumer receives the information,\nnotice or statement.\n\nSECTION 9. [_NEW MATERIAL_] USE OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM--\nNOTICE AND DISCLOSURE TO THE STATE DEPARTMENT OF JUSTICE--INSPECTION OF PUBLIC\nRECORDS ACT EXEMPTION.--\n\nA. When a deployer discovers that a high-risk artificial intelligence system\nthat has been used has caused algorithmic discrimination, the deployer shall\nas expeditiously as possible but at a maximum within ninety days notify the\nstate department of justice of the discovery. The notice shall be in a form\nand manner prescribed by the department.\n\nB. Upon request by the state department of justice, a deployer shall within\nninety days submit to the state department of justice any risk management\npolicy, impact assessment or records conducted, implemented, maintained or\nreceived pursuant to the Artificial Intelligence Act. The submission shall be\nin a form and manner prescribed by the department.\n\nC. The state department of justice may evaluate risk management policies,\nimpact assessments or records submitted pursuant to this section for\ncompliance with the Artificial Intelligence Act.\n\nD. A risk management policy, impact assessment or record submitted to the\nstate department of justice pursuant to this section is exempt from disclosure\npursuant to the Inspection of Public Records Act.\n\nE. In a submission pursuant to this section, a deployer may designate a\nportion of the submission as including proprietary information or a trade\nsecret and to the extent that a submission contains information subject to\nattorney-client privilege or work-product protection, the submission does not\nconstitute a waiver of the privilege or protection.\n\nSECTION 10. [_NEW MATERIAL_] INTERACTION OF ARTIFICIAL INTELLIGENCE SYSTEM\nWITH CONSUMERS--REQUIRED DISCLOSURE.--A developer that offers or makes\navailable an artificial intelligence system intended to interact with\nconsumers shall ensure that a consumer is informed that the consumer is\ninteracting with an artificial intelligence system. This section does not\napply when it would be obvious to a reasonable person that the consumer is\ninteracting with an artificial intelligence system.\n\nSECTION 11. [_NEW MATERIAL_] EXEMPTION FROM DISCLOSURE--TRADE SECRETS AND\nOTHER INFORMATION PROTECTED BY LAW--NOTICE TO CONSUMER.--\n\nA. Nothing in the Artificial Intelligence Act shall require a deployer or\ndeveloper to disclose a trade secret or other information protected from\ndisclosure by state or federal law.\n\nB. To the extent that a deployer or developer withholds information pursuant\nto this section that would otherwise be part of a disclosure pursuant to the\nArtificial Intelligence Act, the deployer or developer shall notify a consumer\nand provide a basis for the withholding.\n\nSECTION 12. [_NEW MATERIAL_] APPLICABILITY EXEMPTIONS-- OTHER LAW--SECURITY\nAND TESTING--FEDERAL USE--INSURANCE PROVIDERS.--\n\nA. No provision of the Artificial Intelligence Act shall be construed to\nrestrict a person's ability to:\n\n(1) comply with federal, state or municipal laws or regulations;\n\n(2) comply with a civil, criminal or regulatory inquiry, investigation,\nsubpoena or summons by a governmental authority;\n\n(3) cooperate with a law enforcement agency concerning activity that the\nperson reasonably and in good faith believes may violate other laws or\nregulations;\n\n(4) defend, exercise or investigate legal claims;\n\n(5) act to protect an interest that is essential for the life or physical\nsafety of a person;\n\n(6) by means other than the use of facial recognition technology:\n\n(a) detect, prevent, protect against or respond to deceptive, illegal or\nmalicious activity, fraud, identity theft, harassment or security incidents;\nor\n\n(b) investigate, prosecute or report persons responsible for the actions\nlisted in Subparagraph (a) of this paragraph;\n\n(7) preserve the integrity or security of artificial intelligence, computer,\nelectronic or internet connection systems;\n\n(8) engage in public or peer-reviewed scientific or statistical research that\nadheres to and is conducted in accordance with applicable federal and state\nlaw;\n\n(9) engage in pre-market testing other than testing conducted under real-world\nconditions, including development, research and testing of artificial\nintelligence systems; or\n\n(10) assist another person with compliance with the Artificial Intelligence\nUse Act.\n\nB. No provision of the Artificial Intelligence Act shall be construed to\nrestrict:\n\n(1) a product recall; or\n\n(2) identification or repair of technical errors that impair the functionality\nof an artificial intelligence system.\n\nC. The Artificial Intelligence Act shall not apply in circumstances where\ncompliance would violate an evidentiary privilege pursuant to law.\n\nD. No provision of the Artificial Intelligence Act shall be construed so as to\nlimit a person's rights to free speech or freedom of the press pursuant to the\nfirst amendment to the United States constitution or Article 2, Section 17 of\nthe constitution of New Mexico.\n\nE. The Artificial Intelligence Act shall not apply to a developer, deployer or\nother person who:\n\n(1) uses or intentionally and substantially modifies a high-risk artificial\nintelligence system that:\n\n(a) has been authorized by a federal agency in accordance with federal law;\nand\n\n(b) is in compliance with standards established by a federal agency in\naccordance with federal law when such standards are substantially equivalent\nor more stringent than the requirements of the Artificial Intelligence Act;\n\n(2) conducts research to support an application for certification or review by\na federal agency pursuant to federal law;\n\n(3) performs work under or in connection with a contract with a federal\nagency, unless the work is on a high- risk artificial intelligence system used\nto make or as a substantial factor in making a decision concerning employment\nor housing; or\n\n(4) is a covered entity pursuant to federal health insurance law and is\nproviding health care recommendations:\n\n(a) generated by an artificial intelligence system;\n\n(b) that require a health care provider to take action to implement the\nrecommendations; and\n\n(c) that are not considered to be high risk.\n\nF. The Artificial Intelligence Act shall not apply to an artificial\nintelligence system acquired by the federal government, except for a high-risk\nartificial intelligence system used to make or as a substantial factor in\nmaking a decision concerning employment or housing.\n\nG. A financial institution or affiliate or subsidiary of a financial\ninstitution that is subject to prudential regulation by another state or by\nthe federal government pursuant to laws that apply to the use of high-risk\nartificial intelligence systems shall be deemed to be in compliance with the\nArtificial Intelligence Act when the applicable laws:\n\n(1) impose requirements that are substantially equivalent to or more stringent\nthan the requirements imposed by the Artificial Intelligence Act; and\n\n(2) at a minimum, require the financial institution to:\n\n(a) regularly audit the institution's use of high-risk artificial intelligence\nsystems for compliance with state and federal antidiscrimination laws; and\n\n(b) mitigate any algorithmic discrimination caused by the use of a high-risk\nartificial intelligence system.\n\nH. As used in this section, \"financial institution\" means an insured state or\nnational bank, a state or federal savings and loan association or savings\nbank, a state or federal credit union or authorized branches of each of the\nforegoing.\n\nI. A developer, deployer or other person who engages in an action pursuant to\nan exemption set forth in this section shall bear the burden of demonstrating\nthat the action qualifies for the exemption.\n\nSECTION 13. [_NEW MATERIAL_] ENFORCEMENT--STATE DEPARTMENT OF JUSTICE--\nCONSUMER CIVIL ACTIONS.--\n\nA. Upon the promulgation of rules pursuant to Section 14 of the Artificial\nIntelligence Act:\n\n(1) the state department of justice shall have authority to enforce that act;\nand\n\n(2) a consumer may bring a civil action in district court against a developer\nor deployer for declaratory or injunctive relief and attorney fees for a\nviolation of that act.\n\nB. In an action by the state department of justice to enforce the Artificial\nIntelligence Act, it is an affirmative defense when:\n\n(1) the developer, deployer or other person discovers and cures a violation of\nthe Artificial Intelligence Act as a result of:\n\n(a) feedback that the developer, deployer or other person encourages the\ndeployer or users to provide; or\n\n(b) adversarial testing, red teaming or an internal review process; and\n\n(2) the developer, deployer or other person is in compliance with a risk\nmanagement framework for artificial intelligence systems designated by the\nstate department of justice by rule.\n\nC. In an action by the state department of justice to enforce the Artificial\nIntelligence Act, the developer, deployer or other person who is the subject\nof the enforcement shall bear the burden of demonstrating that the\nrequirements for an affirmative defense pursuant to this section have been\nmet.\n\nD. Nothing within the Artificial Intelligence Act, including the enforcement\nauthority granted to the state department of justice pursuant to this section,\npreempts or otherwise affects any right, claim, remedy, presumption or defense\navailable in law or equity.\n\nE. An affirmative defense or rebuttable presumption established by the\nArtificial Intelligence Act applies only to an enforcement action by the state\ndepartment of justice and does not apply to any right, claim, remedy,\npresumption or defense available in law or equity.\n\nF. A violation of the Artificial Intelligence Act is an unfair practice and\nmay be enforced pursuant to the Unfair Practices Act.\n\nG. As used in this section:\n\n(1) \"adversarial testing\" means to proactively try to break an application by\nproviding it with data most likely to elicit problematic output, or as defined\nby the state department of justice by rule; and\n\n(2) \"red teaming\" means the practice of simulating attack scenarios on an\nartificial intelligence application to pinpoint weaknesses and plan preventive\nmeasures or as defined by the state department of justice by rule.\n\nSECTION 14. [_NEW MATERIAL_] RULEMAKING.--On or before January 1, 2027, the\nstate department of justice shall promulgate rules to implement the Artificial\nIntelligence Act and shall post them prominently on the state department of\njustice's website.\n\nSECTION 15. EFFECTIVE DATE.--The effective date of the provisions of this act\nis July 1, 2026.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "01/21/2025",
      "label": "Introduced",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:NM2025000H60&verid=NM2025000H60_20250121_0_I&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 NM H 60</td> <td><table><tr><td class=\"label\">Author:</td> <td>Chandler</td></tr> <tr><td class=\"label\">Version:</td> <td>Introduced</td></tr> <tr><td class=\"label\">Version Date:</td> <td>01/21/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">HOUSE BILL 60</p>\n   <p class=\"center\">57th legislature - STATE OF NEW MEXICO - FIRST SESSION, 2025</p>\n   <p class=\"center\">INTRODUCED BY</p>\n   <p class=\"center\">Christine Chandler</p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"center\">AN ACT</p>\n   <p class=\"left\">RELATING TO ARTIFICIAL INTELLIGENCE; ENACTING THE ARTIFICIAL INTELLIGENCE ACT; REQUIRING NOTICE OF USE, DOCUMENTATION OF SYSTEMS, DISCLOSURE OF ALGORITHMIC DISCRIMINATION RISK AND RISK INCIDENTS; REQUIRING RISK MANAGEMENT POLICIES AND IMPACT ASSESSMENTS; PROVIDING FOR ENFORCEMENT BY THE STATE DEPARTMENT OF JUSTICE AND FOR CIVIL ACTIONS BY CONSUMERS FOR INJUNCTIVE OR DECLARATORY RELIEF. </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"left\">BE IT ENACTED BY THE LEGISLATURE OF THE STATE OF NEW MEXICO:</p>\n   </span>\n   <p class=\"indent\">SECTION 1. [<u>NEW MATERIAL</u>] SHORT TITLE.--This act may be cited as the &quot;Artificial Intelligence Act&quot;.</p>\n   <p class=\"indent\">SECTION 2. [<u>NEW MATERIAL</u>] DEFINITIONS.--As used in the Artificial Intelligence Act:</p>\n   <p class=\"indent\">A. &quot;algorithmic discrimination&quot; means any condition in which the use of an artificial intelligence system results in an unlawful differential treatment or impact that disfavors a person on the basis of the person&#39;s actual or perceived age, color, disability, ethnicity, gender, genetic information, proficiency in the English language, national origin, race, religion, reproductive health, veteran status or other status protected by state or federal law, but does not include:</p>\n   <p class=\"indent\">(1) the offer, license or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of:</p>\n   <p class=\"indent\">(a) the developer&#39;s or deployer&#39;s self-testing to identify, mitigate or ensure compliance with state and federal law; or</p>\n   <p class=\"indent\">(b) expanding an applicant, customer or participant pool to increase diversity or redress historical discrimination; or</p>\n   <p class=\"indent\">(2) an act or omission by or on behalf of a private club or other entity that is not open to the public pursuant to federal law;</p>\n   <p class=\"indent\">B. &quot;artificial intelligence system&quot; means any machine-based system that for an explicit or implicit objective infers from the inputs the system receives how to generate outputs, including content, decisions, predictions or recommendations, that can influence physical or virtual environments;</p>\n   <p class=\"indent\">C. &quot;consequential decision&quot; means a decision that has a material legal or similarly significant effect on the provision or denial to a consumer of or the cost or terms of:</p>\n   <p class=\"indent\">(1) education enrollment or an educational opportunity;</p>\n   <p class=\"indent\">(2) employment or an employment opportunity;</p>\n   <p class=\"indent\">(3) a financial or lending service;</p>\n   <p class=\"indent\">(4) health care service;</p>\n   <p class=\"indent\">(5) housing;</p>\n   <p class=\"indent\">(6) insurance; or</p>\n   <p class=\"indent\">(7) legal service;</p>\n   <p class=\"indent\">D. &quot;consumer&quot; means a resident of New Mexico;</p>\n   <p class=\"indent\">E. &quot;deploy&quot; means to use an artificial intelligence system;</p>\n   <p class=\"indent\">F. &quot;deployer&quot; means a person who deploys an artificial intelligence system;</p>\n   <p class=\"indent\">G. &quot;developer&quot; means a person who develops or intentionally and substantially modifies an artificial intelligence system;</p>\n   <p class=\"indent\">H. &quot;health care services&quot; means treatment, services or research designed to promote the improved health of a person, including primary care, prenatal care, dental care, behavioral health care, alcohol or drug detoxification and rehabilitation, hospital care, the provision of prescription drugs, preventive care or health outreach;</p>\n   <p class=\"indent\">I. &quot;high-level summary&quot; means information about the data and data sets used to train the high-risk artificial intelligence system, including:</p>\n   <p class=\"indent\">(1) the sources or owners of the data sets and whether the data sets were purchased or licensed by the developer;</p>\n   <p class=\"indent\">(2) the factors in the data, including attributes or other information about a consumer, that the system uses to produce its outputs, scores or recommendations;</p>\n   <p class=\"indent\">(3) the demographic groups represented in the data sets and the proportion of each age, ethnic, gender or racial group in each dataset;</p>\n   <p class=\"indent\">(4) a description of the types of data points within the data sets, including, for data sets that include labels, a description of the types of labels used;</p>\n   <p class=\"indent\">(5) whether the data sets include any data protected by copyright, trademark or patent or whether the data sets are entirely in the public domain;</p>\n   <p class=\"indent\">(6) whether there was any cleaning, processing or other modification to the data sets by the developer, including the intended purpose of those efforts in relation to the high-risk artificial intelligence system;</p>\n   <p class=\"indent\">(7) the time period during which the data in the data sets were collected, including a notice when data collection is ongoing;</p>\n   <p class=\"indent\">(8) the geographical regions or jurisdictions in which the data sets were collected, including whether the data sets were collected solely in New Mexico, solely in other states or in New Mexico in combination with other states; and</p>\n   <p class=\"indent\">(9) other information as required by the state department of justice by rule;</p>\n   <p class=\"indent\">J. &quot;high-risk artificial intelligence system&quot; means any artificial intelligence system that when deployed makes or is a substantial factor in making a consequential decision, but does not include:</p>\n   <p class=\"indent\">(1) an artificial intelligence system intended to:</p>\n   <p class=\"indent\">(a) perform a narrow procedural task; or</p>\n   <p class=\"indent\">(b) detect decision-making patterns or deviations from prior decision-making patterns and is not intended to replace or influence a previously completed human assessment without sufficient human review; or</p>\n   <p class=\"indent\">(2) the following technologies, unless the technologies make or are a substantial factor in making a consequential decision when the technologies are deployed:</p>\n   <p class=\"indent\">(a) anti-fraud technology that does not use facial recognition technology;</p>\n   <p class=\"indent\">(b) anti-malware;</p>\n   <p class=\"indent\">(c) antivirus;</p>\n   <p class=\"indent\">(d) artificial-intelligence-enabled video games;</p>\n   <p class=\"indent\">(e) calculators;</p>\n   <p class=\"indent\">(f) cybersecurity;</p>\n   <p class=\"indent\">(g) databases;</p>\n   <p class=\"indent\">(h) data storage;</p>\n   <p class=\"indent\">(i) firewalls;</p>\n   <p class=\"indent\">(j) internet domain registration;</p>\n   <p class=\"indent\">(k) internet website loading;</p>\n   <p class=\"indent\">(l) networking;</p>\n   <p class=\"indent\">(m) spam and robocall filtering;</p>\n   <p class=\"indent\">(n) spell checking;</p>\n   <p class=\"indent\">(o) spreadsheets;</p>\n   <p class=\"indent\">(p) web caching;</p>\n   <p class=\"indent\">(q) web hosting or similar technology; or</p>\n   <p class=\"indent\">(r) technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations and answering questions and is subject to an accepted use policy that prohibits generating content that is discriminatory or harmful;</p>\n   <p class=\"indent\">K. &quot;intentional and substantial modification&quot; and &quot;intentionally and substantially modifies&quot; means a deliberate change made to an artificial intelligence system that results in a new reasonably foreseeable risk of algorithmic discrimination, but does not include a change made to a high-risk artificial intelligence system or the performance of a high-risk artificial intelligence system when:</p>\n   <p class=\"indent\">(1) the high-risk artificial intelligence system continues to learn after the system is:</p>\n   <p class=\"indent\">(a) offered, sold, leased, licensed, given or otherwise made available to a deployer; or</p>\n   <p class=\"indent\">(b) deployed;</p>\n   <p class=\"indent\">(2) the change is made as a result of system learning after being made available to a deployer or being deployed;</p>\n   <p class=\"indent\">(3) the change was predetermined by the deployer or a third party contracted by the deployer when the deployer or third party completed an impact assessment of the high-risk artificial intelligence system pursuant to Section 6 of the Artificial Intelligence Act; or</p>\n   <p class=\"indent\">(4) the change is included in technical documentation for the high-risk artificial intelligence system;</p>\n   <p class=\"indent\">L. &quot;offered or made available&quot; includes a gift, lease, sale or other conveyance of an artificial intelligence system to a recipient deployer or a developer other than the original system developer;</p>\n   <p class=\"indent\">M. &quot;recipient&quot; means a deployer who has received an artificial intelligence system from a developer or a developer who has received an artificial intelligence system from another developer;</p>\n   <p class=\"indent\">N. &quot;risk incident&quot; means an incident when a developer discovers or receives a credible report from a deployer that a high-risk artificial intelligence system offered or made available by the developer has caused or is reasonably likely to have caused algorithmic discrimination;</p>\n   <p class=\"indent\">O. &quot;substantial factor&quot; means:</p>\n   <p class=\"indent\">(1) a factor that:</p>\n   <p class=\"indent\">(a) assists in making a consequential decision;</p>\n   <p class=\"indent\">(b) is capable of altering, advising or influencing the outcome of a consequential decision; and</p>\n   <p class=\"indent\">(c) is generated by an artificial intelligence system; or</p>\n   <p class=\"indent\">(2) content, decisions, labels, predictions, recommendations or scores generated by an artificial intelligence system concerning a consumer that are used as a basis, partial basis or recommendation to make a consequential decision concerning the consumer; and</p>\n   <p class=\"indent\">P. &quot;trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique or process, that:</p>\n   <p class=\"indent\">(1) derives independent economic value, actual or potential, from not being generally known to and not being readily ascertainable by proper means by other persons who could obtain economic value from the information&#39;s disclosure or use; and</p>\n   <p class=\"indent\">(2) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</p>\n   <p class=\"indent\">SECTION 3. [<u>NEW MATERIAL</u>] DUTY OF CARE--DISCLOSURE OF RISK POTENTIAL--PROVISION OF DOCUMENTATION.--A developer shall:</p>\n   <p class=\"indent\">A. use reasonable care to protect consumers from known or foreseeable risks of algorithmic discrimination arising from intended and contracted uses of a high-risk artificial intelligence system;</p>\n   <p class=\"indent\">B. except for information excluded pursuant to Subsection C of Section 4 of the Artificial Intelligence Act, make the following available to a recipient of the developer&#39;s high-risk artificial intelligence system:</p>\n   <p class=\"indent\">(1) a general summary describing the reasonably foreseeable uses and known harmful or inappropriate uses of the system; and</p>\n   <p class=\"indent\">(2) documentation disclosing:</p>\n   <p class=\"indent\">(a) the purpose, intended uses and benefits of the system;</p>\n   <p class=\"indent\">(b) a high-level summary of the type of data used to train the system;</p>\n   <p class=\"indent\">(c) known or reasonable foreseeable limitations of the system, including the risk of algorithmic discrimination arising from the intended use of the system;</p>\n   <p class=\"indent\">(d) how the system was evaluated for performance and mitigation of algorithmic discrimination prior to being offered or made available to the deployer, including: 1) the metrics of performance and bias that were used; 2) how the metrics were measured; 3) any independent studies carried out to evaluate the system for performance and risk of discrimination; and 4) whether the studies are publicly available or peer-reviewed;</p>\n   <p class=\"indent\">(e) the measures governing the data sets used to train the system, the suitability of data sources, possible biases and bias mitigation;</p>\n   <p class=\"indent\">(f) the intended outputs of the system;</p>\n   <p class=\"indent\">(g) the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that are reasonably foreseeable from the use of the system;</p>\n   <p class=\"indent\">(h) how the system should be used and monitored by the deployer;</p>\n   <p class=\"indent\">(i) any additional information that is reasonably necessary to assist the deployer in understanding the outputs and monitoring the performance of the system for risks of algorithmic discrimination; and</p>\n   <p class=\"indent\">(j) any other information necessary to allow the deployer to comply with the requirements of this section;</p>\n   <p class=\"indent\">C. except for information excluded pursuant to Subsection C of Section 4 of the Artificial Intelligence Act, to the extent feasible make available to the recipient the necessary information to conduct an impact assessment as required pursuant to Section 6 of the Artificial Intelligence Act. Such information shall include model cards, dataset cards or previous impact assessments relevant to the system, its development or use;</p>\n   <p class=\"indent\">D. post on the developer&#39;s website in a clear and readily available manner a statement or public-use case inventory that summarizes:</p>\n   <p class=\"indent\">(1) the types of high-risk artificial intelligence systems that the developer has developed or intentionally and substantially modified and currently offers or makes available to recipients; and</p>\n   <p class=\"indent\">(2) how the developer manages known or reasonably foreseeable risks of algorithmic discrimination that may arise from the use or intentional and substantial modification of the systems listed on the developer&#39;s website pursuant to this subsection; and</p>\n   <p class=\"indent\">E. ensure that the statement or public-use case inventory posted pursuant to this section remains accurate and is updated within ninety days of an intentional and substantial modification of a high-risk artificial intelligence system offered or made available by the developer to recipients.</p>\n   <p class=\"indent\">SECTION 4. [<u>NEW MATERIAL</u>] RISK INCIDENTS--REQUIRED DISCLOSURE AND SUBMISSION--EXCEPTIONS.--</p>\n   <p class=\"indent\">A. Within ninety days of a risk incident and in a form and manner prescribed by the state department of justice, a developer shall disclose to the department and all known recipients of the high-risk artificial intelligence system that is the basis of the risk incident the known and foreseeable risks of algorithmic discrimination that may arise from the intended uses of the system.</p>\n   <p class=\"indent\">B. Within ninety days of a request by the state department of justice, a developer shall submit to the department a copy of the summary and documentation the developer has made available to recipients pursuant to Section 3 of the Artificial Intelligence Act. A developer may designate the summary or documentation as including proprietary information or a trade secret. To the extent that information contained in the summary or documentation includes information subject to attorney-client privilege or work-product protection, compliance with this section does not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">C. As part of a disclosure, notice or submission pursuant to the Artificial Intelligence Act, a developer shall not be required to disclose a trade secret, information protected from disclosure by state or federal law or information that would create a security risk to the developer. Such disclosure, notice or submission shall be exempt from disclosure pursuant to the Inspection of Public Records Act.</p>\n   <p class=\"indent\">SECTION 5. [<u>NEW MATERIAL</u>] DEPLOYER RISK-MANAGEMENT POLICY REQUIRED.--</p>\n   <p class=\"indent\">A. A deployer shall use reasonable care to protect consumers from known or reasonably foreseeable risks of algorithmic discrimination.</p>\n   <p class=\"indent\">B. A deployer shall implement a risk management policy and program to govern the deployer&#39;s deployment of a high-risk artificial intelligence system. The risk management policy and program shall:</p>\n   <p class=\"indent\">(1) specify and incorporate the principles, processes and personnel that the deployer uses to identify, document and mitigate known or reasonably foreseeable risks of algorithmic discrimination; and</p>\n   <p class=\"indent\">(2) be an iterative process planned, implemented and regularly and systematically updated over the life cycle of a high-risk artificial intelligence system and include regular systematic review and updates.</p>\n   <p class=\"indent\">C. A risk management policy shall meet standards established by the state department of justice by rule.</p>\n   <p class=\"indent\">SECTION 6. [<u>NEW MATERIAL</u>] DEPLOYER IMPACT ASSESSMENTS.--</p>\n   <p class=\"indent\">A. Except as provided in Subsections D, E and H of this section, a deployer shall conduct an impact assessment for any high-risk artificial intelligence system deployed by the deployer:</p>\n   <p class=\"indent\">(1) annually; and</p>\n   <p class=\"indent\">(2) within ninety days of an intentional and substantial modification to the system.</p>\n   <p class=\"indent\">B. An impact assessment of a high-risk artificial intelligence system completed pursuant to this section shall include, to the extent reasonably known by or available to the deployer:</p>\n   <p class=\"indent\">(1) a statement of the intended uses, deployment contexts and benefits of the system;</p>\n   <p class=\"indent\">(2) an analysis of any known or reasonably foreseeable risks of algorithmic discrimination posed by the system, and when a risk exists, the nature of the algorithmic discrimination and the steps that have been taken to mitigate the risk;</p>\n   <p class=\"indent\">(3) a description of the categories of data the system processes as inputs and the outputs the system produces;</p>\n   <p class=\"indent\">(4) a summary of categories of any data used to customize the system;</p>\n   <p class=\"indent\">(5) the metrics used to evaluate the performance and known limitations of the system, including:</p>\n   <p class=\"indent\">(a) whether the evaluation was carried out using test data;</p>\n   <p class=\"indent\">(b) whether the test data sets were collected solely in New Mexico, solely in other states or in New Mexico in combination with other states;</p>\n   <p class=\"indent\">(c) the demographic groups represented in the test data sets and the proportion of each age, ethnic, gender or racial group in each data set; and</p>\n   <p class=\"indent\">(d) any independent studies carried out to evaluate the system for performance and risk of discrimination and whether the studies are publicly available or peer-reviewed;</p>\n   <p class=\"indent\">(6) a description of any transparency measures taken concerning the system, including measures taken to disclose to a consumer when the system is in use; and</p>\n   <p class=\"indent\">(7) a description of the post-deployment monitoring and user safeguards provided for the system, including oversight, use and learning processes used by the deployer to address issues arising from deployment of the system.</p>\n   <p class=\"indent\">C. An impact assessment conducted due to an intentional and substantial modification of a high-risk artificial intelligence system shall include a disclosure of the extent to which the system was used in a manner consistent with, or that varied from, the developer&#39;s intended uses of the system.</p>\n   <p class=\"indent\">D. A deployer may use a single impact assessment to address a set of comparable high-risk artificial intelligence systems.</p>\n   <p class=\"indent\">E. An impact assessment conducted for the purpose of complying with another applicable law or rule shall satisfy the requirement of this section when the assessment:</p>\n   <p class=\"indent\">(1) meets the requirements of this section; and</p>\n   <p class=\"indent\">(2) is reasonably similar in scope and effect to an assessment that would otherwise be conducted pursuant to this section.</p>\n   <p class=\"indent\">F. For at least three years following the final deployment of a high-risk artificial intelligence system, a deployer shall maintain records of the most recently conducted impact assessment for the system, including all records concerning the assessment and all prior assessments for the system.</p>\n   <p class=\"indent\">G. On or before March 1, 2027, a deployer shall review each high-risk artificial intelligence system that the deployer has deployed to ensure that the system is not causing algorithmic discrimination.</p>\n   <p class=\"indent\">H. This section is not applicable when:</p>\n   <p class=\"indent\">(1) a deployer using a high-risk artificial intelligence system:</p>\n   <p class=\"indent\">(a) employs fewer than fifty full-time employees;</p>\n   <p class=\"indent\">(b) does not use the deployer&#39;s own data to train the system;</p>\n   <p class=\"indent\">(c) uses the system solely for the system&#39;s intended uses as disclosed by a developer pursuant to the Artificial Intelligence Act; and</p>\n   <p class=\"indent\">(d) makes any impact assessment of the system that has been provided by the developer pursuant to the Artificial Intelligence Act available to consumers; and</p>\n   <p class=\"indent\">(2) the system continues learning based on data derived from sources other than the deployer&#39;s own data.</p>\n   <p class=\"indent\">SECTION 7. [<u>NEW MATERIAL</u>] DEPLOYER GENERAL NOTICE TO CONSUMERS.--</p>\n   <p class=\"indent\">A. A deployer shall make readily available to its consumers and on its website:</p>\n   <p class=\"indent\">(1) a summary of the types of high-risk artificial intelligence systems that the deployer currently deploys and how known or reasonably foreseeable risks of algorithmic discrimination from the deployment of each system are managed; and</p>\n   <p class=\"indent\">(2) a detailed explanation of the nature, source and extent of the information collected and used by the deployer.</p>\n   <p class=\"indent\">B. At a minimum, a deployer shall update the information posted on its website pursuant to this section annually and when the deployer deploys a new high-risk artificial intelligence system.</p>\n   <p class=\"indent\">SECTION 8. [<u>NEW MATERIAL</u>] USE OF ARTIFICIAL INTELLIGENCE SYSTEMS WHEN MAKING CONSEQUENTIAL DECISIONS--DIRECT NOTICE TO AFFECTED CONSUMERS--ADVERSE DECISIONS--OPPORTUNITY FOR APPEAL.--</p>\n   <p class=\"indent\">A. Except as provided in Subsection E of this section, before a high-risk artificial intelligence system is used to make or is a substantial factor in making a consequential decision concerning a consumer, a deployer shall provide directly to the consumer:</p>\n   <p class=\"indent\">(1) notice that the system will be used to make or be a substantial factor in making the decision; and</p>\n   <p class=\"indent\">(2) information describing:</p>\n   <p class=\"indent\">(a) the system and how to access the deployer&#39;s notice required pursuant to Section 7 of the Artificial Intelligence Act;</p>\n   <p class=\"indent\">(b) the purpose of the system and the nature of the consequential decision being made; and</p>\n   <p class=\"indent\">(c) the deployer&#39;s contact information.</p>\n   <p class=\"indent\">B. Except as provided in Subsection E of this section, when a high-risk artificial intelligence system has been used to make or has been a substantial factor in making a consequential decision concerning a consumer that is adverse to the consumer, the deployer shall provide directly to the consumer:</p>\n   <p class=\"indent\">(1) a statement explaining:</p>\n   <p class=\"indent\">(a) the principal reason or reasons for the decision;</p>\n   <p class=\"indent\">(b) the degree and manner in which the system contributed to the decision; and</p>\n   <p class=\"indent\">(c) the source and type of data that was processed by the system to make or that was a substantial factor in making the decision;</p>\n   <p class=\"indent\">(2) an opportunity to correct any incorrect personal data that the system processed to make or that was a substantial factor in making the decision; and</p>\n   <p class=\"indent\">(3) an opportunity to appeal the adverse decision except in instances where an appeal is not in the best interest of the consumer, such as creating a delay that may pose a risk of life or safety to the consumer.</p>\n   <p class=\"indent\">C. If technically feasible, an appeal of an adverse decision pursuant to this section shall allow for human review.</p>\n   <p class=\"indent\">D. All information, notices and statements to a consumer as required by this section shall be provided:</p>\n   <p class=\"indent\">(1) in plain language and in all languages in which the deployer in the ordinary course of business provides contracts, disclaimers, sale announcements and other information to consumers; and</p>\n   <p class=\"indent\">(2) in a format that is accessible to consumers with disabilities.</p>\n   <p class=\"indent\">E. When a deployer is unable to provide information, notice or a statement required pursuant to this section directly to a consumer, the deployer shall make such information, notices or statements available in a manner that is reasonably calculated to ensure that the consumer receives the information, notice or statement.</p>\n   <p class=\"indent\">SECTION 9. [<u>NEW MATERIAL</u>] USE OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM--NOTICE AND DISCLOSURE TO THE STATE DEPARTMENT OF JUSTICE--INSPECTION OF PUBLIC RECORDS ACT EXEMPTION.--</p>\n   <p class=\"indent\">A. When a deployer discovers that a high-risk artificial intelligence system that has been used has caused algorithmic discrimination, the deployer shall as expeditiously as possible but at a maximum within ninety days notify the state department of justice of the discovery. The notice shall be in a form and manner prescribed by the department.</p>\n   <p class=\"indent\">B. Upon request by the state department of justice, a deployer shall within ninety days submit to the state department of justice any risk management policy, impact assessment or records conducted, implemented, maintained or received pursuant to the Artificial Intelligence Act. The submission shall be in a form and manner prescribed by the department.</p>\n   <p class=\"indent\">C. The state department of justice may evaluate risk management policies, impact assessments or records submitted pursuant to this section for compliance with the Artificial Intelligence Act.</p>\n   <p class=\"indent\">D. A risk management policy, impact assessment or record submitted to the state department of justice pursuant to this section is exempt from disclosure pursuant to the Inspection of Public Records Act.</p>\n   <p class=\"indent\">E. In a submission pursuant to this section, a deployer may designate a portion of the submission as including proprietary information or a trade secret and to the extent that a submission contains information subject to attorney-client privilege or work-product protection, the submission does not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">SECTION 10. [<u>NEW MATERIAL</u>] INTERACTION OF ARTIFICIAL INTELLIGENCE SYSTEM WITH CONSUMERS--REQUIRED DISCLOSURE.--A developer that offers or makes available an artificial intelligence system intended to interact with consumers shall ensure that a consumer is informed that the consumer is interacting with an artificial intelligence system. This section does not apply when it would be obvious to a reasonable person that the consumer is interacting with an artificial intelligence system.</p>\n   <p class=\"indent\">SECTION 11. [<u>NEW MATERIAL</u>] EXEMPTION FROM DISCLOSURE--TRADE SECRETS AND OTHER INFORMATION PROTECTED BY LAW--NOTICE TO CONSUMER.--</p>\n   <p class=\"indent\">A. Nothing in the Artificial Intelligence Act shall require a deployer or developer to disclose a trade secret or other information protected from disclosure by state or federal law.</p>\n   <p class=\"indent\">B. To the extent that a deployer or developer withholds information pursuant to this section that would otherwise be part of a disclosure pursuant to the Artificial Intelligence Act, the deployer or developer shall notify a consumer and provide a basis for the withholding.</p>\n   <p class=\"indent\">SECTION 12. [<u>NEW MATERIAL</u>] APPLICABILITY EXEMPTIONS-- OTHER LAW--SECURITY AND TESTING--FEDERAL USE--INSURANCE PROVIDERS.--</p>\n   <p class=\"indent\">A. No provision of the Artificial Intelligence Act shall be construed to restrict a person&#39;s ability to:</p>\n   <p class=\"indent\">(1) comply with federal, state or municipal laws or regulations;</p>\n   <p class=\"indent\">(2) comply with a civil, criminal or regulatory inquiry, investigation, subpoena or summons by a governmental authority;</p>\n   <p class=\"indent\">(3) cooperate with a law enforcement agency concerning activity that the person reasonably and in good faith believes may violate other laws or regulations;</p>\n   <p class=\"indent\">(4) defend, exercise or investigate legal claims;</p>\n   <p class=\"indent\">(5) act to protect an interest that is essential for the life or physical safety of a person;</p>\n   <p class=\"indent\">(6) by means other than the use of facial recognition technology:</p>\n   <p class=\"indent\">(a) detect, prevent, protect against or respond to deceptive, illegal or malicious activity, fraud, identity theft, harassment or security incidents; or</p>\n   <p class=\"indent\">(b) investigate, prosecute or report persons responsible for the actions listed in Subparagraph (a) of this paragraph;</p>\n   <p class=\"indent\">(7) preserve the integrity or security of artificial intelligence, computer, electronic or internet connection systems;</p>\n   <p class=\"indent\">(8) engage in public or peer-reviewed scientific or statistical research that adheres to and is conducted in accordance with applicable federal and state law;</p>\n   <p class=\"indent\">(9) engage in pre-market testing other than testing conducted under real-world conditions, including development, research and testing of artificial intelligence systems; or</p>\n   <p class=\"indent\">(10) assist another person with compliance with the Artificial Intelligence Use Act.</p>\n   <p class=\"indent\">B. No provision of the Artificial Intelligence Act shall be construed to restrict:</p>\n   <p class=\"indent\">(1) a product recall; or</p>\n   <p class=\"indent\">(2) identification or repair of technical errors that impair the functionality of an artificial intelligence system.</p>\n   <p class=\"indent\">C. The Artificial Intelligence Act shall not apply in circumstances where compliance would violate an evidentiary privilege pursuant to law.</p>\n   <p class=\"indent\">D. No provision of the Artificial Intelligence Act shall be construed so as to limit a person&#39;s rights to free speech or freedom of the press pursuant to the first amendment to the United States constitution or Article 2, Section 17 of the constitution of New Mexico.</p>\n   <p class=\"indent\">E. The Artificial Intelligence Act shall not apply to a developer, deployer or other person who:</p>\n   <p class=\"indent\">(1) uses or intentionally and substantially modifies a high-risk artificial intelligence system that:</p>\n   <p class=\"indent\">(a) has been authorized by a federal agency in accordance with federal law; and</p>\n   <p class=\"indent\">(b) is in compliance with standards established by a federal agency in accordance with federal law when such standards are substantially equivalent or more stringent than the requirements of the Artificial Intelligence Act;</p>\n   <p class=\"indent\">(2) conducts research to support an application for certification or review by a federal agency pursuant to federal law;</p>\n   <p class=\"indent\">(3) performs work under or in connection with a contract with a federal agency, unless the work is on a high- risk artificial intelligence system used to make or as a substantial factor in making a decision concerning employment or housing; or</p>\n   <p class=\"indent\">(4) is a covered entity pursuant to federal health insurance law and is providing health care recommendations:</p>\n   <p class=\"indent\">(a) generated by an artificial intelligence system;</p>\n   <p class=\"indent\">(b) that require a health care provider to take action to implement the recommendations; and</p>\n   <p class=\"indent\">(c) that are not considered to be high risk.</p>\n   <p class=\"indent\">F. The Artificial Intelligence Act shall not apply to an artificial intelligence system acquired by the federal government, except for a high-risk artificial intelligence system used to make or as a substantial factor in making a decision concerning employment or housing.</p>\n   <p class=\"indent\">G. A financial institution or affiliate or subsidiary of a financial institution that is subject to prudential regulation by another state or by the federal government pursuant to laws that apply to the use of high-risk artificial intelligence systems shall be deemed to be in compliance with the Artificial Intelligence Act when the applicable laws:</p>\n   <p class=\"indent\">(1) impose requirements that are substantially equivalent to or more stringent than the requirements imposed by the Artificial Intelligence Act; and</p>\n   <p class=\"indent\">(2) at a minimum, require the financial institution to:</p>\n   <p class=\"indent\">(a) regularly audit the institution&#39;s use of high-risk artificial intelligence systems for compliance with state and federal antidiscrimination laws; and</p>\n   <p class=\"indent\">(b) mitigate any algorithmic discrimination caused by the use of a high-risk artificial intelligence system.</p>\n   <p class=\"indent\">H. As used in this section, &quot;financial institution&quot; means an insured state or national bank, a state or federal savings and loan association or savings bank, a state or federal credit union or authorized branches of each of the foregoing.</p>\n   <p class=\"indent\">I. A developer, deployer or other person who engages in an action pursuant to an exemption set forth in this section shall bear the burden of demonstrating that the action qualifies for the exemption.</p>\n   <p class=\"indent\">SECTION 13. [<u>NEW MATERIAL</u>] ENFORCEMENT--STATE DEPARTMENT OF JUSTICE--CONSUMER CIVIL ACTIONS.--</p>\n   <p class=\"indent\">A. Upon the promulgation of rules pursuant to Section 14 of the Artificial Intelligence Act:</p>\n   <p class=\"indent\">(1) the state department of justice shall have authority to enforce that act; and</p>\n   <p class=\"indent\">(2) a consumer may bring a civil action in district court against a developer or deployer for declaratory or injunctive relief and attorney fees for a violation of that act.</p>\n   <p class=\"indent\">B. In an action by the state department of justice to enforce the Artificial Intelligence Act, it is an affirmative defense when:</p>\n   <p class=\"indent\">(1) the developer, deployer or other person discovers and cures a violation of the Artificial Intelligence Act as a result of:</p>\n   <p class=\"indent\">(a) feedback that the developer, deployer or other person encourages the deployer or users to provide; or</p>\n   <p class=\"indent\">(b) adversarial testing, red teaming or an internal review process; and</p>\n   <p class=\"indent\">(2) the developer, deployer or other person is in compliance with a risk management framework for artificial intelligence systems designated by the state department of justice by rule.</p>\n   <p class=\"indent\">C. In an action by the state department of justice to enforce the Artificial Intelligence Act, the developer, deployer or other person who is the subject of the enforcement shall bear the burden of demonstrating that the requirements for an affirmative defense pursuant to this section have been met.</p>\n   <p class=\"indent\">D. Nothing within the Artificial Intelligence Act, including the enforcement authority granted to the state department of justice pursuant to this section, preempts or otherwise affects any right, claim, remedy, presumption or defense available in law or equity.</p>\n   <p class=\"indent\">E. An affirmative defense or rebuttable presumption established by the Artificial Intelligence Act applies only to an enforcement action by the state department of justice and does not apply to any right, claim, remedy, presumption or defense available in law or equity.</p>\n   <p class=\"indent\">F. A violation of the Artificial Intelligence Act is an unfair practice and may be enforced pursuant to the Unfair Practices Act.</p>\n   <p class=\"indent\">G. As used in this section:</p>\n   <p class=\"indent\">(1) &quot;adversarial testing&quot; means to proactively try to break an application by providing it with data most likely to elicit problematic output, or as defined by the state department of justice by rule; and</p>\n   <p class=\"indent\">(2) &quot;red teaming&quot; means the practice of simulating attack scenarios on an artificial intelligence application to pinpoint weaknesses and plan preventive measures or as defined by the state department of justice by rule.</p>\n   <p class=\"indent\">SECTION 14. [<u>NEW MATERIAL</u>] RULEMAKING.--On or before January 1, 2027, the state department of justice shall promulgate rules to implement the Artificial Intelligence Act and shall post them prominently on the state department of justice&#39;s website.</p>\n   <effective_clause>\n    <p class=\"indent\">SECTION 15. EFFECTIVE DATE.--The effective date of the provisions of this act is July 1, 2026.</p>\n   </effective_clause>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 NM H 60 | | Author: | Chandler  \n---|---  \nVersion: | Introduced  \nVersion Date: | 01/21/2025  \n  \nHOUSE BILL 60\n\n57th legislature - STATE OF NEW MEXICO - FIRST SESSION, 2025\n\nINTRODUCED BY\n\nChristine Chandler\n\nAN ACT\n\nRELATING TO ARTIFICIAL INTELLIGENCE; ENACTING THE ARTIFICIAL INTELLIGENCE ACT;\nREQUIRING NOTICE OF USE, DOCUMENTATION OF SYSTEMS, DISCLOSURE OF ALGORITHMIC\nDISCRIMINATION RISK AND RISK INCIDENTS; REQUIRING RISK MANAGEMENT POLICIES AND\nIMPACT ASSESSMENTS; PROVIDING FOR ENFORCEMENT BY THE STATE DEPARTMENT OF\nJUSTICE AND FOR CIVIL ACTIONS BY CONSUMERS FOR INJUNCTIVE OR DECLARATORY\nRELIEF.\n\nBE IT ENACTED BY THE LEGISLATURE OF THE STATE OF NEW MEXICO:\n\nSECTION 1. [_NEW MATERIAL_] SHORT TITLE.--This act may be cited as the\n\"Artificial Intelligence Act\".\n\nSECTION 2. [_NEW MATERIAL_] DEFINITIONS.--As used in the Artificial\nIntelligence Act:\n\nA. \"algorithmic discrimination\" means any condition in which the use of an\nartificial intelligence system results in an unlawful differential treatment\nor impact that disfavors a person on the basis of the person's actual or\nperceived age, color, disability, ethnicity, gender, genetic information,\nproficiency in the English language, national origin, race, religion,\nreproductive health, veteran status or other status protected by state or\nfederal law, but does not include:\n\n(1) the offer, license or use of a high-risk artificial intelligence system by\na developer or deployer for the sole purpose of:\n\n(a) the developer's or deployer's self-testing to identify, mitigate or ensure\ncompliance with state and federal law; or\n\n(b) expanding an applicant, customer or participant pool to increase diversity\nor redress historical discrimination; or\n\n(2) an act or omission by or on behalf of a private club or other entity that\nis not open to the public pursuant to federal law;\n\nB. \"artificial intelligence system\" means any machine-based system that for an\nexplicit or implicit objective infers from the inputs the system receives how\nto generate outputs, including content, decisions, predictions or\nrecommendations, that can influence physical or virtual environments;\n\nC. \"consequential decision\" means a decision that has a material legal or\nsimilarly significant effect on the provision or denial to a consumer of or\nthe cost or terms of:\n\n(1) education enrollment or an educational opportunity;\n\n(2) employment or an employment opportunity;\n\n(3) a financial or lending service;\n\n(4) health care service;\n\n(5) housing;\n\n(6) insurance; or\n\n(7) legal service;\n\nD. \"consumer\" means a resident of New Mexico;\n\nE. \"deploy\" means to use an artificial intelligence system;\n\nF. \"deployer\" means a person who deploys an artificial intelligence system;\n\nG. \"developer\" means a person who develops or intentionally and substantially\nmodifies an artificial intelligence system;\n\nH. \"health care services\" means treatment, services or research designed to\npromote the improved health of a person, including primary care, prenatal\ncare, dental care, behavioral health care, alcohol or drug detoxification and\nrehabilitation, hospital care, the provision of prescription drugs, preventive\ncare or health outreach;\n\nI. \"high-level summary\" means information about the data and data sets used to\ntrain the high-risk artificial intelligence system, including:\n\n(1) the sources or owners of the data sets and whether the data sets were\npurchased or licensed by the developer;\n\n(2) the factors in the data, including attributes or other information about a\nconsumer, that the system uses to produce its outputs, scores or\nrecommendations;\n\n(3) the demographic groups represented in the data sets and the proportion of\neach age, ethnic, gender or racial group in each dataset;\n\n(4) a description of the types of data points within the data sets, including,\nfor data sets that include labels, a description of the types of labels used;\n\n(5) whether the data sets include any data protected by copyright, trademark\nor patent or whether the data sets are entirely in the public domain;\n\n(6) whether there was any cleaning, processing or other modification to the\ndata sets by the developer, including the intended purpose of those efforts in\nrelation to the high-risk artificial intelligence system;\n\n(7) the time period during which the data in the data sets were collected,\nincluding a notice when data collection is ongoing;\n\n(8) the geographical regions or jurisdictions in which the data sets were\ncollected, including whether the data sets were collected solely in New\nMexico, solely in other states or in New Mexico in combination with other\nstates; and\n\n(9) other information as required by the state department of justice by rule;\n\nJ. \"high-risk artificial intelligence system\" means any artificial\nintelligence system that when deployed makes or is a substantial factor in\nmaking a consequential decision, but does not include:\n\n(1) an artificial intelligence system intended to:\n\n(a) perform a narrow procedural task; or\n\n(b) detect decision-making patterns or deviations from prior decision-making\npatterns and is not intended to replace or influence a previously completed\nhuman assessment without sufficient human review; or\n\n(2) the following technologies, unless the technologies make or are a\nsubstantial factor in making a consequential decision when the technologies\nare deployed:\n\n(a) anti-fraud technology that does not use facial recognition technology;\n\n(b) anti-malware;\n\n(c) antivirus;\n\n(d) artificial-intelligence-enabled video games;\n\n(e) calculators;\n\n(f) cybersecurity;\n\n(g) databases;\n\n(h) data storage;\n\n(i) firewalls;\n\n(j) internet domain registration;\n\n(k) internet website loading;\n\n(l) networking;\n\n(m) spam and robocall filtering;\n\n(n) spell checking;\n\n(o) spreadsheets;\n\n(p) web caching;\n\n(q) web hosting or similar technology; or\n\n(r) technology that communicates with consumers in natural language for the\npurpose of providing users with information, making referrals or\nrecommendations and answering questions and is subject to an accepted use\npolicy that prohibits generating content that is discriminatory or harmful;\n\nK. \"intentional and substantial modification\" and \"intentionally and\nsubstantially modifies\" means a deliberate change made to an artificial\nintelligence system that results in a new reasonably foreseeable risk of\nalgorithmic discrimination, but does not include a change made to a high-risk\nartificial intelligence system or the performance of a high-risk artificial\nintelligence system when:\n\n(1) the high-risk artificial intelligence system continues to learn after the\nsystem is:\n\n(a) offered, sold, leased, licensed, given or otherwise made available to a\ndeployer; or\n\n(b) deployed;\n\n(2) the change is made as a result of system learning after being made\navailable to a deployer or being deployed;\n\n(3) the change was predetermined by the deployer or a third party contracted\nby the deployer when the deployer or third party completed an impact\nassessment of the high-risk artificial intelligence system pursuant to Section\n6 of the Artificial Intelligence Act; or\n\n(4) the change is included in technical documentation for the high-risk\nartificial intelligence system;\n\nL. \"offered or made available\" includes a gift, lease, sale or other\nconveyance of an artificial intelligence system to a recipient deployer or a\ndeveloper other than the original system developer;\n\nM. \"recipient\" means a deployer who has received an artificial intelligence\nsystem from a developer or a developer who has received an artificial\nintelligence system from another developer;\n\nN. \"risk incident\" means an incident when a developer discovers or receives a\ncredible report from a deployer that a high-risk artificial intelligence\nsystem offered or made available by the developer has caused or is reasonably\nlikely to have caused algorithmic discrimination;\n\nO. \"substantial factor\" means:\n\n(1) a factor that:\n\n(a) assists in making a consequential decision;\n\n(b) is capable of altering, advising or influencing the outcome of a\nconsequential decision; and\n\n(c) is generated by an artificial intelligence system; or\n\n(2) content, decisions, labels, predictions, recommendations or scores\ngenerated by an artificial intelligence system concerning a consumer that are\nused as a basis, partial basis or recommendation to make a consequential\ndecision concerning the consumer; and\n\nP. \"trade secret\" means information, including a formula, pattern,\ncompilation, program, device, method, technique or process, that:\n\n(1) derives independent economic value, actual or potential, from not being\ngenerally known to and not being readily ascertainable by proper means by\nother persons who could obtain economic value from the information's\ndisclosure or use; and\n\n(2) is the subject of efforts that are reasonable under the circumstances to\nmaintain its secrecy.\n\nSECTION 3. [_NEW MATERIAL_] DUTY OF CARE--DISCLOSURE OF RISK POTENTIAL--\nPROVISION OF DOCUMENTATION.--A developer shall:\n\nA. use reasonable care to protect consumers from known or foreseeable risks of\nalgorithmic discrimination arising from intended and contracted uses of a\nhigh-risk artificial intelligence system;\n\nB. except for information excluded pursuant to Subsection C of Section 4 of\nthe Artificial Intelligence Act, make the following available to a recipient\nof the developer's high-risk artificial intelligence system:\n\n(1) a general summary describing the reasonably foreseeable uses and known\nharmful or inappropriate uses of the system; and\n\n(2) documentation disclosing:\n\n(a) the purpose, intended uses and benefits of the system;\n\n(b) a high-level summary of the type of data used to train the system;\n\n(c) known or reasonable foreseeable limitations of the system, including the\nrisk of algorithmic discrimination arising from the intended use of the\nsystem;\n\n(d) how the system was evaluated for performance and mitigation of algorithmic\ndiscrimination prior to being offered or made available to the deployer,\nincluding: 1) the metrics of performance and bias that were used; 2) how the\nmetrics were measured; 3) any independent studies carried out to evaluate the\nsystem for performance and risk of discrimination; and 4) whether the studies\nare publicly available or peer-reviewed;\n\n(e) the measures governing the data sets used to train the system, the\nsuitability of data sources, possible biases and bias mitigation;\n\n(f) the intended outputs of the system;\n\n(g) the measures the developer has taken to mitigate known or reasonably\nforeseeable risks of algorithmic discrimination that are reasonably\nforeseeable from the use of the system;\n\n(h) how the system should be used and monitored by the deployer;\n\n(i) any additional information that is reasonably necessary to assist the\ndeployer in understanding the outputs and monitoring the performance of the\nsystem for risks of algorithmic discrimination; and\n\n(j) any other information necessary to allow the deployer to comply with the\nrequirements of this section;\n\nC. except for information excluded pursuant to Subsection C of Section 4 of\nthe Artificial Intelligence Act, to the extent feasible make available to the\nrecipient the necessary information to conduct an impact assessment as\nrequired pursuant to Section 6 of the Artificial Intelligence Act. Such\ninformation shall include model cards, dataset cards or previous impact\nassessments relevant to the system, its development or use;\n\nD. post on the developer's website in a clear and readily available manner a\nstatement or public-use case inventory that summarizes:\n\n(1) the types of high-risk artificial intelligence systems that the developer\nhas developed or intentionally and substantially modified and currently offers\nor makes available to recipients; and\n\n(2) how the developer manages known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the use or intentional and\nsubstantial modification of the systems listed on the developer's website\npursuant to this subsection; and\n\nE. ensure that the statement or public-use case inventory posted pursuant to\nthis section remains accurate and is updated within ninety days of an\nintentional and substantial modification of a high-risk artificial\nintelligence system offered or made available by the developer to recipients.\n\nSECTION 4. [_NEW MATERIAL_] RISK INCIDENTS--REQUIRED DISCLOSURE AND SUBMISSION\n--EXCEPTIONS.--\n\nA. Within ninety days of a risk incident and in a form and manner prescribed\nby the state department of justice, a developer shall disclose to the\ndepartment and all known recipients of the high-risk artificial intelligence\nsystem that is the basis of the risk incident the known and foreseeable risks\nof algorithmic discrimination that may arise from the intended uses of the\nsystem.\n\nB. Within ninety days of a request by the state department of justice, a\ndeveloper shall submit to the department a copy of the summary and\ndocumentation the developer has made available to recipients pursuant to\nSection 3 of the Artificial Intelligence Act. A developer may designate the\nsummary or documentation as including proprietary information or a trade\nsecret. To the extent that information contained in the summary or\ndocumentation includes information subject to attorney-client privilege or\nwork-product protection, compliance with this section does not constitute a\nwaiver of the privilege or protection.\n\nC. As part of a disclosure, notice or submission pursuant to the Artificial\nIntelligence Act, a developer shall not be required to disclose a trade\nsecret, information protected from disclosure by state or federal law or\ninformation that would create a security risk to the developer. Such\ndisclosure, notice or submission shall be exempt from disclosure pursuant to\nthe Inspection of Public Records Act.\n\nSECTION 5. [_NEW MATERIAL_] DEPLOYER RISK-MANAGEMENT POLICY REQUIRED.--\n\nA. A deployer shall use reasonable care to protect consumers from known or\nreasonably foreseeable risks of algorithmic discrimination.\n\nB. A deployer shall implement a risk management policy and program to govern\nthe deployer's deployment of a high-risk artificial intelligence system. The\nrisk management policy and program shall:\n\n(1) specify and incorporate the principles, processes and personnel that the\ndeployer uses to identify, document and mitigate known or reasonably\nforeseeable risks of algorithmic discrimination; and\n\n(2) be an iterative process planned, implemented and regularly and\nsystematically updated over the life cycle of a high-risk artificial\nintelligence system and include regular systematic review and updates.\n\nC. A risk management policy shall meet standards established by the state\ndepartment of justice by rule.\n\nSECTION 6. [_NEW MATERIAL_] DEPLOYER IMPACT ASSESSMENTS.--\n\nA. Except as provided in Subsections D, E and H of this section, a deployer\nshall conduct an impact assessment for any high-risk artificial intelligence\nsystem deployed by the deployer:\n\n(1) annually; and\n\n(2) within ninety days of an intentional and substantial modification to the\nsystem.\n\nB. An impact assessment of a high-risk artificial intelligence system\ncompleted pursuant to this section shall include, to the extent reasonably\nknown by or available to the deployer:\n\n(1) a statement of the intended uses, deployment contexts and benefits of the\nsystem;\n\n(2) an analysis of any known or reasonably foreseeable risks of algorithmic\ndiscrimination posed by the system, and when a risk exists, the nature of the\nalgorithmic discrimination and the steps that have been taken to mitigate the\nrisk;\n\n(3) a description of the categories of data the system processes as inputs and\nthe outputs the system produces;\n\n(4) a summary of categories of any data used to customize the system;\n\n(5) the metrics used to evaluate the performance and known limitations of the\nsystem, including:\n\n(a) whether the evaluation was carried out using test data;\n\n(b) whether the test data sets were collected solely in New Mexico, solely in\nother states or in New Mexico in combination with other states;\n\n(c) the demographic groups represented in the test data sets and the\nproportion of each age, ethnic, gender or racial group in each data set; and\n\n(d) any independent studies carried out to evaluate the system for performance\nand risk of discrimination and whether the studies are publicly available or\npeer-reviewed;\n\n(6) a description of any transparency measures taken concerning the system,\nincluding measures taken to disclose to a consumer when the system is in use;\nand\n\n(7) a description of the post-deployment monitoring and user safeguards\nprovided for the system, including oversight, use and learning processes used\nby the deployer to address issues arising from deployment of the system.\n\nC. An impact assessment conducted due to an intentional and substantial\nmodification of a high-risk artificial intelligence system shall include a\ndisclosure of the extent to which the system was used in a manner consistent\nwith, or that varied from, the developer's intended uses of the system.\n\nD. A deployer may use a single impact assessment to address a set of\ncomparable high-risk artificial intelligence systems.\n\nE. An impact assessment conducted for the purpose of complying with another\napplicable law or rule shall satisfy the requirement of this section when the\nassessment:\n\n(1) meets the requirements of this section; and\n\n(2) is reasonably similar in scope and effect to an assessment that would\notherwise be conducted pursuant to this section.\n\nF. For at least three years following the final deployment of a high-risk\nartificial intelligence system, a deployer shall maintain records of the most\nrecently conducted impact assessment for the system, including all records\nconcerning the assessment and all prior assessments for the system.\n\nG. On or before March 1, 2027, a deployer shall review each high-risk\nartificial intelligence system that the deployer has deployed to ensure that\nthe system is not causing algorithmic discrimination.\n\nH. This section is not applicable when:\n\n(1) a deployer using a high-risk artificial intelligence system:\n\n(a) employs fewer than fifty full-time employees;\n\n(b) does not use the deployer's own data to train the system;\n\n(c) uses the system solely for the system's intended uses as disclosed by a\ndeveloper pursuant to the Artificial Intelligence Act; and\n\n(d) makes any impact assessment of the system that has been provided by the\ndeveloper pursuant to the Artificial Intelligence Act available to consumers;\nand\n\n(2) the system continues learning based on data derived from sources other\nthan the deployer's own data.\n\nSECTION 7. [_NEW MATERIAL_] DEPLOYER GENERAL NOTICE TO CONSUMERS.--\n\nA. A deployer shall make readily available to its consumers and on its\nwebsite:\n\n(1) a summary of the types of high-risk artificial intelligence systems that\nthe deployer currently deploys and how known or reasonably foreseeable risks\nof algorithmic discrimination from the deployment of each system are managed;\nand\n\n(2) a detailed explanation of the nature, source and extent of the information\ncollected and used by the deployer.\n\nB. At a minimum, a deployer shall update the information posted on its website\npursuant to this section annually and when the deployer deploys a new high-\nrisk artificial intelligence system.\n\nSECTION 8. [_NEW MATERIAL_] USE OF ARTIFICIAL INTELLIGENCE SYSTEMS WHEN MAKING\nCONSEQUENTIAL DECISIONS--DIRECT NOTICE TO AFFECTED CONSUMERS--ADVERSE\nDECISIONS--OPPORTUNITY FOR APPEAL.--\n\nA. Except as provided in Subsection E of this section, before a high-risk\nartificial intelligence system is used to make or is a substantial factor in\nmaking a consequential decision concerning a consumer, a deployer shall\nprovide directly to the consumer:\n\n(1) notice that the system will be used to make or be a substantial factor in\nmaking the decision; and\n\n(2) information describing:\n\n(a) the system and how to access the deployer's notice required pursuant to\nSection 7 of the Artificial Intelligence Act;\n\n(b) the purpose of the system and the nature of the consequential decision\nbeing made; and\n\n(c) the deployer's contact information.\n\nB. Except as provided in Subsection E of this section, when a high-risk\nartificial intelligence system has been used to make or has been a substantial\nfactor in making a consequential decision concerning a consumer that is\nadverse to the consumer, the deployer shall provide directly to the consumer:\n\n(1) a statement explaining:\n\n(a) the principal reason or reasons for the decision;\n\n(b) the degree and manner in which the system contributed to the decision; and\n\n(c) the source and type of data that was processed by the system to make or\nthat was a substantial factor in making the decision;\n\n(2) an opportunity to correct any incorrect personal data that the system\nprocessed to make or that was a substantial factor in making the decision; and\n\n(3) an opportunity to appeal the adverse decision except in instances where an\nappeal is not in the best interest of the consumer, such as creating a delay\nthat may pose a risk of life or safety to the consumer.\n\nC. If technically feasible, an appeal of an adverse decision pursuant to this\nsection shall allow for human review.\n\nD. All information, notices and statements to a consumer as required by this\nsection shall be provided:\n\n(1) in plain language and in all languages in which the deployer in the\nordinary course of business provides contracts, disclaimers, sale\nannouncements and other information to consumers; and\n\n(2) in a format that is accessible to consumers with disabilities.\n\nE. When a deployer is unable to provide information, notice or a statement\nrequired pursuant to this section directly to a consumer, the deployer shall\nmake such information, notices or statements available in a manner that is\nreasonably calculated to ensure that the consumer receives the information,\nnotice or statement.\n\nSECTION 9. [_NEW MATERIAL_] USE OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM--\nNOTICE AND DISCLOSURE TO THE STATE DEPARTMENT OF JUSTICE--INSPECTION OF PUBLIC\nRECORDS ACT EXEMPTION.--\n\nA. When a deployer discovers that a high-risk artificial intelligence system\nthat has been used has caused algorithmic discrimination, the deployer shall\nas expeditiously as possible but at a maximum within ninety days notify the\nstate department of justice of the discovery. The notice shall be in a form\nand manner prescribed by the department.\n\nB. Upon request by the state department of justice, a deployer shall within\nninety days submit to the state department of justice any risk management\npolicy, impact assessment or records conducted, implemented, maintained or\nreceived pursuant to the Artificial Intelligence Act. The submission shall be\nin a form and manner prescribed by the department.\n\nC. The state department of justice may evaluate risk management policies,\nimpact assessments or records submitted pursuant to this section for\ncompliance with the Artificial Intelligence Act.\n\nD. A risk management policy, impact assessment or record submitted to the\nstate department of justice pursuant to this section is exempt from disclosure\npursuant to the Inspection of Public Records Act.\n\nE. In a submission pursuant to this section, a deployer may designate a\nportion of the submission as including proprietary information or a trade\nsecret and to the extent that a submission contains information subject to\nattorney-client privilege or work-product protection, the submission does not\nconstitute a waiver of the privilege or protection.\n\nSECTION 10. [_NEW MATERIAL_] INTERACTION OF ARTIFICIAL INTELLIGENCE SYSTEM\nWITH CONSUMERS--REQUIRED DISCLOSURE.--A developer that offers or makes\navailable an artificial intelligence system intended to interact with\nconsumers shall ensure that a consumer is informed that the consumer is\ninteracting with an artificial intelligence system. This section does not\napply when it would be obvious to a reasonable person that the consumer is\ninteracting with an artificial intelligence system.\n\nSECTION 11. [_NEW MATERIAL_] EXEMPTION FROM DISCLOSURE--TRADE SECRETS AND\nOTHER INFORMATION PROTECTED BY LAW--NOTICE TO CONSUMER.--\n\nA. Nothing in the Artificial Intelligence Act shall require a deployer or\ndeveloper to disclose a trade secret or other information protected from\ndisclosure by state or federal law.\n\nB. To the extent that a deployer or developer withholds information pursuant\nto this section that would otherwise be part of a disclosure pursuant to the\nArtificial Intelligence Act, the deployer or developer shall notify a consumer\nand provide a basis for the withholding.\n\nSECTION 12. [_NEW MATERIAL_] APPLICABILITY EXEMPTIONS-- OTHER LAW--SECURITY\nAND TESTING--FEDERAL USE--INSURANCE PROVIDERS.--\n\nA. No provision of the Artificial Intelligence Act shall be construed to\nrestrict a person's ability to:\n\n(1) comply with federal, state or municipal laws or regulations;\n\n(2) comply with a civil, criminal or regulatory inquiry, investigation,\nsubpoena or summons by a governmental authority;\n\n(3) cooperate with a law enforcement agency concerning activity that the\nperson reasonably and in good faith believes may violate other laws or\nregulations;\n\n(4) defend, exercise or investigate legal claims;\n\n(5) act to protect an interest that is essential for the life or physical\nsafety of a person;\n\n(6) by means other than the use of facial recognition technology:\n\n(a) detect, prevent, protect against or respond to deceptive, illegal or\nmalicious activity, fraud, identity theft, harassment or security incidents;\nor\n\n(b) investigate, prosecute or report persons responsible for the actions\nlisted in Subparagraph (a) of this paragraph;\n\n(7) preserve the integrity or security of artificial intelligence, computer,\nelectronic or internet connection systems;\n\n(8) engage in public or peer-reviewed scientific or statistical research that\nadheres to and is conducted in accordance with applicable federal and state\nlaw;\n\n(9) engage in pre-market testing other than testing conducted under real-world\nconditions, including development, research and testing of artificial\nintelligence systems; or\n\n(10) assist another person with compliance with the Artificial Intelligence\nUse Act.\n\nB. No provision of the Artificial Intelligence Act shall be construed to\nrestrict:\n\n(1) a product recall; or\n\n(2) identification or repair of technical errors that impair the functionality\nof an artificial intelligence system.\n\nC. The Artificial Intelligence Act shall not apply in circumstances where\ncompliance would violate an evidentiary privilege pursuant to law.\n\nD. No provision of the Artificial Intelligence Act shall be construed so as to\nlimit a person's rights to free speech or freedom of the press pursuant to the\nfirst amendment to the United States constitution or Article 2, Section 17 of\nthe constitution of New Mexico.\n\nE. The Artificial Intelligence Act shall not apply to a developer, deployer or\nother person who:\n\n(1) uses or intentionally and substantially modifies a high-risk artificial\nintelligence system that:\n\n(a) has been authorized by a federal agency in accordance with federal law;\nand\n\n(b) is in compliance with standards established by a federal agency in\naccordance with federal law when such standards are substantially equivalent\nor more stringent than the requirements of the Artificial Intelligence Act;\n\n(2) conducts research to support an application for certification or review by\na federal agency pursuant to federal law;\n\n(3) performs work under or in connection with a contract with a federal\nagency, unless the work is on a high- risk artificial intelligence system used\nto make or as a substantial factor in making a decision concerning employment\nor housing; or\n\n(4) is a covered entity pursuant to federal health insurance law and is\nproviding health care recommendations:\n\n(a) generated by an artificial intelligence system;\n\n(b) that require a health care provider to take action to implement the\nrecommendations; and\n\n(c) that are not considered to be high risk.\n\nF. The Artificial Intelligence Act shall not apply to an artificial\nintelligence system acquired by the federal government, except for a high-risk\nartificial intelligence system used to make or as a substantial factor in\nmaking a decision concerning employment or housing.\n\nG. A financial institution or affiliate or subsidiary of a financial\ninstitution that is subject to prudential regulation by another state or by\nthe federal government pursuant to laws that apply to the use of high-risk\nartificial intelligence systems shall be deemed to be in compliance with the\nArtificial Intelligence Act when the applicable laws:\n\n(1) impose requirements that are substantially equivalent to or more stringent\nthan the requirements imposed by the Artificial Intelligence Act; and\n\n(2) at a minimum, require the financial institution to:\n\n(a) regularly audit the institution's use of high-risk artificial intelligence\nsystems for compliance with state and federal antidiscrimination laws; and\n\n(b) mitigate any algorithmic discrimination caused by the use of a high-risk\nartificial intelligence system.\n\nH. As used in this section, \"financial institution\" means an insured state or\nnational bank, a state or federal savings and loan association or savings\nbank, a state or federal credit union or authorized branches of each of the\nforegoing.\n\nI. A developer, deployer or other person who engages in an action pursuant to\nan exemption set forth in this section shall bear the burden of demonstrating\nthat the action qualifies for the exemption.\n\nSECTION 13. [_NEW MATERIAL_] ENFORCEMENT--STATE DEPARTMENT OF JUSTICE--\nCONSUMER CIVIL ACTIONS.--\n\nA. Upon the promulgation of rules pursuant to Section 14 of the Artificial\nIntelligence Act:\n\n(1) the state department of justice shall have authority to enforce that act;\nand\n\n(2) a consumer may bring a civil action in district court against a developer\nor deployer for declaratory or injunctive relief and attorney fees for a\nviolation of that act.\n\nB. In an action by the state department of justice to enforce the Artificial\nIntelligence Act, it is an affirmative defense when:\n\n(1) the developer, deployer or other person discovers and cures a violation of\nthe Artificial Intelligence Act as a result of:\n\n(a) feedback that the developer, deployer or other person encourages the\ndeployer or users to provide; or\n\n(b) adversarial testing, red teaming or an internal review process; and\n\n(2) the developer, deployer or other person is in compliance with a risk\nmanagement framework for artificial intelligence systems designated by the\nstate department of justice by rule.\n\nC. In an action by the state department of justice to enforce the Artificial\nIntelligence Act, the developer, deployer or other person who is the subject\nof the enforcement shall bear the burden of demonstrating that the\nrequirements for an affirmative defense pursuant to this section have been\nmet.\n\nD. Nothing within the Artificial Intelligence Act, including the enforcement\nauthority granted to the state department of justice pursuant to this section,\npreempts or otherwise affects any right, claim, remedy, presumption or defense\navailable in law or equity.\n\nE. An affirmative defense or rebuttable presumption established by the\nArtificial Intelligence Act applies only to an enforcement action by the state\ndepartment of justice and does not apply to any right, claim, remedy,\npresumption or defense available in law or equity.\n\nF. A violation of the Artificial Intelligence Act is an unfair practice and\nmay be enforced pursuant to the Unfair Practices Act.\n\nG. As used in this section:\n\n(1) \"adversarial testing\" means to proactively try to break an application by\nproviding it with data most likely to elicit problematic output, or as defined\nby the state department of justice by rule; and\n\n(2) \"red teaming\" means the practice of simulating attack scenarios on an\nartificial intelligence application to pinpoint weaknesses and plan preventive\nmeasures or as defined by the state department of justice by rule.\n\nSECTION 14. [_NEW MATERIAL_] RULEMAKING.--On or before January 1, 2027, the\nstate department of justice shall promulgate rules to implement the Artificial\nIntelligence Act and shall post them prominently on the state department of\njustice's website.\n\nSECTION 15. EFFECTIVE DATE.--The effective date of the provisions of this act\nis July 1, 2026.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": false
    },
    {
      "date": "02/25/2025",
      "label": "Substituted",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:NM2025000H60&verid=NM2025000H60_20250225_0_S&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 NM H 60</td> <td><table><tr><td class=\"label\">Author:</td> <td>Chandler</td></tr> <tr><td class=\"label\">Version:</td> <td>Substituted</td></tr> <tr><td class=\"label\">Version Date:</td> <td>02/25/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"center\">HOUSE JUDICIARY COMMITTEE SUBSTITUTE FOR</p>\n   <p class=\"center\">HOUSE BILL 60</p>\n   <p class=\"center\">57th legislature - STATE OF NEW MEXICO - FIRST SESSION, 2025</p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"center\">AN ACT</p>\n   <p class=\"left\">RELATING TO ARTIFICIAL INTELLIGENCE; ENACTING THE ARTIFICIAL INTELLIGENCE ACT; REQUIRING NOTICE OF USE, DOCUMENTATION OF SYSTEMS, DISCLOSURE OF ALGORITHMIC DISCRIMINATION RISK AND RISK INCIDENTS; REQUIRING RISK MANAGEMENT POLICIES AND IMPACT ASSESSMENTS; PROVIDING FOR ENFORCEMENT BY THE STATE DEPARTMENT OF JUSTICE AND FOR CIVIL ACTIONS BY CONSUMERS FOR INJUNCTIVE OR DECLARATORY RELIEF. </p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"left\">BE IT ENACTED BY THE LEGISLATURE OF THE STATE OF NEW MEXICO:</p>\n   </span>\n   <p class=\"indent\">SECTION 1. [<u>NEW MATERIAL</u>] SHORT TITLE.--This act may be cited as the &quot;Artificial Intelligence Act&quot;.</p>\n   <p class=\"indent\">SECTION 2. [<u>NEW MATERIAL</u>] DEFINITIONS.--As used in the Artificial Intelligence Act:</p>\n   <p class=\"indent\">A. &quot;algorithmic discrimination&quot; means any condition in which the use of an artificial intelligence system results in an unlawful differential treatment or impact that disfavors a person on the basis of the person&#39;s actual or perceived age, color, disability, ethnicity, gender, gender identity, genetic information, proficiency in the English language, national origin, race, religion, reproductive health, veteran status or other status protected by the New Mexico Civil Rights Act or federal law, but does not include:</p>\n   <p class=\"indent\">(1) the offer, license or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of:</p>\n   <p class=\"indent\">(a) the developer&#39;s or deployer&#39;s self-testing to identify, mitigate or ensure compliance with state and federal law; or</p>\n   <p class=\"indent\">(b) expanding an applicant, customer or participant pool to increase diversity or redress historical discrimination; or</p>\n   <p class=\"indent\">(2) an act or omission by or on behalf of a private club or other entity that is not open to the public pursuant to federal law;</p>\n   <p class=\"indent\">B. &quot;artificial intelligence system&quot; means a:</p>\n   <p class=\"indent\">(1) machine learning-based system that, for an objective, infers from the inputs the system receives how to generate outputs, including content, decisions, predictions and recommendations, that can influence physical or virtual environments; or</p>\n   <p class=\"indent\">(2) system that a developer markets or describes in its technical documentation as using artificial intelligence or machine learning;</p>\n   <p class=\"indent\">C. &quot;consequential decision&quot; means a decision that has a material legal or similarly significant effect on the provision or denial to a consumer of or the cost or terms of:</p>\n   <p class=\"indent\">(1) education enrollment;</p>\n   <p class=\"indent\">(2) employment or an employment opportunity;</p>\n   <p class=\"indent\">(3) a financial or lending service;</p>\n   <p class=\"indent\">(4) health care service;</p>\n   <p class=\"indent\">(5) housing;</p>\n   <p class=\"indent\">(6) insurance; or</p>\n   <p class=\"indent\">(7) legal service;</p>\n   <p class=\"indent\">D. &quot;consumer&quot; means a resident of New Mexico;</p>\n   <p class=\"indent\">E. &quot;department&quot; means the state department of justice;</p>\n   <p class=\"indent\">F. &quot;deploy&quot; means to put into effect, host or otherwise commercialize an artificial intelligence system;</p>\n   <p class=\"indent\">G. &quot;deployer&quot; means a person or public entity that deploys or uses a high-risk artificial intelligence system to make a consequential decision affecting a consumer in New Mexico;</p>\n   <p class=\"indent\">H. &quot;developer&quot; means a person or entity doing business in New Mexico that:</p>\n   <p class=\"indent\">(1) makes an artificial intelligence system publicly available for use in New Mexico;</p>\n   <p class=\"indent\">(2) intentionally and substantially modifies a high-risk artificial intelligence system that is used in New Mexico; or</p>\n   <p class=\"indent\">(3) intentionally and substantially modifies a non-high-risk artificial intelligence system so that it becomes a high-risk artificial intelligence system that is used in New Mexico;</p>\n   <p class=\"indent\">I. &quot;health care services&quot; means treatment or services designed to maintain and promote the improved health of a person, including primary care, prenatal care, dental care, behavioral health care, alcohol or drug detoxification and rehabilitation, enrollment in a clinical trial or similar activity, hospital care, hospice care, the provision of prescription drugs, preventive care or health outreach;</p>\n   <p class=\"indent\">J. &quot;high-level summary&quot; means information about the data and data sets used to train a high-risk artificial intelligence system, including:</p>\n   <p class=\"indent\">(1) the sources or owners of the data sets and whether the data sets were purchased or licensed by the developer;</p>\n   <p class=\"indent\">(2) the factors in the data, including attributes or other information about a consumer, that the system uses to produce its outputs, scores or recommendations;</p>\n   <p class=\"indent\">(3) the demographic groups represented in the data sets and the proportion of each age, ethnic, gender or racial group in each dataset;</p>\n   <p class=\"indent\">(4) a description of the types of data points within the data sets, including, for data sets that include labels, a description of the types of labels used;</p>\n   <p class=\"indent\">(5) whether the data sets include any data protected by copyright, trademark or patent or whether the data sets are entirely in the public domain;</p>\n   <p class=\"indent\">(6) whether there was any cleaning, processing or other modification to the data sets by the developer, including the intended purpose of those efforts in relation to the high-risk artificial intelligence system;</p>\n   <p class=\"indent\">(7) the time period during which the data in the data sets were collected, including a notice when data collection is ongoing;</p>\n   <p class=\"indent\">(8) the geographical regions or jurisdictions in which the data sets were collected, including whether the data sets were collected solely in New Mexico, solely in other states or in New Mexico in combination with other states; and</p>\n   <p class=\"indent\">(9) other information as required by the department by rule;</p>\n   <p class=\"indent\">K. &quot;high-risk artificial intelligence system&quot; means any artificial intelligence system that when deployed makes or is a substantial factor in making a consequential decision, but does not include:</p>\n   <p class=\"indent\">(1) anti-fraud technology;</p>\n   <p class=\"indent\">(2) anti-malware technology;</p>\n   <p class=\"indent\">(3) antivirus technology;</p>\n   <p class=\"indent\">(4) cybersecurity technology;</p>\n   <p class=\"indent\">(5) databases;</p>\n   <p class=\"indent\">(6) database, spreadsheet or other technology that does no more than organize data already in possession of a deployer;</p>\n   <p class=\"indent\">(7) data storage;</p>\n   <p class=\"indent\">(8) firewall technology;</p>\n   <p class=\"indent\">(9) internet domain registration;</p>\n   <p class=\"indent\">(10) internet website loading;</p>\n   <p class=\"indent\">(11) networking;</p>\n   <p class=\"indent\">(12) spam and robocall filtering;</p>\n   <p class=\"indent\">(13) spell checking technology;</p>\n   <p class=\"indent\">(14) transcription and transition technology;</p>\n   <p class=\"indent\">(15) web caching;</p>\n   <p class=\"indent\">(16) web hosting or similar technology; or</p>\n   <p class=\"indent\">(17) technology that communicates with consumers solely in spoken or written natural language for the purpose of providing consumers with information, making referrals or recommendations and answering questions:</p>\n   <p class=\"indent\">(a) subject to the deployer&#39;s accepted use policy as explicitly accepted by the consumer that may prohibit generation of specific content by the technology; and</p>\n   <p class=\"indent\">(b) that is not used to take any autonomous action without consumer intervention;</p>\n   <p class=\"indent\">L. &quot;intentional and substantial modification&quot; and &quot;intentionally and substantially modifies&quot; means a deliberate and material change made to an artificial intelligence system that results in a new reasonably foreseeable risk of algorithmic discrimination, but does not include a change made to a high-risk artificial intelligence system or the performance of a high-risk artificial intelligence system when:</p>\n   <p class=\"indent\">(1) the high-risk artificial intelligence system continues to learn after the system is:</p>\n   <p class=\"indent\">(a) offered, sold, leased, licensed, given or otherwise made available to a deployer; or</p>\n   <p class=\"indent\">(b) deployed;</p>\n   <p class=\"indent\">(2) the change is made as a result of system learning after being made available to a deployer or being deployed;</p>\n   <p class=\"indent\">(3) the change was predetermined by the deployer or a third party contracted by the deployer when the deployer or third party completed an impact assessment of the high-risk artificial intelligence system pursuant to Section 6 of the Artificial Intelligence Act; or</p>\n   <p class=\"indent\">(4) the change is included in technical documentation for the high-risk artificial intelligence system;</p>\n   <p class=\"indent\">M. &quot;machine learning&quot; means the development and incorporation of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction;</p>\n   <p class=\"indent\">N. &quot;offered or made available&quot; includes a gift, lease, sale or other conveyance of an artificial intelligence system to a recipient deployer or a developer other than the original system developer;</p>\n   <p class=\"indent\">O. &quot;recipient&quot; means a deployer who has received an artificial intelligence system from a developer or a developer who has received an artificial intelligence system from another developer;</p>\n   <p class=\"indent\">P. &quot;risk incident&quot; means an incident when a developer discovers or receives a credible report from a deployer that a high-risk artificial intelligence system offered or made available by the developer has caused or is reasonably likely to have caused algorithmic discrimination;</p>\n   <p class=\"indent\">Q. &quot;substantial factor&quot; means a decision, score, label, prediction or recommendation generated by an artificial intelligence system that is used as a basis or partial basis to make a consequential decision; and</p>\n   <p class=\"indent\">R. &quot;trade secret&quot; means information, including a formula, pattern, compilation, program, device, method, technique or process, that:</p>\n   <p class=\"indent\">(1) derives independent economic value, actual or potential, from not being generally known to and not being readily ascertainable by proper means by other persons who could obtain economic value from the information&#39;s disclosure or use; and</p>\n   <p class=\"indent\">(2) is the subject of efforts that are reasonable under the circumstances to maintain its secrecy.</p>\n   <p class=\"indent\">SECTION 3. [<u>NEW MATERIAL</u>] DUTY OF CARE--DISCLOSURE OF RISK POTENTIAL--PROVISION OF DOCUMENTATION.--A developer shall:</p>\n   <p class=\"indent\">A. use reasonable care to protect consumers from known or foreseeable risks of algorithmic discrimination arising from intended and contracted uses of a high-risk artificial intelligence system;</p>\n   <p class=\"indent\">B. except for information excluded pursuant to Subsection C of Section 4 of the Artificial Intelligence Act, make the following available to a recipient of the developer&#39;s high-risk artificial intelligence system:</p>\n   <p class=\"indent\">(1) a general summary describing the reasonably foreseeable uses and known harmful or inappropriate uses of the system; and</p>\n   <p class=\"indent\">(2) documentation disclosing:</p>\n   <p class=\"indent\">(a) the purpose, intended uses and benefits of the system;</p>\n   <p class=\"indent\">(b) a high-level summary of the types of data used to train the system;</p>\n   <p class=\"indent\">(c) known or reasonable foreseeable limitations of the system, including the risk of algorithmic discrimination arising from the intended use of the system;</p>\n   <p class=\"indent\">(d) how the system was evaluated for performance and mitigation of algorithmic discrimination prior to being offered or made available to the deployer, including: 1) the metrics of performance and bias that were used; 2) how the metrics were measured; 3) any independent studies carried out to evaluate the system for performance and risk of discrimination; and 4) whether the studies are publicly available or peer-reviewed;</p>\n   <p class=\"indent\">(e) the data governance measures used to cover the training datasets and the measures used to examine the suitability of data sources, possible biases and bias mitigation;</p>\n   <p class=\"indent\">(f) the intended outputs of the system;</p>\n   <p class=\"indent\">(g) the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that are reasonably foreseeable from the use of the system;</p>\n   <p class=\"indent\">(h) how the system should be used and monitored by the deployer;</p>\n   <p class=\"indent\">(i) any additional information that is reasonably necessary to assist the deployer in understanding the outputs and monitoring the performance of the system for risks of algorithmic discrimination; and</p>\n   <p class=\"indent\">(j) any other information necessary to allow the deployer to comply with the requirements of the Artificial Intelligence Act;</p>\n   <p class=\"indent\">C. except for information excluded pursuant to Subsection C of Section 4 of the Artificial Intelligence Act, to the extent feasible, make available to the deployer the necessary information to conduct an impact assessment as required pursuant to Section 6 of the Artificial Intelligence Act. The information shall include comprehensive information about the high-risk artificial intelligence system, including:</p>\n   <p class=\"indent\">(1) the name, version and a brief description of the system;</p>\n   <p class=\"indent\">(2) the intended use of the system;</p>\n   <p class=\"indent\">(3) information about the data set used to train the system, including all model input data and training data, demographic composition, data collection methods, data sources, preprocessing steps, potential biases and known limitations;</p>\n   <p class=\"indent\">(4) limitations or risks associated with the system&#39;s use; and</p>\n   <p class=\"indent\">(5) previous impact assessments relevant to the system, its development or use;</p>\n   <p class=\"indent\">D. post on the developer&#39;s website in a clear and readily available manner a statement or public-use case inventory that summarizes:</p>\n   <p class=\"indent\">(1) the types of high-risk artificial intelligence systems that the developer has developed or intentionally and substantially modified and currently offers or makes available to recipients; and</p>\n   <p class=\"indent\">(2) how the developer manages known or reasonably foreseeable risks of algorithmic discrimination that may arise from the use or intentional and substantial modification of the systems listed on the developer&#39;s website pursuant to this subsection; and</p>\n   <p class=\"indent\">E. ensure that the statement or public-use case inventory posted pursuant to this section remains accurate and is updated within ninety days of an intentional and substantial modification of a high-risk artificial intelligence system offered or made available by the developer to recipients.</p>\n   <p class=\"indent\">SECTION 4. [<u>NEW MATERIAL</u>] RISK INCIDENTS--REQUIRED DISCLOSURE AND SUBMISSION--EXCEPTIONS.--</p>\n   <p class=\"indent\">A. Within ninety days of a risk incident and in a form and manner prescribed by the department, a developer shall disclose to the department and all known recipients of the high-risk artificial intelligence system that is the basis of the risk incident the known and foreseeable risks of algorithmic discrimination that may arise from the intended uses of the system.</p>\n   <p class=\"indent\">B. Within ninety days of a request by the department, a developer shall submit to the department a copy of the summary and documentation the developer has made available to recipients pursuant to Section 3 of the Artificial Intelligence Act. A developer may designate the summary or documentation as including a trade secret. To the extent that information contained in the summary or documentation includes information subject to attorney-client privilege or work-product protection, compliance with this section does not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">C. As part of a disclosure, notice or submission pursuant to the Artificial Intelligence Act, a developer shall not be required to disclose a trade secret, information protected from disclosure by state or federal law or information that would create a security risk to the developer. Such disclosure, notice or submission shall be exempt from disclosure pursuant to the Inspection of Public Records Act.</p>\n   <p class=\"indent\">SECTION 5. [<u>NEW MATERIAL</u>] DEPLOYER RISK-MANAGEMENT POLICY REQUIRED.--</p>\n   <p class=\"indent\">A. A deployer shall use reasonable care to protect consumers from known or reasonably foreseeable risks of algorithmic discrimination.</p>\n   <p class=\"indent\">B. A deployer shall implement a risk management policy and program to govern the deployer&#39;s deployment of a high-risk artificial intelligence system. The risk management policy and program shall:</p>\n   <p class=\"indent\">(1) specify and incorporate the principles, processes and personnel that the deployer uses to identify, document and mitigate known or reasonably foreseeable risks of algorithmic discrimination; and</p>\n   <p class=\"indent\">(2) be an iterative process planned, implemented and regularly and systematically updated over the life cycle of a high-risk artificial intelligence system and include regular systematic review and updates.</p>\n   <p class=\"indent\">C. A risk management policy shall meet standards established by the department by rule.</p>\n   <p class=\"indent\">SECTION 6. [<u>NEW MATERIAL</u>] DEPLOYER IMPACT ASSESSMENTS.--</p>\n   <p class=\"indent\">A. Except as provided in Subsections D, E and H of this section, a deployer shall conduct an impact assessment for any high-risk artificial intelligence system deployed by the deployer:</p>\n   <p class=\"indent\">(1) annually; and</p>\n   <p class=\"indent\">(2) within ninety days of an intentional and substantial modification to the system.</p>\n   <p class=\"indent\">B. An impact assessment of a high-risk artificial intelligence system completed pursuant to this section shall include, to the extent reasonably known by or available to the deployer:</p>\n   <p class=\"indent\">(1) a statement of the intended uses, deployment contexts and benefits of the system;</p>\n   <p class=\"indent\">(2) an analysis of any known or reasonably foreseeable risks of algorithmic discrimination posed by the system and when:</p>\n   <p class=\"indent\">(a) a risk exists, the nature of the algorithmic discrimination and the steps that have been taken to mitigate the risk;</p>\n   <p class=\"indent\">(b) the impact assessment is dependent on developer information outside of the deployer&#39;s control, include a statement detailing that dependence; and</p>\n   <p class=\"indent\">(c) the deployer has cause to believe algorithmic discrimination exists, the deployer shall use reasonable efforts to mitigate the impacts of such discrimination;</p>\n   <p class=\"indent\">(3) a description of the categories of data the system processes as inputs and the outputs the system produces;</p>\n   <p class=\"indent\">(4) a summary of categories of any data used to customize the system;</p>\n   <p class=\"indent\">(5) the metrics used to evaluate the performance and known limitations of the system, including:</p>\n   <p class=\"indent\">(a) whether the evaluation was carried out using test data;</p>\n   <p class=\"indent\">(b) whether the test data sets were collected solely in New Mexico, solely in other states or in New Mexico in combination with other states;</p>\n   <p class=\"indent\">(c) the demographic groups represented in the test data sets and the proportion of each age, ethnic, gender or racial group in each data set; and</p>\n   <p class=\"indent\">(d) any independent studies carried out to evaluate the system for performance and risk of discrimination and whether the studies are publicly available or peer-reviewed;</p>\n   <p class=\"indent\">(6) a description of any transparency measures taken concerning the system, including measures taken to disclose to a consumer when the system is in use; and</p>\n   <p class=\"indent\">(7) a description of the post-deployment monitoring and user safeguards provided for the system, including oversight, use and learning processes used by the deployer to address issues arising from deployment of the system.</p>\n   <p class=\"indent\">C. An impact assessment conducted following an intentional and substantial modification of a high-risk artificial intelligence system shall include a disclosure of the extent to which the system was used in a manner consistent with, or that varied from, the developer&#39;s intended uses of the system.</p>\n   <p class=\"indent\">D. A deployer may use a single impact assessment to address a set of comparable high-risk artificial intelligence systems.</p>\n   <p class=\"indent\">E. An impact assessment conducted for the purpose of complying with another applicable law or rule shall satisfy the requirement of this section when the assessment:</p>\n   <p class=\"indent\">(1) meets the requirements of this section; and</p>\n   <p class=\"indent\">(2) is reasonably similar in scope and effect to an assessment that would otherwise be conducted pursuant to this section.</p>\n   <p class=\"indent\">F. For at least three years following the final deployment of a high-risk artificial intelligence system, a deployer shall maintain records of the most recently conducted impact assessment for the system, including all records concerning the assessment and all prior assessments for the system.</p>\n   <p class=\"indent\">G. One hundred twenty days after the department has promulgated rules pursuant to Section 14 of the Artificial Intelligence Act, a deployer shall review each high-risk artificial intelligence system that the deployer has deployed to ensure that the system is not causing algorithmic discrimination.</p>\n   <p class=\"indent\">H. This section is not applicable when:</p>\n   <p class=\"indent\">(1) a deployer using a high-risk artificial intelligence system:</p>\n   <p class=\"indent\">(a) impacts fewer than fifty consumers;</p>\n   <p class=\"indent\">(b) does not use the deployer&#39;s own data to train the system;</p>\n   <p class=\"indent\">(c) uses the system solely for the system&#39;s intended uses as disclosed by a developer pursuant to the Artificial Intelligence Act; and</p>\n   <p class=\"indent\">(d) makes any impact assessment of the system that has been provided by the developer pursuant to the Artificial Intelligence Act available to consumers; and</p>\n   <p class=\"indent\">(2) the system continues learning based on data derived from sources other than the deployer&#39;s own data.</p>\n   <p class=\"indent\">I. A deployer may supply documentation provided by a developer to complete the requirements for an item pursuant to Subsection B of this section; provided that the deployer has not modified the item.</p>\n   <p class=\"indent\">SECTION 7. [<u>NEW MATERIAL</u>] DEPLOYER GENERAL NOTICE TO CONSUMERS.--</p>\n   <p class=\"indent\">A. A deployer shall make readily available to its consumers and on its website:</p>\n   <p class=\"indent\">(1) a summary of the types of high-risk artificial intelligence systems that the deployer currently deploys and how known or reasonably foreseeable risks of algorithmic discrimination from the deployment of each system are managed; and</p>\n   <p class=\"indent\">(2) a detailed explanation of the nature, source and extent of the information collected and used by the deployer.</p>\n   <p class=\"indent\">B. At a minimum, a deployer shall update the information posted on its website pursuant to this section annually and when the deployer deploys a new high-risk artificial intelligence system.</p>\n   <p class=\"indent\">SECTION 8. [<u>NEW MATERIAL</u>] USE OF ARTIFICIAL INTELLIGENCE SYSTEMS WHEN MAKING CONSEQUENTIAL DECISIONS--DIRECT NOTICE TO AFFECTED CONSUMERS--ADVERSE DECISIONS--OPPORTUNITY FOR APPEAL.--</p>\n   <p class=\"indent\">A. Except as provided in Subsection E of this section, before a high-risk artificial intelligence system is used to make or is a substantial factor in making a consequential decision concerning a consumer, a deployer shall provide directly to the consumer:</p>\n   <p class=\"indent\">(1) notice that the system will be used to make or be a substantial factor in making the decision; and</p>\n   <p class=\"indent\">(2) information describing:</p>\n   <p class=\"indent\">(a) the system and how to access the deployer&#39;s notice required pursuant to Section 7 of the Artificial Intelligence Act;</p>\n   <p class=\"indent\">(b) the purpose of the system and the nature of the consequential decision being made; and</p>\n   <p class=\"indent\">(c) the deployer&#39;s contact information.</p>\n   <p class=\"indent\">B. Except as provided in Subsection E of this section, when a high-risk artificial intelligence system has been used to make or has been a substantial factor in making a consequential decision concerning a consumer that is adverse to the consumer, the deployer shall provide directly to the consumer:</p>\n   <p class=\"indent\">(1) a statement explaining:</p>\n   <p class=\"indent\">(a) the principal reason or reasons for the decision;</p>\n   <p class=\"indent\">(b) the degree and manner in which the system contributed to the decision; and</p>\n   <p class=\"indent\">(c) the source and type of data that was processed by the system to make or that was a substantial factor in making the decision;</p>\n   <p class=\"indent\">(2) an opportunity to correct any incorrect personal data that the system processed to make or that was a substantial factor in making the decision; and</p>\n   <p class=\"indent\">(3) an opportunity to appeal the adverse decision except in instances where an appeal may pose a risk of life or safety to the consumer.</p>\n   <p class=\"indent\">C. If technically feasible, an appeal of an adverse decision pursuant to this section shall allow for human review.</p>\n   <p class=\"indent\">D. All information, notices and statements to a consumer as required by this section shall be provided:</p>\n   <p class=\"indent\">(1) in plain language and in all languages in which the deployer in the ordinary course of business provides contracts, disclaimers, sale announcements and other information to consumers; and</p>\n   <p class=\"indent\">(2) in a format that is accessible to consumers with disabilities.</p>\n   <p class=\"indent\">E. When a deployer is unable to provide information, notice or a statement required pursuant to this section directly to a consumer, the deployer shall make such information, notices or statements available in a manner that is reasonably calculated to ensure that the consumer receives the information, notice or statement.</p>\n   <p class=\"indent\">SECTION 9. [<u>NEW MATERIAL</u>] USE OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM--NOTICE AND DISCLOSURE TO THE DEPARTMENT--INSPECTION OF PUBLIC RECORDS ACT EXEMPTION.--</p>\n   <p class=\"indent\">A. When a deployer discovers that a high-risk artificial intelligence system that has been used has caused algorithmic discrimination, the deployer shall as expeditiously as possible but at a maximum within ninety days notify the department of the discovery. The notice shall be in a form and manner prescribed by the department.</p>\n   <p class=\"indent\">B. Upon request by the department, a deployer shall within ninety days submit to the department any risk management policy, impact assessment or records conducted, implemented, maintained or received pursuant to the Artificial Intelligence Act. The submission shall be in a form and manner prescribed by the department.</p>\n   <p class=\"indent\">C. The department may evaluate risk management policies, impact assessments or records submitted pursuant to this section for compliance with the Artificial Intelligence Act.</p>\n   <p class=\"indent\">D. As part of a disclosure, notice or submission pursuant to the Artificial Intelligence Act, a deployer shall not be required to disclose a trade secret, information protected from disclosure by state or federal law or information that would create a security risk to the deployer. Such a disclosure, notice or submission shall be exempt from disclosure pursuant to the Inspection of Public Records Act.</p>\n   <p class=\"indent\">E. Within ninety days of a request by the department, a developer shall submit to the department a copy of the summary and documentation the developer has made available to recipients pursuant to Section 3 of the Artificial Intelligence Act. A developer may designate the summary or documentation as including a trade secret. To the extent that information contained in the summary or documentation includes information subject to attorney-client privilege or work-product protection, compliance with this section does not constitute a waiver of the privilege or protection.</p>\n   <p class=\"indent\">SECTION 10. [<u>NEW MATERIAL</u>] INTERACTION OF ARTIFICIAL INTELLIGENCE SYSTEM WITH CONSUMERS--REQUIRED DISCLOSURE.--</p>\n   <p class=\"indent\">A. A developer or a deployer that offers or makes available an artificial intelligence system intended to interact with consumers shall ensure that a consumer is informed that the consumer is interacting with an artificial intelligence system.</p>\n   <p class=\"indent\">B. Prior to deploying a high-risk artificial intelligence system to make, or be a substantial factor in making, a consequential decision concerning a consumer, a deployer shall notify the consumer that the high-risk artificial intelligence system is being deployed and of the system&#39;s role in making the consequential decision.</p>\n   <p class=\"indent\">SECTION 11. [<u>NEW MATERIAL</u>] EXEMPTION FROM DISCLOSURE--TRADE SECRETS AND OTHER INFORMATION PROTECTED BY LAW--NOTICE TO CONSUMER.--</p>\n   <p class=\"indent\">A. Nothing in the Artificial Intelligence Act shall require a deployer or developer to disclose a trade secret or other information protected from disclosure by state or federal law.</p>\n   <p class=\"indent\">B. To the extent that a deployer or developer withholds information pursuant to this section that would otherwise be part of a disclosure pursuant to the Artificial Intelligence Act, the deployer or developer shall notify all affected consumers, provide a basis for the withholding and include all information not protected as a trade secret pursuant to the Uniform Trade Secrets Act or other state or federal law.</p>\n   <p class=\"indent\">SECTION 12. [<u>NEW MATERIAL</u>] APPLICABILITY EXEMPTIONS-- OTHER LAW--SECURITY AND TESTING--FEDERAL USE--INSURANCE PROVIDERS.--</p>\n   <p class=\"indent\">A. No provision of the Artificial Intelligence Act shall be construed to restrict a person&#39;s ability to:</p>\n   <p class=\"indent\">(1) comply with federal, state or municipal laws or regulations;</p>\n   <p class=\"indent\">(2) comply with a civil, criminal or regulatory inquiry, investigation, subpoena or summons by a governmental authority;</p>\n   <p class=\"indent\">(3) cooperate with a law enforcement agency concerning activity that the person reasonably and in good faith believes may violate other laws or regulations;</p>\n   <p class=\"indent\">(4) defend, exercise or investigate legal claims;</p>\n   <p class=\"indent\">(5) act to protect an interest that is essential for the life or physical safety of a person;</p>\n   <p class=\"indent\">(6) by any means:</p>\n   <p class=\"indent\">(a) detect, prevent, protect against or respond to deceptive, illegal or malicious activity, fraud, identity theft, harassment or security incidents; or</p>\n   <p class=\"indent\">(b) investigate, prosecute or report persons responsible for the actions listed in Subparagraph (a) of this paragraph;</p>\n   <p class=\"indent\">(7) preserve the integrity or security of artificial intelligence, computer, electronic or internet connection systems;</p>\n   <p class=\"indent\">(8) engage in public or peer-reviewed scientific or statistical research, including clinical trials, that adheres to and is conducted in accordance with applicable federal and state law;</p>\n   <p class=\"indent\">(9) engage in pre-market testing other than testing conducted under real-world conditions, including development, research and testing of artificial intelligence systems; or</p>\n   <p class=\"indent\">(10) assist another person with compliance with the Artificial Intelligence Act.</p>\n   <p class=\"indent\">B. No provision of the Artificial Intelligence Act shall be construed to restrict:</p>\n   <p class=\"indent\">(1) a product recall; or</p>\n   <p class=\"indent\">(2) identification or repair of technical errors that impair the functionality of the artificial intelligence system.</p>\n   <p class=\"indent\">C. The Artificial Intelligence Act does not apply in circumstances in which compliance would violate an evidentiary privilege pursuant to law.</p>\n   <p class=\"indent\">D. No provision of the Artificial Intelligence Act shall be construed so as to limit the rights of a person, including the rights to free speech or freedom of the press pursuant to the first amendment to the United States constitution or Article 2, Section 17 of the constitution of New Mexico.</p>\n   <p class=\"indent\">E. The Artificial Intelligence Act does not apply to a developer, deployer or other person who:</p>\n   <p class=\"indent\">(1) uses or intentionally and substantially modifies a high-risk artificial intelligence system that:</p>\n   <p class=\"indent\">(a) has been authorized by a federal agency in accordance with federal law; and</p>\n   <p class=\"indent\">(b) is in compliance with standards established by a federal agency in accordance with federal law when such standards are substantially equivalent or more stringent than the requirements of the Artificial Intelligence Act;</p>\n   <p class=\"indent\">(2) conducts research to support an application for approval, certification or review by a federal agency pursuant to federal law; or</p>\n   <p class=\"indent\">(3) performs work under or in connection with a contract with a federal agency, unless the work is on a high- risk artificial intelligence system used to make or as a substantial factor in making a decision concerning employment or housing.</p>\n   <p class=\"indent\">F. The Artificial Intelligence Act does not apply to an artificial intelligence system to the extent the system is used by the federal government, except for a high-risk artificial intelligence system used to make or as a substantial factor in making a decision concerning employment or housing.</p>\n   <p class=\"indent\">G. A financial institution, an affiliate or a subsidiary of a financial institution or a service provider that is subject to prudential regulation by another state or by the federal government pursuant to laws that apply to the use of high-risk artificial intelligence systems shall be deemed to be in compliance with the Artificial Intelligence Act when the applicable laws:</p>\n   <p class=\"indent\">(1) impose requirements that are substantially equivalent to or more stringent than the requirements imposed by the Artificial Intelligence Act; and</p>\n   <p class=\"indent\">(2) at a minimum, require the financial institution, affiliate or service provider to:</p>\n   <p class=\"indent\">(a) notify consumers subject to the high-risk artificial intelligence system of the system&#39;s use and its role in consequential decisions;</p>\n   <p class=\"indent\">(b) regularly audit the institution&#39;s use of high-risk artificial intelligence systems for compliance with state and federal antidiscrimination laws; and</p>\n   <p class=\"indent\">(c) mitigate any algorithmic discrimination caused by the use of a high-risk artificial intelligence system.</p>\n   <p class=\"indent\">H. A developer, deployer or other person who engages in an action pursuant to an exemption set forth in this section shall bear the burden of demonstrating that the action qualifies for the exemption.</p>\n   <p class=\"indent\">I. As used in this section, &quot;financial institution&quot; means an insured state or national bank, a state or federal savings and loan association or savings bank, a state or federal credit union or authorized branches of each of the foregoing.</p>\n   <p class=\"indent\">SECTION 13. [<u>NEW MATERIAL</u>] ENFORCEMENT--DEPARTMENT--OPPORTUNITY TO CURE--CONSUMER CIVIL ACTIONS.--</p>\n   <p class=\"indent\">A. Upon the promulgation of rules pursuant to Section 14 of the Artificial Intelligence Act:</p>\n   <p class=\"indent\">(1) the department shall have authority to enforce that act; and</p>\n   <p class=\"indent\">(2) a consumer may bring a civil action in district court against a developer or deployer for declaratory or injunctive relief and attorney fees for a violation of that act.</p>\n   <p class=\"indent\">B. Prior to the promulgation of rules by the department pursuant to Section 14 of the Artificial Intelligence Act, the department shall issue a notice to a prospective defendant prior to initiating an action for violation of the act. The notice shall include a detailed description of the alleged violation and the actions required to cure the violation. The prospective defendant shall have ninety days from the receipt of the notice to submit evidence satisfactory to the department that the violation has been cured. Ninety-one days after the prospective defendant has received the notice, if the department has not received satisfactory evidence that the violation has been cured, the department may file an action in district court for the violation.</p>\n   <p class=\"indent\">C. For one calendar year from the date the department promulgates rules pursuant to Section 14 of the Artificial Intelligence Act, it shall be an affirmative defense in an action brought by the department to enforce the Artificial Intelligence Act when:</p>\n   <p class=\"indent\">(1) the developer, deployer or other person discovers a violation of the Artificial Intelligence Act as a result of adversarial testing, red teaming or an internal review process;</p>\n   <p class=\"indent\">(2) the developer, deployer or other person reports the violation to the department and cures the violation within seven days of the violation;</p>\n   <p class=\"indent\">(3) the developer, deployer or other person is in compliance with a risk management framework for artificial intelligence systems designated by the department by rule;</p>\n   <p class=\"indent\">(4) the deployer is dependent on documentation from the developer to cure or otherwise resolve a violation and the deployer complies with the requirements in Paragraph (2) of Subsection B of Section 6 of the Artificial Intelligence Act; and</p>\n   <p class=\"indent\">(5) the developer, deployer or other person demonstrates that the violation was inadvertent, affected fewer than one hundred consumers and could not have been discovered through reasonable diligence.</p>\n   <p class=\"indent\">D. After one calendar year from the date the department promulgates rules pursuant to Section 14 of the Artificial Intelligence Act, a deployer, developer or other person subject to enforcement for a violation of that act shall have no right to cure the violation or an affirmative defense pursuant to this section.</p>\n   <p class=\"indent\">E. In an action by the department to enforce the Artificial Intelligence Act, the developer, deployer or other person who is the subject of the enforcement shall bear the burden of demonstrating that the requirements for an affirmative defense pursuant to this section have been met.</p>\n   <p class=\"indent\">F. Nothing in the Artificial Intelligence Act, including the enforcement authority granted to the department pursuant to this section, preempts or otherwise affects any right, claim, remedy, presumption or defense available in law or equity.</p>\n   <p class=\"indent\">G. An affirmative defense presumption established by the Artificial Intelligence Act applies only to an enforcement action by the department and does not apply to any right, claim, remedy, presumption or defense available in law or equity.</p>\n   <p class=\"indent\">H. A violation of the Artificial Intelligence Act is an unfair practice and may be enforced pursuant to the Unfair Practices Act.</p>\n   <p class=\"indent\">I. As used in this section:</p>\n   <p class=\"indent\">(1) &quot;adversarial testing&quot; means to proactively try to break an application by providing it with data most likely to elicit problematic output, or as defined by the department by rule; and</p>\n   <p class=\"indent\">(2) &quot;red teaming&quot; means the practice of simulating attack scenarios on an artificial intelligence application to pinpoint weaknesses and plan preventive measures or as defined by the department by rule.</p>\n   <p class=\"indent\">SECTION 14. [<u>NEW MATERIAL</u>] RULEMAKING.--</p>\n   <p class=\"indent\">A. On or before January 1, 2027, the department shall promulgate rules to implement the Artificial Intelligence Act and shall post them prominently on the department&#39;s website.</p>\n   <p class=\"indent\">B. The department shall consult artificial intelligence experts, academic researchers, civil rights organizations, deployers, developers, labor unions and organizations representing the interests of consumers when developing the rules to be promulgated pursuant to the Artificial Intelligence Act.</p>\n   <effective_clause>\n    <p class=\"indent\">SECTION 15. EFFECTIVE DATE.--The effective date of the provisions of this act is July 1, 2026.</p>\n   </effective_clause>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 NM H 60 | | Author: | Chandler  \n---|---  \nVersion: | Substituted  \nVersion Date: | 02/25/2025  \n  \nHOUSE JUDICIARY COMMITTEE SUBSTITUTE FOR\n\nHOUSE BILL 60\n\n57th legislature - STATE OF NEW MEXICO - FIRST SESSION, 2025\n\nAN ACT\n\nRELATING TO ARTIFICIAL INTELLIGENCE; ENACTING THE ARTIFICIAL INTELLIGENCE ACT;\nREQUIRING NOTICE OF USE, DOCUMENTATION OF SYSTEMS, DISCLOSURE OF ALGORITHMIC\nDISCRIMINATION RISK AND RISK INCIDENTS; REQUIRING RISK MANAGEMENT POLICIES AND\nIMPACT ASSESSMENTS; PROVIDING FOR ENFORCEMENT BY THE STATE DEPARTMENT OF\nJUSTICE AND FOR CIVIL ACTIONS BY CONSUMERS FOR INJUNCTIVE OR DECLARATORY\nRELIEF.\n\nBE IT ENACTED BY THE LEGISLATURE OF THE STATE OF NEW MEXICO:\n\nSECTION 1. [_NEW MATERIAL_] SHORT TITLE.--This act may be cited as the\n\"Artificial Intelligence Act\".\n\nSECTION 2. [_NEW MATERIAL_] DEFINITIONS.--As used in the Artificial\nIntelligence Act:\n\nA. \"algorithmic discrimination\" means any condition in which the use of an\nartificial intelligence system results in an unlawful differential treatment\nor impact that disfavors a person on the basis of the person's actual or\nperceived age, color, disability, ethnicity, gender, gender identity, genetic\ninformation, proficiency in the English language, national origin, race,\nreligion, reproductive health, veteran status or other status protected by the\nNew Mexico Civil Rights Act or federal law, but does not include:\n\n(1) the offer, license or use of a high-risk artificial intelligence system by\na developer or deployer for the sole purpose of:\n\n(a) the developer's or deployer's self-testing to identify, mitigate or ensure\ncompliance with state and federal law; or\n\n(b) expanding an applicant, customer or participant pool to increase diversity\nor redress historical discrimination; or\n\n(2) an act or omission by or on behalf of a private club or other entity that\nis not open to the public pursuant to federal law;\n\nB. \"artificial intelligence system\" means a:\n\n(1) machine learning-based system that, for an objective, infers from the\ninputs the system receives how to generate outputs, including content,\ndecisions, predictions and recommendations, that can influence physical or\nvirtual environments; or\n\n(2) system that a developer markets or describes in its technical\ndocumentation as using artificial intelligence or machine learning;\n\nC. \"consequential decision\" means a decision that has a material legal or\nsimilarly significant effect on the provision or denial to a consumer of or\nthe cost or terms of:\n\n(1) education enrollment;\n\n(2) employment or an employment opportunity;\n\n(3) a financial or lending service;\n\n(4) health care service;\n\n(5) housing;\n\n(6) insurance; or\n\n(7) legal service;\n\nD. \"consumer\" means a resident of New Mexico;\n\nE. \"department\" means the state department of justice;\n\nF. \"deploy\" means to put into effect, host or otherwise commercialize an\nartificial intelligence system;\n\nG. \"deployer\" means a person or public entity that deploys or uses a high-risk\nartificial intelligence system to make a consequential decision affecting a\nconsumer in New Mexico;\n\nH. \"developer\" means a person or entity doing business in New Mexico that:\n\n(1) makes an artificial intelligence system publicly available for use in New\nMexico;\n\n(2) intentionally and substantially modifies a high-risk artificial\nintelligence system that is used in New Mexico; or\n\n(3) intentionally and substantially modifies a non-high-risk artificial\nintelligence system so that it becomes a high-risk artificial intelligence\nsystem that is used in New Mexico;\n\nI. \"health care services\" means treatment or services designed to maintain and\npromote the improved health of a person, including primary care, prenatal\ncare, dental care, behavioral health care, alcohol or drug detoxification and\nrehabilitation, enrollment in a clinical trial or similar activity, hospital\ncare, hospice care, the provision of prescription drugs, preventive care or\nhealth outreach;\n\nJ. \"high-level summary\" means information about the data and data sets used to\ntrain a high-risk artificial intelligence system, including:\n\n(1) the sources or owners of the data sets and whether the data sets were\npurchased or licensed by the developer;\n\n(2) the factors in the data, including attributes or other information about a\nconsumer, that the system uses to produce its outputs, scores or\nrecommendations;\n\n(3) the demographic groups represented in the data sets and the proportion of\neach age, ethnic, gender or racial group in each dataset;\n\n(4) a description of the types of data points within the data sets, including,\nfor data sets that include labels, a description of the types of labels used;\n\n(5) whether the data sets include any data protected by copyright, trademark\nor patent or whether the data sets are entirely in the public domain;\n\n(6) whether there was any cleaning, processing or other modification to the\ndata sets by the developer, including the intended purpose of those efforts in\nrelation to the high-risk artificial intelligence system;\n\n(7) the time period during which the data in the data sets were collected,\nincluding a notice when data collection is ongoing;\n\n(8) the geographical regions or jurisdictions in which the data sets were\ncollected, including whether the data sets were collected solely in New\nMexico, solely in other states or in New Mexico in combination with other\nstates; and\n\n(9) other information as required by the department by rule;\n\nK. \"high-risk artificial intelligence system\" means any artificial\nintelligence system that when deployed makes or is a substantial factor in\nmaking a consequential decision, but does not include:\n\n(1) anti-fraud technology;\n\n(2) anti-malware technology;\n\n(3) antivirus technology;\n\n(4) cybersecurity technology;\n\n(5) databases;\n\n(6) database, spreadsheet or other technology that does no more than organize\ndata already in possession of a deployer;\n\n(7) data storage;\n\n(8) firewall technology;\n\n(9) internet domain registration;\n\n(10) internet website loading;\n\n(11) networking;\n\n(12) spam and robocall filtering;\n\n(13) spell checking technology;\n\n(14) transcription and transition technology;\n\n(15) web caching;\n\n(16) web hosting or similar technology; or\n\n(17) technology that communicates with consumers solely in spoken or written\nnatural language for the purpose of providing consumers with information,\nmaking referrals or recommendations and answering questions:\n\n(a) subject to the deployer's accepted use policy as explicitly accepted by\nthe consumer that may prohibit generation of specific content by the\ntechnology; and\n\n(b) that is not used to take any autonomous action without consumer\nintervention;\n\nL. \"intentional and substantial modification\" and \"intentionally and\nsubstantially modifies\" means a deliberate and material change made to an\nartificial intelligence system that results in a new reasonably foreseeable\nrisk of algorithmic discrimination, but does not include a change made to a\nhigh-risk artificial intelligence system or the performance of a high-risk\nartificial intelligence system when:\n\n(1) the high-risk artificial intelligence system continues to learn after the\nsystem is:\n\n(a) offered, sold, leased, licensed, given or otherwise made available to a\ndeployer; or\n\n(b) deployed;\n\n(2) the change is made as a result of system learning after being made\navailable to a deployer or being deployed;\n\n(3) the change was predetermined by the deployer or a third party contracted\nby the deployer when the deployer or third party completed an impact\nassessment of the high-risk artificial intelligence system pursuant to Section\n6 of the Artificial Intelligence Act; or\n\n(4) the change is included in technical documentation for the high-risk\nartificial intelligence system;\n\nM. \"machine learning\" means the development and incorporation of algorithms to\nbuild data-derived statistical models that are capable of drawing inferences\nfrom previously unseen data without explicit human instruction;\n\nN. \"offered or made available\" includes a gift, lease, sale or other\nconveyance of an artificial intelligence system to a recipient deployer or a\ndeveloper other than the original system developer;\n\nO. \"recipient\" means a deployer who has received an artificial intelligence\nsystem from a developer or a developer who has received an artificial\nintelligence system from another developer;\n\nP. \"risk incident\" means an incident when a developer discovers or receives a\ncredible report from a deployer that a high-risk artificial intelligence\nsystem offered or made available by the developer has caused or is reasonably\nlikely to have caused algorithmic discrimination;\n\nQ. \"substantial factor\" means a decision, score, label, prediction or\nrecommendation generated by an artificial intelligence system that is used as\na basis or partial basis to make a consequential decision; and\n\nR. \"trade secret\" means information, including a formula, pattern,\ncompilation, program, device, method, technique or process, that:\n\n(1) derives independent economic value, actual or potential, from not being\ngenerally known to and not being readily ascertainable by proper means by\nother persons who could obtain economic value from the information's\ndisclosure or use; and\n\n(2) is the subject of efforts that are reasonable under the circumstances to\nmaintain its secrecy.\n\nSECTION 3. [_NEW MATERIAL_] DUTY OF CARE--DISCLOSURE OF RISK POTENTIAL--\nPROVISION OF DOCUMENTATION.--A developer shall:\n\nA. use reasonable care to protect consumers from known or foreseeable risks of\nalgorithmic discrimination arising from intended and contracted uses of a\nhigh-risk artificial intelligence system;\n\nB. except for information excluded pursuant to Subsection C of Section 4 of\nthe Artificial Intelligence Act, make the following available to a recipient\nof the developer's high-risk artificial intelligence system:\n\n(1) a general summary describing the reasonably foreseeable uses and known\nharmful or inappropriate uses of the system; and\n\n(2) documentation disclosing:\n\n(a) the purpose, intended uses and benefits of the system;\n\n(b) a high-level summary of the types of data used to train the system;\n\n(c) known or reasonable foreseeable limitations of the system, including the\nrisk of algorithmic discrimination arising from the intended use of the\nsystem;\n\n(d) how the system was evaluated for performance and mitigation of algorithmic\ndiscrimination prior to being offered or made available to the deployer,\nincluding: 1) the metrics of performance and bias that were used; 2) how the\nmetrics were measured; 3) any independent studies carried out to evaluate the\nsystem for performance and risk of discrimination; and 4) whether the studies\nare publicly available or peer-reviewed;\n\n(e) the data governance measures used to cover the training datasets and the\nmeasures used to examine the suitability of data sources, possible biases and\nbias mitigation;\n\n(f) the intended outputs of the system;\n\n(g) the measures the developer has taken to mitigate known or reasonably\nforeseeable risks of algorithmic discrimination that are reasonably\nforeseeable from the use of the system;\n\n(h) how the system should be used and monitored by the deployer;\n\n(i) any additional information that is reasonably necessary to assist the\ndeployer in understanding the outputs and monitoring the performance of the\nsystem for risks of algorithmic discrimination; and\n\n(j) any other information necessary to allow the deployer to comply with the\nrequirements of the Artificial Intelligence Act;\n\nC. except for information excluded pursuant to Subsection C of Section 4 of\nthe Artificial Intelligence Act, to the extent feasible, make available to the\ndeployer the necessary information to conduct an impact assessment as required\npursuant to Section 6 of the Artificial Intelligence Act. The information\nshall include comprehensive information about the high-risk artificial\nintelligence system, including:\n\n(1) the name, version and a brief description of the system;\n\n(2) the intended use of the system;\n\n(3) information about the data set used to train the system, including all\nmodel input data and training data, demographic composition, data collection\nmethods, data sources, preprocessing steps, potential biases and known\nlimitations;\n\n(4) limitations or risks associated with the system's use; and\n\n(5) previous impact assessments relevant to the system, its development or\nuse;\n\nD. post on the developer's website in a clear and readily available manner a\nstatement or public-use case inventory that summarizes:\n\n(1) the types of high-risk artificial intelligence systems that the developer\nhas developed or intentionally and substantially modified and currently offers\nor makes available to recipients; and\n\n(2) how the developer manages known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the use or intentional and\nsubstantial modification of the systems listed on the developer's website\npursuant to this subsection; and\n\nE. ensure that the statement or public-use case inventory posted pursuant to\nthis section remains accurate and is updated within ninety days of an\nintentional and substantial modification of a high-risk artificial\nintelligence system offered or made available by the developer to recipients.\n\nSECTION 4. [_NEW MATERIAL_] RISK INCIDENTS--REQUIRED DISCLOSURE AND SUBMISSION\n--EXCEPTIONS.--\n\nA. Within ninety days of a risk incident and in a form and manner prescribed\nby the department, a developer shall disclose to the department and all known\nrecipients of the high-risk artificial intelligence system that is the basis\nof the risk incident the known and foreseeable risks of algorithmic\ndiscrimination that may arise from the intended uses of the system.\n\nB. Within ninety days of a request by the department, a developer shall submit\nto the department a copy of the summary and documentation the developer has\nmade available to recipients pursuant to Section 3 of the Artificial\nIntelligence Act. A developer may designate the summary or documentation as\nincluding a trade secret. To the extent that information contained in the\nsummary or documentation includes information subject to attorney-client\nprivilege or work-product protection, compliance with this section does not\nconstitute a waiver of the privilege or protection.\n\nC. As part of a disclosure, notice or submission pursuant to the Artificial\nIntelligence Act, a developer shall not be required to disclose a trade\nsecret, information protected from disclosure by state or federal law or\ninformation that would create a security risk to the developer. Such\ndisclosure, notice or submission shall be exempt from disclosure pursuant to\nthe Inspection of Public Records Act.\n\nSECTION 5. [_NEW MATERIAL_] DEPLOYER RISK-MANAGEMENT POLICY REQUIRED.--\n\nA. A deployer shall use reasonable care to protect consumers from known or\nreasonably foreseeable risks of algorithmic discrimination.\n\nB. A deployer shall implement a risk management policy and program to govern\nthe deployer's deployment of a high-risk artificial intelligence system. The\nrisk management policy and program shall:\n\n(1) specify and incorporate the principles, processes and personnel that the\ndeployer uses to identify, document and mitigate known or reasonably\nforeseeable risks of algorithmic discrimination; and\n\n(2) be an iterative process planned, implemented and regularly and\nsystematically updated over the life cycle of a high-risk artificial\nintelligence system and include regular systematic review and updates.\n\nC. A risk management policy shall meet standards established by the department\nby rule.\n\nSECTION 6. [_NEW MATERIAL_] DEPLOYER IMPACT ASSESSMENTS.--\n\nA. Except as provided in Subsections D, E and H of this section, a deployer\nshall conduct an impact assessment for any high-risk artificial intelligence\nsystem deployed by the deployer:\n\n(1) annually; and\n\n(2) within ninety days of an intentional and substantial modification to the\nsystem.\n\nB. An impact assessment of a high-risk artificial intelligence system\ncompleted pursuant to this section shall include, to the extent reasonably\nknown by or available to the deployer:\n\n(1) a statement of the intended uses, deployment contexts and benefits of the\nsystem;\n\n(2) an analysis of any known or reasonably foreseeable risks of algorithmic\ndiscrimination posed by the system and when:\n\n(a) a risk exists, the nature of the algorithmic discrimination and the steps\nthat have been taken to mitigate the risk;\n\n(b) the impact assessment is dependent on developer information outside of the\ndeployer's control, include a statement detailing that dependence; and\n\n(c) the deployer has cause to believe algorithmic discrimination exists, the\ndeployer shall use reasonable efforts to mitigate the impacts of such\ndiscrimination;\n\n(3) a description of the categories of data the system processes as inputs and\nthe outputs the system produces;\n\n(4) a summary of categories of any data used to customize the system;\n\n(5) the metrics used to evaluate the performance and known limitations of the\nsystem, including:\n\n(a) whether the evaluation was carried out using test data;\n\n(b) whether the test data sets were collected solely in New Mexico, solely in\nother states or in New Mexico in combination with other states;\n\n(c) the demographic groups represented in the test data sets and the\nproportion of each age, ethnic, gender or racial group in each data set; and\n\n(d) any independent studies carried out to evaluate the system for performance\nand risk of discrimination and whether the studies are publicly available or\npeer-reviewed;\n\n(6) a description of any transparency measures taken concerning the system,\nincluding measures taken to disclose to a consumer when the system is in use;\nand\n\n(7) a description of the post-deployment monitoring and user safeguards\nprovided for the system, including oversight, use and learning processes used\nby the deployer to address issues arising from deployment of the system.\n\nC. An impact assessment conducted following an intentional and substantial\nmodification of a high-risk artificial intelligence system shall include a\ndisclosure of the extent to which the system was used in a manner consistent\nwith, or that varied from, the developer's intended uses of the system.\n\nD. A deployer may use a single impact assessment to address a set of\ncomparable high-risk artificial intelligence systems.\n\nE. An impact assessment conducted for the purpose of complying with another\napplicable law or rule shall satisfy the requirement of this section when the\nassessment:\n\n(1) meets the requirements of this section; and\n\n(2) is reasonably similar in scope and effect to an assessment that would\notherwise be conducted pursuant to this section.\n\nF. For at least three years following the final deployment of a high-risk\nartificial intelligence system, a deployer shall maintain records of the most\nrecently conducted impact assessment for the system, including all records\nconcerning the assessment and all prior assessments for the system.\n\nG. One hundred twenty days after the department has promulgated rules pursuant\nto Section 14 of the Artificial Intelligence Act, a deployer shall review each\nhigh-risk artificial intelligence system that the deployer has deployed to\nensure that the system is not causing algorithmic discrimination.\n\nH. This section is not applicable when:\n\n(1) a deployer using a high-risk artificial intelligence system:\n\n(a) impacts fewer than fifty consumers;\n\n(b) does not use the deployer's own data to train the system;\n\n(c) uses the system solely for the system's intended uses as disclosed by a\ndeveloper pursuant to the Artificial Intelligence Act; and\n\n(d) makes any impact assessment of the system that has been provided by the\ndeveloper pursuant to the Artificial Intelligence Act available to consumers;\nand\n\n(2) the system continues learning based on data derived from sources other\nthan the deployer's own data.\n\nI. A deployer may supply documentation provided by a developer to complete the\nrequirements for an item pursuant to Subsection B of this section; provided\nthat the deployer has not modified the item.\n\nSECTION 7. [_NEW MATERIAL_] DEPLOYER GENERAL NOTICE TO CONSUMERS.--\n\nA. A deployer shall make readily available to its consumers and on its\nwebsite:\n\n(1) a summary of the types of high-risk artificial intelligence systems that\nthe deployer currently deploys and how known or reasonably foreseeable risks\nof algorithmic discrimination from the deployment of each system are managed;\nand\n\n(2) a detailed explanation of the nature, source and extent of the information\ncollected and used by the deployer.\n\nB. At a minimum, a deployer shall update the information posted on its website\npursuant to this section annually and when the deployer deploys a new high-\nrisk artificial intelligence system.\n\nSECTION 8. [_NEW MATERIAL_] USE OF ARTIFICIAL INTELLIGENCE SYSTEMS WHEN MAKING\nCONSEQUENTIAL DECISIONS--DIRECT NOTICE TO AFFECTED CONSUMERS--ADVERSE\nDECISIONS--OPPORTUNITY FOR APPEAL.--\n\nA. Except as provided in Subsection E of this section, before a high-risk\nartificial intelligence system is used to make or is a substantial factor in\nmaking a consequential decision concerning a consumer, a deployer shall\nprovide directly to the consumer:\n\n(1) notice that the system will be used to make or be a substantial factor in\nmaking the decision; and\n\n(2) information describing:\n\n(a) the system and how to access the deployer's notice required pursuant to\nSection 7 of the Artificial Intelligence Act;\n\n(b) the purpose of the system and the nature of the consequential decision\nbeing made; and\n\n(c) the deployer's contact information.\n\nB. Except as provided in Subsection E of this section, when a high-risk\nartificial intelligence system has been used to make or has been a substantial\nfactor in making a consequential decision concerning a consumer that is\nadverse to the consumer, the deployer shall provide directly to the consumer:\n\n(1) a statement explaining:\n\n(a) the principal reason or reasons for the decision;\n\n(b) the degree and manner in which the system contributed to the decision; and\n\n(c) the source and type of data that was processed by the system to make or\nthat was a substantial factor in making the decision;\n\n(2) an opportunity to correct any incorrect personal data that the system\nprocessed to make or that was a substantial factor in making the decision; and\n\n(3) an opportunity to appeal the adverse decision except in instances where an\nappeal may pose a risk of life or safety to the consumer.\n\nC. If technically feasible, an appeal of an adverse decision pursuant to this\nsection shall allow for human review.\n\nD. All information, notices and statements to a consumer as required by this\nsection shall be provided:\n\n(1) in plain language and in all languages in which the deployer in the\nordinary course of business provides contracts, disclaimers, sale\nannouncements and other information to consumers; and\n\n(2) in a format that is accessible to consumers with disabilities.\n\nE. When a deployer is unable to provide information, notice or a statement\nrequired pursuant to this section directly to a consumer, the deployer shall\nmake such information, notices or statements available in a manner that is\nreasonably calculated to ensure that the consumer receives the information,\nnotice or statement.\n\nSECTION 9. [_NEW MATERIAL_] USE OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM--\nNOTICE AND DISCLOSURE TO THE DEPARTMENT--INSPECTION OF PUBLIC RECORDS ACT\nEXEMPTION.--\n\nA. When a deployer discovers that a high-risk artificial intelligence system\nthat has been used has caused algorithmic discrimination, the deployer shall\nas expeditiously as possible but at a maximum within ninety days notify the\ndepartment of the discovery. The notice shall be in a form and manner\nprescribed by the department.\n\nB. Upon request by the department, a deployer shall within ninety days submit\nto the department any risk management policy, impact assessment or records\nconducted, implemented, maintained or received pursuant to the Artificial\nIntelligence Act. The submission shall be in a form and manner prescribed by\nthe department.\n\nC. The department may evaluate risk management policies, impact assessments or\nrecords submitted pursuant to this section for compliance with the Artificial\nIntelligence Act.\n\nD. As part of a disclosure, notice or submission pursuant to the Artificial\nIntelligence Act, a deployer shall not be required to disclose a trade secret,\ninformation protected from disclosure by state or federal law or information\nthat would create a security risk to the deployer. Such a disclosure, notice\nor submission shall be exempt from disclosure pursuant to the Inspection of\nPublic Records Act.\n\nE. Within ninety days of a request by the department, a developer shall submit\nto the department a copy of the summary and documentation the developer has\nmade available to recipients pursuant to Section 3 of the Artificial\nIntelligence Act. A developer may designate the summary or documentation as\nincluding a trade secret. To the extent that information contained in the\nsummary or documentation includes information subject to attorney-client\nprivilege or work-product protection, compliance with this section does not\nconstitute a waiver of the privilege or protection.\n\nSECTION 10. [_NEW MATERIAL_] INTERACTION OF ARTIFICIAL INTELLIGENCE SYSTEM\nWITH CONSUMERS--REQUIRED DISCLOSURE.--\n\nA. A developer or a deployer that offers or makes available an artificial\nintelligence system intended to interact with consumers shall ensure that a\nconsumer is informed that the consumer is interacting with an artificial\nintelligence system.\n\nB. Prior to deploying a high-risk artificial intelligence system to make, or\nbe a substantial factor in making, a consequential decision concerning a\nconsumer, a deployer shall notify the consumer that the high-risk artificial\nintelligence system is being deployed and of the system's role in making the\nconsequential decision.\n\nSECTION 11. [_NEW MATERIAL_] EXEMPTION FROM DISCLOSURE--TRADE SECRETS AND\nOTHER INFORMATION PROTECTED BY LAW--NOTICE TO CONSUMER.--\n\nA. Nothing in the Artificial Intelligence Act shall require a deployer or\ndeveloper to disclose a trade secret or other information protected from\ndisclosure by state or federal law.\n\nB. To the extent that a deployer or developer withholds information pursuant\nto this section that would otherwise be part of a disclosure pursuant to the\nArtificial Intelligence Act, the deployer or developer shall notify all\naffected consumers, provide a basis for the withholding and include all\ninformation not protected as a trade secret pursuant to the Uniform Trade\nSecrets Act or other state or federal law.\n\nSECTION 12. [_NEW MATERIAL_] APPLICABILITY EXEMPTIONS-- OTHER LAW--SECURITY\nAND TESTING--FEDERAL USE--INSURANCE PROVIDERS.--\n\nA. No provision of the Artificial Intelligence Act shall be construed to\nrestrict a person's ability to:\n\n(1) comply with federal, state or municipal laws or regulations;\n\n(2) comply with a civil, criminal or regulatory inquiry, investigation,\nsubpoena or summons by a governmental authority;\n\n(3) cooperate with a law enforcement agency concerning activity that the\nperson reasonably and in good faith believes may violate other laws or\nregulations;\n\n(4) defend, exercise or investigate legal claims;\n\n(5) act to protect an interest that is essential for the life or physical\nsafety of a person;\n\n(6) by any means:\n\n(a) detect, prevent, protect against or respond to deceptive, illegal or\nmalicious activity, fraud, identity theft, harassment or security incidents;\nor\n\n(b) investigate, prosecute or report persons responsible for the actions\nlisted in Subparagraph (a) of this paragraph;\n\n(7) preserve the integrity or security of artificial intelligence, computer,\nelectronic or internet connection systems;\n\n(8) engage in public or peer-reviewed scientific or statistical research,\nincluding clinical trials, that adheres to and is conducted in accordance with\napplicable federal and state law;\n\n(9) engage in pre-market testing other than testing conducted under real-world\nconditions, including development, research and testing of artificial\nintelligence systems; or\n\n(10) assist another person with compliance with the Artificial Intelligence\nAct.\n\nB. No provision of the Artificial Intelligence Act shall be construed to\nrestrict:\n\n(1) a product recall; or\n\n(2) identification or repair of technical errors that impair the functionality\nof the artificial intelligence system.\n\nC. The Artificial Intelligence Act does not apply in circumstances in which\ncompliance would violate an evidentiary privilege pursuant to law.\n\nD. No provision of the Artificial Intelligence Act shall be construed so as to\nlimit the rights of a person, including the rights to free speech or freedom\nof the press pursuant to the first amendment to the United States constitution\nor Article 2, Section 17 of the constitution of New Mexico.\n\nE. The Artificial Intelligence Act does not apply to a developer, deployer or\nother person who:\n\n(1) uses or intentionally and substantially modifies a high-risk artificial\nintelligence system that:\n\n(a) has been authorized by a federal agency in accordance with federal law;\nand\n\n(b) is in compliance with standards established by a federal agency in\naccordance with federal law when such standards are substantially equivalent\nor more stringent than the requirements of the Artificial Intelligence Act;\n\n(2) conducts research to support an application for approval, certification or\nreview by a federal agency pursuant to federal law; or\n\n(3) performs work under or in connection with a contract with a federal\nagency, unless the work is on a high- risk artificial intelligence system used\nto make or as a substantial factor in making a decision concerning employment\nor housing.\n\nF. The Artificial Intelligence Act does not apply to an artificial\nintelligence system to the extent the system is used by the federal\ngovernment, except for a high-risk artificial intelligence system used to make\nor as a substantial factor in making a decision concerning employment or\nhousing.\n\nG. A financial institution, an affiliate or a subsidiary of a financial\ninstitution or a service provider that is subject to prudential regulation by\nanother state or by the federal government pursuant to laws that apply to the\nuse of high-risk artificial intelligence systems shall be deemed to be in\ncompliance with the Artificial Intelligence Act when the applicable laws:\n\n(1) impose requirements that are substantially equivalent to or more stringent\nthan the requirements imposed by the Artificial Intelligence Act; and\n\n(2) at a minimum, require the financial institution, affiliate or service\nprovider to:\n\n(a) notify consumers subject to the high-risk artificial intelligence system\nof the system's use and its role in consequential decisions;\n\n(b) regularly audit the institution's use of high-risk artificial intelligence\nsystems for compliance with state and federal antidiscrimination laws; and\n\n(c) mitigate any algorithmic discrimination caused by the use of a high-risk\nartificial intelligence system.\n\nH. A developer, deployer or other person who engages in an action pursuant to\nan exemption set forth in this section shall bear the burden of demonstrating\nthat the action qualifies for the exemption.\n\nI. As used in this section, \"financial institution\" means an insured state or\nnational bank, a state or federal savings and loan association or savings\nbank, a state or federal credit union or authorized branches of each of the\nforegoing.\n\nSECTION 13. [_NEW MATERIAL_] ENFORCEMENT--DEPARTMENT--OPPORTUNITY TO CURE--\nCONSUMER CIVIL ACTIONS.--\n\nA. Upon the promulgation of rules pursuant to Section 14 of the Artificial\nIntelligence Act:\n\n(1) the department shall have authority to enforce that act; and\n\n(2) a consumer may bring a civil action in district court against a developer\nor deployer for declaratory or injunctive relief and attorney fees for a\nviolation of that act.\n\nB. Prior to the promulgation of rules by the department pursuant to Section 14\nof the Artificial Intelligence Act, the department shall issue a notice to a\nprospective defendant prior to initiating an action for violation of the act.\nThe notice shall include a detailed description of the alleged violation and\nthe actions required to cure the violation. The prospective defendant shall\nhave ninety days from the receipt of the notice to submit evidence\nsatisfactory to the department that the violation has been cured. Ninety-one\ndays after the prospective defendant has received the notice, if the\ndepartment has not received satisfactory evidence that the violation has been\ncured, the department may file an action in district court for the violation.\n\nC. For one calendar year from the date the department promulgates rules\npursuant to Section 14 of the Artificial Intelligence Act, it shall be an\naffirmative defense in an action brought by the department to enforce the\nArtificial Intelligence Act when:\n\n(1) the developer, deployer or other person discovers a violation of the\nArtificial Intelligence Act as a result of adversarial testing, red teaming or\nan internal review process;\n\n(2) the developer, deployer or other person reports the violation to the\ndepartment and cures the violation within seven days of the violation;\n\n(3) the developer, deployer or other person is in compliance with a risk\nmanagement framework for artificial intelligence systems designated by the\ndepartment by rule;\n\n(4) the deployer is dependent on documentation from the developer to cure or\notherwise resolve a violation and the deployer complies with the requirements\nin Paragraph (2) of Subsection B of Section 6 of the Artificial Intelligence\nAct; and\n\n(5) the developer, deployer or other person demonstrates that the violation\nwas inadvertent, affected fewer than one hundred consumers and could not have\nbeen discovered through reasonable diligence.\n\nD. After one calendar year from the date the department promulgates rules\npursuant to Section 14 of the Artificial Intelligence Act, a deployer,\ndeveloper or other person subject to enforcement for a violation of that act\nshall have no right to cure the violation or an affirmative defense pursuant\nto this section.\n\nE. In an action by the department to enforce the Artificial Intelligence Act,\nthe developer, deployer or other person who is the subject of the enforcement\nshall bear the burden of demonstrating that the requirements for an\naffirmative defense pursuant to this section have been met.\n\nF. Nothing in the Artificial Intelligence Act, including the enforcement\nauthority granted to the department pursuant to this section, preempts or\notherwise affects any right, claim, remedy, presumption or defense available\nin law or equity.\n\nG. An affirmative defense presumption established by the Artificial\nIntelligence Act applies only to an enforcement action by the department and\ndoes not apply to any right, claim, remedy, presumption or defense available\nin law or equity.\n\nH. A violation of the Artificial Intelligence Act is an unfair practice and\nmay be enforced pursuant to the Unfair Practices Act.\n\nI. As used in this section:\n\n(1) \"adversarial testing\" means to proactively try to break an application by\nproviding it with data most likely to elicit problematic output, or as defined\nby the department by rule; and\n\n(2) \"red teaming\" means the practice of simulating attack scenarios on an\nartificial intelligence application to pinpoint weaknesses and plan preventive\nmeasures or as defined by the department by rule.\n\nSECTION 14. [_NEW MATERIAL_] RULEMAKING.--\n\nA. On or before January 1, 2027, the department shall promulgate rules to\nimplement the Artificial Intelligence Act and shall post them prominently on\nthe department's website.\n\nB. The department shall consult artificial intelligence experts, academic\nresearchers, civil rights organizations, deployers, developers, labor unions\nand organizations representing the interests of consumers when developing the\nrules to be promulgated pursuant to the Artificial Intelligence Act.\n\nSECTION 15. EFFECTIVE DATE.--The effective date of the provisions of this act\nis July 1, 2026.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": true
    }
  ]
}