{
  "bill_id": "MA2025000H97",
  "source_url": "http://custom.statenet.com/public/resources.cgi?id=ID:bill:MA2025000H97&cuiq=93d84396-c63b-526a-b152-38b7f79b4cfd&client_md=e4f6fea4-27b4-5d41-b7d3-766fe52569f0",
  "versions": [
    {
      "date": "02/27/2025",
      "label": "Introduced",
      "url": "https://custom.statenet.com/public/resources.cgi?mode=show_text&id=ID:bill:MA2025000H97&verid=MA2025000H97_20250227_0_I&",
      "raw_html": "<html>\n<head>\n<title>Bill Resource</title>\n\n<!--   <link href=\"https://custom.statenet.com/network/Common/css/extregtext.css\" rel=\"stylesheet\" type=\"text/css\" />-->\n   <link href=\"https://custom.statenet.com/network/Common/css/xmltext-2.0.css\" rel=\"stylesheet\" type=\"text/css\" />\n   <link href=\"https://custom.statenet.com/network/Common/css/additional-text.css\" rel=\"stylesheet\" type=\"text/css\" />\n\n\t<style type=\"text/css\">\n<!--\n\ntd, body {\n     background-color: white;\n\tfont-family: Verdana, Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n\n.resourceContainer {\n   border: 1px solid black;\n}\n-->\n</style>\n</head>\n<body>\n\n    <div style=\"width: 750px; margin: auto\">\n       <div style=\"font-size: .8em;margin-bottom: 10px\"><table width=\"100%\"><tr><td align=\"left\" style=\"font-size: .8em;\"><div>The following has special meaning:</div>\n<div><u class=\"amendmentInsertedText\">green underline denotes added text</u></div>\n<div><strike class=\"amendmentDeletedText\">red struck out text denotes deleted text</strike></div></td> <td align=\"right\"><a href=\"https://www.lexisnexis.com/statenet/\"><img alt=\"Powered by State Net\" src=\"https://custom.statenet.com/network/poweredby.gif\" /></a></td></tr></table></div><table id=\"text-identifier\"><tr><td class=\"key\">2025 MA H 97</td> <td><table><tr><td class=\"label\">Author:</td> <td>Rogers D</td></tr> <tr><td class=\"label\">Version:</td> <td>Introduced</td></tr> <tr><td class=\"label\">Version Date:</td> <td>02/27/2025</td></tr></table></td></tr></table><div class=\"documentBody\">\n  <a name=\"head_document_section\"></a><div class=\"head\">\n   <p class=\"left\">HOUSE No. 97</p>\n   <p class=\"center\">The Commonwealth of Massachusetts</p>\n   <p class=\"center\">In the One Hundred and Ninety-Fourth General Court</p>\n   <p class=\"center\">(2025-2026)</p>\n  </div>\n  <a name=\"title_document_section\"></a><div class=\"title\">\n   <p class=\"left\">AN ACT ESTABLISHING THE COMPREHENSIVE MASSACHUSETTS CONSUMER DATA PRIVACY ACT.</p>\n  </div>\n  <a name=\"text_document_section\"></a><div class=\"text\">\n   <span>\n    <p class=\"indent\">Be it enacted by the Senate and House of Representatives in General Court assembled, and by the authority of the same, as follows: </p>\n   </span>\n   <p class=\"indent\">SECTION 1. The General Laws, as appearing in the 2022 Official Edition, are hereby amended by inserting a new chapter: </p>\n   <p class=\"indent\">CHAPTER 93M. Consumer Protections in interactions with Artificial Intelligence Systems </p>\n   <p class=\"indent\">Section 1. Definitions </p>\n   <p class=\"indent\">The following words shall, unless the context clearly requires otherwise, have the following meanings:&ndash; </p>\n   <p class=\"indent\">&quot;Algorithmic discrimination&quot; means any condition in which the use of an artificial intelligence system results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis of their actual or perceived age, color, disability, ethnicity, genetic information, limited proficiency in the English language, national origin, race, religion, reproductive health, sex, veteran status, or other classification protected under the laws of this state or federal law. </p>\n   <p class=\"indent\">&quot;Algorithmic discrimination&quot; does not include: </p>\n   <p class=\"indent\">(1) the offer, license, or use of a high-risk artificial intelligence system by a developer or deployer for the sole purpose of: </p>\n   <p class=\"indent\">(i) the developer&#39;s or deployer&#39;s self-testing to identify, mitigate, or prevent discrimination or otherwise ensure compliance with state and federal law; or </p>\n   <p class=\"indent\">(ii) expanding an applicant, customer, or participant pool to increase diversity or redress historical discrimination; or </p>\n   <p class=\"indent\">(2) an act or omission by or on behalf of a private club or other establishment that is not in fact open to the public, as set forth in Title II of the federal &quot;Civil Rights Act of 1964&quot;, 42 U.S.C. Sec. 2000a (e), as amended. </p>\n   <p class=\"indent\">&quot;Artificial intelligence system&quot; means any machine-based system that, for any explicit or implicit objective, infers from the inputs the system receives how to generate outputs, including content, decisions, predictions, or recommendations, that can influence physical or virtual environments. </p>\n   <p class=\"indent\">&quot;Consequential decision&quot; means a decision that has a material legal or similarly significant effect on the provision or denial to any consumer of, or the cost or terms of: </p>\n   <p class=\"indent\">(1) education enrollment or an education opportunity; </p>\n   <p class=\"indent\">(2) employment or an employment opportunity; </p>\n   <p class=\"indent\">(3) a financial or lending service; </p>\n   <p class=\"indent\">(4) an essential government service; </p>\n   <p class=\"indent\">(5) health-care services; </p>\n   <p class=\"indent\">(6) housing; </p>\n   <p class=\"indent\">(7) insurance; or </p>\n   <p class=\"indent\">(8) a legal service. </p>\n   <p class=\"indent\">&quot;Consumer&quot; means an individual who is a Massachusetts resident. </p>\n   <p class=\"indent\">&quot;Deploy&quot; means to use a high-risk artificial intelligence system. </p>\n   <p class=\"indent\">&quot;Deployer&quot; means a person doing business in this state that deploys a high-risk artificial intelligence system. </p>\n   <p class=\"indent\">&quot;Developer&quot; means a person doing business in this state that develops or intentionally and substantially modifies an artificial intelligence system. </p>\n   <p class=\"indent\">&quot;Health-care services&quot; has the same meaning as provided in 42 U.S.C. Sec. 234 (d)(2). </p>\n   <p class=\"indent\">&quot;High-risk artificial intelligence system&quot; means any artificial intelligence system that, when deployed, makes, or is a substantial factor in making, a consequential decision. </p>\n   <p class=\"indent\">&quot;High-risk artificial intelligence system&quot; does not include: </p>\n   <p class=\"indent\">(1) an artificial intelligence system if the artificial intelligence system is intended to: </p>\n   <p class=\"indent\">(i) perform a narrow procedural task; or </p>\n   <p class=\"indent\">(ii) detect decision-making patterns or deviations from prior decision-making patterns and is not intended to replace or influence a previously completed human assessment without sufficient human review; or </p>\n   <p class=\"indent\">(2) the following technologies, unless the technologies, when deployed, make, or are a substantial factor in making, a consequential decision: </p>\n   <p class=\"indent\">(i) anti-fraud technology that does not use facial recognition technology; </p>\n   <p class=\"indent\">(ii) anti-malware; </p>\n   <p class=\"indent\">(iii) anti-virus; </p>\n   <p class=\"indent\">(iv) artificial intelligence-enabled video games; </p>\n   <p class=\"indent\">(v) calculators; </p>\n   <p class=\"indent\">(vi) cybersecurity; </p>\n   <p class=\"indent\">(vii) databases; </p>\n   <p class=\"indent\">(viii) data storage; </p>\n   <p class=\"indent\">(ix) firewall; </p>\n   <p class=\"indent\">(x) internet domain registration; </p>\n   <p class=\"indent\">(xi) internet website loading; </p>\n   <p class=\"indent\">(xii) networking; </p>\n   <p class=\"indent\">(xiii) spam- and robocall-filtering; </p>\n   <p class=\"indent\">(xiv) spell-checking; </p>\n   <p class=\"indent\">(xv) spreadsheets; </p>\n   <p class=\"indent\">(xvi) web caching; </p>\n   <p class=\"indent\">(xvii) web hosting or any similar technology; or </p>\n   <p class=\"indent\">(xviii) technology that communicates with consumers in natural language for the purpose of providing users with information, making referrals or recommendations, and answering questions and is subject to an accepted use policy that prohibits generating content that is discriminatory or harmful. </p>\n   <p class=\"indent\">&quot;Intentional and substantial modification&quot; or &quot;intentionally and substantially modifies&quot; means a deliberate change made to an artificial intelligence system that results in any new reasonably foreseeable risk of algorithmic discrimination. </p>\n   <p class=\"indent\">&quot;Intentional and substantial modification&quot; or &quot;intentionally and substantially modifies&quot; does not include a change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if: </p>\n   <p class=\"indent\">(1) the high-risk artificial intelligence system continues to learn after the high-risk artificial intelligence system is: </p>\n   <p class=\"indent\">(i) offered, sold, leased, licensed, given, or otherwise made available to a deployer; or </p>\n   <p class=\"indent\">(ii) deployed; </p>\n   <p class=\"indent\">(2) the change is made to the high-risk artificial intelligence system as a result of any learning described in paragraph (1)(i) of this subsection; </p>\n   <p class=\"indent\">(3) the change was predetermined by the deployer, or a third party contracted by the deployer, when the deployer or third party completed an initial impact assessment of such high-risk artificial intelligence system pursuant to section 3 (c) (1); and </p>\n   <p class=\"indent\">(4) the change is included in technical documentation for the high-risk artificial intelligence system. </p>\n   <p class=\"indent\">&quot;Substantial factor&quot; means a factor that: </p>\n   <p class=\"indent\">(1) assists in making a consequential decision; </p>\n   <p class=\"indent\">(2) is capable of altering the outcome of a consequential decision; and </p>\n   <p class=\"indent\">(3) is generated by an artificial intelligence system. </p>\n   <p class=\"indent\">&quot;Substantial factor&quot; includes any use of an artificial intelligence system to generate any content, decision, prediction, or recommendation concerning a consumer that is used as a basis to make a consequential decision concerning the consumer. </p>\n   <p class=\"indent\">&quot;Trade secret&quot; has the meaning set forth in section 42 (4) of chapter 93 of the General Laws, as appearing in the 2022 Official Edition. </p>\n   <p class=\"indent\">Section 2. Developer duty to avoid algorithmic discrimination - required documentation. </p>\n   <p class=\"indent\">(a) Not later than 6 months after the effective date of this act, a developer of a high-risk artificial intelligence system shall use reasonable care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination arising from the intended and contracted uses of the high-risk artificial intelligence system. In any enforcement action brought not later than 6 months after the effective date of this act, by the attorney general pursuant to section 6, there is a rebuttable presumption that a developer used reasonable care as required under this section if the developer complied with this section and any additional requirements or obligations as set forth in rules promulgated by the attorney general pursuant to section 7. </p>\n   <p class=\"indent\">(b) Not later than 6 months after the effective date of this act, and except as provided in subsection (f) of this section, a developer of a high-risk artificial intelligence system shall make available to the deployer or other developer of the high-risk artificial intelligence system: </p>\n   <p class=\"indent\">(1) a general statement describing the reasonably foreseeable uses and known harmful or inappropriate uses of the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(2) documentation disclosing: </p>\n   <p class=\"indent\">(i) high-level summaries of the type of data used to train the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(ii) known or reasonably foreseeable limitations of the high-risk artificial intelligence system, including known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(iii) the purpose of the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(iv) the intended benefits and uses of the high-risk artificial intelligence system; and </p>\n   <p class=\"indent\">(v) all other information necessary to allow the deployer to comply with the requirements of section 3; </p>\n   <p class=\"indent\">(3) documentation describing: </p>\n   <p class=\"indent\">(i) how the high-risk artificial intelligence system was evaluated for performance and mitigation of algorithmic discrimination before the high-risk artificial intelligence system was offered, sold, leased, licensed, given, or otherwise made available to the deployer; </p>\n   <p class=\"indent\">(ii) the data governance measures used to cover the training datasets and the measures used to examine the suitability of data sources, possible biases, and appropriate mitigation; </p>\n   <p class=\"indent\">(iii) the intended outputs of the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(iv) the measures the developer has taken to mitigate known or reasonably foreseeable risks of algorithmic discrimination that may arise from the reasonably foreseeable deployment of the high-risk artificial intelligence system; and </p>\n   <p class=\"indent\">(v) how the high-risk artificial intelligence system should be used, not be used, and be monitored by an individual when the high-risk artificial intelligence system is used to make, or is a substantial factor in making, a consequential decision; and </p>\n   <p class=\"indent\">(4) any additional documentation that is reasonably necessary to assist the deployer in understanding the outputs and monitor the performance of the high-risk artificial intelligence system for risks of algorithmic discrimination. </p>\n   <p class=\"indent\">(c) (1) except as provided in subsection (f) of this section, a developer that offers, sells, leases, licenses, gives, or otherwise makes available to a deployer or other developer a high-risk artificial intelligence system not later than 6 months after the effective date of this act, shall make available to the deployer or other developer, to the extent feasible, the documentation and information, through artifacts such as model cards, dataset cards, or other impact assessments, necessary for a deployer, or for a third party contracted by a deployer, to complete an impact assessment pursuant to section 3 (c). </p>\n   <p class=\"indent\">(2) a developer that also serves as a deployer for a high-risk artificial intelligence system is not required to generate the documentation required by this section unless the high-risk artificial intelligence system is provided to an unaffiliated entity acting as a deployer. </p>\n   <p class=\"indent\">(d) (1) Not later than 6 months after the effective date of this act, a developer shall make available, in a manner that is clear and readily available on the developer&#39;s website or in a public use case inventory, a statement summarizing: </p>\n   <p class=\"indent\">(i) the types of high-risk artificial intelligence systems that the developer has developed or intentionally and substantially modified and currently makes available to a deployer or other developer; and </p>\n   <p class=\"indent\">(ii) how the developer manages known or reasonably foreseeable risks of algorithmic discrimination that may arise from the development or intentional and substantial modification of the types of high-risk artificial intelligence systems described in accordance with subsection (d)(1)(i) of this section. </p>\n   <p class=\"indent\">(2) a developer shall update the statement described in subsection (d)(1) of this section: </p>\n   <p class=\"indent\">(i) as necessary to ensure that the statement remains accurate; and </p>\n   <p class=\"indent\">(ii) no later than ninety days after the developer intentionally and substantially modifies any high-risk artificial intelligence system described in subsection (d)(1)(i) of this section. </p>\n   <p class=\"indent\">(e) Not later than 6 months after the effective date of this act, a developer of a high-risk artificial intelligence system shall disclose to the attorney general, in a form and manner prescribed by the attorney general, and to all known deployers or other developers of the high-risk artificial intelligence system, any known or reasonably foreseeable risks of algorithmic discrimination arising from the intended uses of the high-risk artificial intelligence system without unreasonable delay but no later than ninety days after the date on which: </p>\n   <p class=\"indent\">(1) the developer discovers through the developer&#39;s ongoing testing and analysis that the developer&#39;s high-risk artificial intelligence system has been deployed and has caused or is reasonably likely to have caused algorithmic discrimination; or </p>\n   <p class=\"indent\">(2) the developer receives from a deployer a credible report that the high-risk artificial intelligence system has been deployed and has caused algorithmic discrimination. </p>\n   <p class=\"indent\">(f) nothing in subsections (b) to (e) of this section requires a developer to disclose a trade secret, information protected from disclosure by state or federal law, or information that would create a security risk to the developer. </p>\n   <p class=\"indent\">(g) Not later than 6 months after the effective date of this act, the attorney general may require that a developer disclose to the attorney general, no later than ninety days after the request and in a form and manner prescribed by the attorney general, the statement or documentation described in subsection (b) of this section. The attorney general may evaluate such statement or documentation to ensure compliance with this chapter, and the statement or documentation is not subject to disclosure under the &ldquo;Massachusetts Public Records Law&rdquo;, chapter 66, section 10 of the General Laws. In a disclosure pursuant to this subsection (g), a developer may designate the statement or documentation as including proprietary information or a trade secret. To the extent that any information contained in the statement or documentation includes information subject to attorney-client privilege or work-product protection, the disclosure does not constitute a waiver of the privilege or protection. </p>\n   <p class=\"indent\">Section 3. Deployer duty to avoid algorithmic discrimination - risk management policy and program. </p>\n   <p class=\"indent\">(a) Not later than 6 months after the effective date of this act, a deployer of a high-risk artificial intelligence system shall use reasonable care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination. In any enforcement action brought not later than 6 months after the effective date of this act, by the attorney general pursuant to section 6, there is a rebuttable presumption that a deployer of a high-risk artificial intelligence system used reasonable care as required under this section if the deployer complied with this section and any additional requirements or obligations as set forth in rules promulgated by the attorney general pursuant to section 7. </p>\n   <p class=\"indent\">(b) (1) Not later than 6 months after the effective date of this act, and except as provided in subsection (f) of this section, a deployer of a high-risk artificial intelligence system shall implement a risk management policy and program to govern the deployer&#39;s deployment of the high-risk artificial intelligence system. The risk management policy and program must specify and incorporate the principles, processes, and personnel that the deployer uses to identify, document, and mitigate known or reasonably foreseeable risks of algorithmic discrimination. The risk management policy and program must be an iterative process planned, implemented, and regularly and systematically reviewed and updated over the life cycle of a high-risk artificial intelligence system, requiring regular, systematic review and updates. A risk management policy and program implemented and maintained pursuant to this subsection (b) must be reasonable considering: </p>\n   <p class=\"indent\">(i) (A) the guidance and standards set forth in the latest version of the &quot;Artificial Intelligence Risk Management Framework&quot; published by the National Institute of Standards and Technology in the United States Department of Commerce, standard ISO/IEC 42001 of the International Organization for Standardization, or another nationally or internationally recognized risk management framework for artificial intelligence systems, if the standards are substantially equivalent to or more stringent than the requirements of this chapter; or </p>\n   <p class=\"indent\">(B) any risk management framework for artificial intelligence systems that the attorney general, in the attorney general&#39;s discretion, may designate; </p>\n   <p class=\"indent\">(ii) the size and complexity of the deployer; </p>\n   <p class=\"indent\">(iii) the nature and scope of the high-risk artificial intelligence systems deployed by the deployer, including the intended uses of the high-risk artificial intelligence systems; and </p>\n   <p class=\"indent\">(iv) the sensitivity and volume of data processed in connection with the high-risk artificial intelligence systems deployed by the deployer. </p>\n   <p class=\"indent\">(2) a risk management policy and program implemented pursuant to subsection (b)(1) of this section may cover multiple high-risk artificial intelligence systems deployed by the deployer. </p>\n   <p class=\"indent\">(c) (1) except as provided in subsections (c)(4), (c)(5), and (f) of this section: </p>\n   <p class=\"indent\">(i) a deployer, or a third party contracted by the deployer, that deploys a high-risk artificial intelligence system not later than 6 months after the effective date of this act, shall complete an impact assessment for the high-risk artificial intelligence system; and </p>\n   <p class=\"indent\">(ii) Not later than 6 months after the effective date of this act, a deployer, or a third party contracted by the deployer, shall complete an impact assessment for a deployed high-risk artificial intelligence system at least annually and within ninety days after any intentional and substantial modification to the high-risk artificial intelligence system is made available. </p>\n   <p class=\"indent\">(2) an impact assessment completed pursuant to this subsection (c) must include, at a minimum, and to the extent reasonably known by or available to the deployer: </p>\n   <p class=\"indent\">(i) a statement by the deployer disclosing the purpose, intended use cases, and deployment context of, and benefits afforded by, the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(ii) an analysis of whether the deployment of the high-risk artificial intelligence system poses any known or reasonably foreseeable risks of algorithmic discrimination and, if so, the nature of the algorithmic discrimination and the steps that have been taken to mitigate the risks; </p>\n   <p class=\"indent\">(iii) a description of the categories of data the high-risk artificial intelligence system processes as inputs and the outputs the high-risk artificial intelligence system produces; </p>\n   <p class=\"indent\">(iv) if the deployer used data to customize the high-risk artificial intelligence system, an overview of the categories of data the deployer used to customize the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(v) any metrics used to evaluate the performance and known limitations of the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(vi) a description of any transparency measures taken concerning the high-risk artificial intelligence system, including any measures taken to disclose to a consumer that the high-risk artificial intelligence system is in use when the high-risk artificial intelligence system is in use; and </p>\n   <p class=\"indent\">(vii) a description of the post-deployment monitoring and user safeguards provided concerning the high-risk artificial intelligence system, including the oversight, use, and learning process established by the deployer to address issues arising from the deployment of the high-risk artificial intelligence system. </p>\n   <p class=\"indent\">(3) in addition to the information required under subsection (3)(b) of this section, an impact assessment completed pursuant to this subsection (c) following an intentional and substantial modification to a high-risk artificial intelligence system not later than 6 months after the effective date of this act, must include a statement disclosing the extent to which the high-risk artificial intelligence system was used in a manner that was consistent with, or varied from, the developer&#39;s intended uses of the high-risk artificial intelligence system. </p>\n   <p class=\"indent\">(4) a single impact assessment may address a comparable set of high-risk artificial intelligence systems deployed by a deployer. </p>\n   <p class=\"indent\">(5) if a deployer, or a third party contracted by the deployer, completes an impact assessment for the purpose of complying with another applicable law or regulation, the impact assessment satisfies the requirements established in this subsection (c) if the impact assessment is reasonably similar in scope and effect to the impact assessment that would otherwise be completed pursuant to this subsection (c). </p>\n   <p class=\"indent\">(6) a deployer shall maintain the most recently completed impact assessment for a high-risk artificial intelligence system as required under this subsection (c), all records concerning each impact assessment, and all prior impact assessments, if any, for at least three years following the final deployment of the high-risk artificial intelligence system. </p>\n   <p class=\"indent\">(7) Not later than 6 months after the effective date of this act, and at least annually thereafter, a deployer, or a third party contracted by the deployer, must review the deployment of each high-risk artificial intelligence system deployed by the deployer to ensure that the high-risk artificial intelligence system is not causing algorithmic discrimination. </p>\n   <p class=\"indent\">(d) (1) Not later than 6 months after the effective date of this act, and no later than the time that a deployer deploys a high-risk artificial intelligence system to make, or be a substantial factor in making, a consequential decision concerning a consumer, the deployer shall: </p>\n   <p class=\"indent\">(i) notify the consumer that the deployer has deployed a high-risk artificial intelligence system to make, or be a substantial factor in making, a consequential decision before the decision is made; </p>\n   <p class=\"indent\">(ii) provide to the consumer a statement disclosing the purpose of the high-risk artificial intelligence system and the nature of the consequential decision; the contact information for the deployer; a description, in plain language, of the high-risk artificial intelligence system; and instructions on how to access the statement required by subsection (5)(a) of this section; and </p>\n   <p class=\"indent\">(iii) provide to the consumer information, if applicable, regarding the consumer&#39;s right to opt out of the processing of personal data concerning the consumer for purposes of profiling in furtherance of decisions that produce legal or similarly significant effects concerning the consumer. </p>\n   <p class=\"indent\">(2) Not later than 6 months after the effective date of this act, a deployer that has deployed a high-risk artificial intelligence system to make, or be a substantial factor in making, a consequential decision concerning a consumer shall, if the consequential decision is adverse to the consumer, provide to the consumer: </p>\n   <p class=\"indent\">(i) a statement disclosing the principal reason or reasons for the consequential decision, including: </p>\n   <p class=\"indent\">(A) the degree to which, and manner in which, the high-risk artificial intelligence system contributed to the consequential decision; </p>\n   <p class=\"indent\">(B) the type of data that was processed by the high-risk artificial intelligence system in making the consequential decision; and </p>\n   <p class=\"indent\">(C) the source or sources of the data described in subsection (d)(2)(i)(B) of this section; </p>\n   <p class=\"indent\">(ii) an opportunity to correct any incorrect personal data that the high-risk artificial intelligence system processed in making, or as a substantial factor in making, the consequential decision; and </p>\n   <p class=\"indent\">(iii) an opportunity to appeal an adverse consequential decision concerning the consumer arising from the deployment of a high-risk artificial intelligence system, which appeal must, if technically feasible, allow for human review unless providing the opportunity for appeal is not in the best interest of the consumer, including in instances in which any delay might pose a risk to the life or safety of such consumer. </p>\n   <p class=\"indent\">(3) (i) except as provided in subsection (d)(3)(ii) of this section, a deployer shall provide the notice, statement, contact information, and description required by subsections (c)(1) and (d)(2) of this section: </p>\n   <p class=\"indent\">(A) directly to the consumer; </p>\n   <p class=\"indent\">(B) in plain language; </p>\n   <p class=\"indent\">(C) in all languages in which the deployer, in the ordinary course of the deployer&#39;s business, provides contracts, disclaimers, sale announcements, and other information to consumers; and </p>\n   <p class=\"indent\">(D) in a format that is accessible to consumers with disabilities. </p>\n   <p class=\"indent\">(ii) if the deployer is unable to provide the notice, statement, contact information, and description required by subsections (d)(1) and (d)(2) of this section directly to the consumer, the deployer shall make the notice, statement, contact information, and description available in a manner that is reasonably calculated to ensure that the consumer receives the notice, statement, contact information, and description. </p>\n   <p class=\"indent\">(e) (1) Not later than 6 months after the effective date of this act, and except as provided in subsection (f) of this section, a deployer shall make available, in a manner that is clear and readily available on the deployer&#39;s website, a statement summarizing: </p>\n   <p class=\"indent\">(i) the types of high-risk artificial intelligence systems that are currently deployed by the deployer; </p>\n   <p class=\"indent\">(ii) how the deployer manages known or reasonably foreseeable risks of algorithmic discrimination that may arise from the deployment of each high-risk artificial intelligence system described pursuant to subsection (e)(1)(i) of this section; and </p>\n   <p class=\"indent\">(iii) in detail, the nature, source, and extent of the information collected and used by the deployer. </p>\n   <p class=\"indent\">(2) a deployer shall periodically update the statement described in subsection (e)(1) of this section. </p>\n   <p class=\"indent\">(f) subsections (b), (c), and (e) of this section do not apply to a deployer if, at the time the deployer deploys a high-risk artificial intelligence system and at all times while the high-risk artificial intelligence system is deployed: </p>\n   <p class=\"indent\">(1) the deployer: </p>\n   <p class=\"indent\">(i) employs fewer than fifty full-time equivalent employees; and </p>\n   <p class=\"indent\">(ii) does not use the deployer&#39;s own data to train the high-risk artificial intelligence system; </p>\n   <p class=\"indent\">(2) the high-risk artificial intelligence system: </p>\n   <p class=\"indent\">(i) is used for the intended uses that are disclosed to the deployer as required by section 2 (b)(1); and </p>\n   <p class=\"indent\">(ii) continues learning based on data derived from sources other than the deployer&#39;s own data; and </p>\n   <p class=\"indent\">(3) the deployer makes available to consumers any impact assessment that: </p>\n   <p class=\"indent\">(i) the developer of the high-risk artificial intelligence system has completed and provided to the deployer; and </p>\n   <p class=\"indent\">(ii) includes information that is substantially similar to the information in the impact assessment required under of this section. </p>\n   <p class=\"indent\">(g) if a deployer deploys a high-risk artificial intelligence system not later than 6 months after the effective date of this act, and subsequently discovers that the high-risk artificial intelligence system has caused algorithmic discrimination, the deployer, without unreasonable delay, but no later than ninety days after the date of the discovery, shall send subsection (c)(2) to the attorney general, in a form and manner prescribed by the attorney general, a notice disclosing the discovery. </p>\n   <p class=\"indent\">(h) nothing in subsections (b) to (e) and (g) of this section requires a deployer to disclose a trade secret or information protected from disclosure by state or federal law. To the extent that a deployer withholds information pursuant to this subsection (h) or section 5 (e), the deployer shall notify the consumer and provide a basis for the withholding. </p>\n   <p class=\"indent\">(i) Not later than 6 months after the effective date of this act, the attorney general may require that a deployer, or a third party contracted by the deployer, disclose to the attorney general, no later than ninety days after the request and in a form and manner prescribed by the attorney general, the risk management policy implemented pursuant to subsection (b) of this section, the impact assessment completed pursuant to subsection (c) of this section, or the records maintained pursuant to subsection (c)(6) of this section. The attorney general may evaluate the risk management policy, impact assessment, or records to ensure compliance with this chapter, and the risk management policy, impact assessment, and records are not subject to disclosure under the &ldquo;Massachusetts Public Records Law&rdquo;, chapter 66, section 10 of the General Laws. In a disclosure pursuant to this subsection (i), a deployer may designate the statement or documentation as including proprietary information or a trade secret. To the extent that any information contained in the risk management policy, impact assessment, or records include information subject to attorney-client privilege or work-product protection, the disclosure does not constitute a waiver of the privilege or protection. </p>\n   <p class=\"indent\">Section 4. Disclosure of an artificial intelligence system to consumer </p>\n   <p class=\"indent\">(a) Not later than 6 months after the effective date of this act, and except as provided in subsection (b) of this section, a deployer or other developer that deploys, offers, sells, leases, licenses, gives, or otherwise makes available an artificial intelligence system that is intended to interact with consumers shall ensure the disclosure to each consumer who interacts with the artificial intelligence system that the consumer is interacting with an artificial intelligence system. </p>\n   <p class=\"indent\">(b) disclosure is not required under subsection (a) of this section under circumstances in which it would be obvious to a reasonable person that the person is interacting with an artificial intelligence system. </p>\n   <p class=\"indent\">Section 5. Compliance with other legal obligations - definitions </p>\n   <p class=\"indent\">(a) nothing in this chapter restricts a developer&#39;s, a deployer&#39;s, or other person&#39;s ability to: </p>\n   <p class=\"indent\">(1) comply with federal, state, or municipal laws, ordinances, or regulations; </p>\n   <p class=\"indent\">(2) comply with a civil, criminal, or regulatory inquiry, investigation, subpoena, or summons by a federal, a state, a municipal, or other governmental authority; </p>\n   <p class=\"indent\">(3) cooperate with a law enforcement agency concerning conduct or activity that the developer, deployer, or other person reasonably and in good faith believes may violate federal, state, or municipal laws, ordinances, or regulations; </p>\n   <p class=\"indent\">(4) investigate, establish, exercise, prepare for, or defend legal claims; </p>\n   <p class=\"indent\">(5) take immediate steps to protect an interest that is essential for the life or physical safety of a consumer or another individual; </p>\n   <p class=\"indent\">(6) by any means other than the use of facial recognition technology, prevent, detect, protect against, or respond to security incidents, identity theft, fraud, harassment, malicious or deceptive activities, or illegal activity; investigate, report, or prosecute the persons responsible for any such action; or preserve the integrity or security of systems; </p>\n   <p class=\"indent\">(7) engage in public or peer-reviewed scientific or statistical research in the public interest that adheres to all other applicable ethics and privacy laws and is conducted in accordance with 45 CFR 46, as amended, or relevant requirements established by the federal Food and Drug Administration; </p>\n   <p class=\"indent\">(8) conduct research, testing, and development activities regarding an artificial intelligence system or model, other than testing conducted under real-world conditions, before the artificial intelligence system or model is placed on the market, deployed, or put into service, as applicable; or </p>\n   <p class=\"indent\">(i) assist another developer, deployer, or other person with any of the obligations imposed under this chapter. </p>\n   <p class=\"indent\">(b) the obligations imposed on developers, deployers, or other persons under this chapter do not restrict a developer&#39;s, a deployer&#39;s, or other person&#39;s ability to: </p>\n   <p class=\"indent\">(1) effectuate a product recall; or </p>\n   <p class=\"indent\">(2) identify and repair technical errors that impair existing or intended functionality. </p>\n   <p class=\"indent\">(c) the obligations imposed on developers, deployers, or other persons under this chapter do not apply where compliance with this chapter by the developer, deployer, or other person would violate an evidentiary privilege under the laws of this state. </p>\n   <p class=\"indent\">(d) nothing in this chapter imposes any obligation on a developer, a deployer, or other person that adversely affects the rights or freedoms of a person, including the rights of a person to freedom of speech or freedom of the press that are guaranteed in: </p>\n   <p class=\"indent\">(1) the First Amendment to the United States constitution; or </p>\n   <p class=\"indent\">(2) Part the First, Article XVI of the state constitution. </p>\n   <p class=\"indent\">(e) nothing in this chapter applies to a developer, a deployer, or other person: </p>\n   <p class=\"indent\">(1) insofar as the developer, deployer, or other person develops, deploys, puts into service, or intentionally and substantially modifies, as applicable, a high-risk artificial intelligence system: </p>\n   <p class=\"indent\">(i) that has been approved, authorized, certified, cleared, developed, or granted by a federal agency, such as the federal food and drug administration or the federal aviation administration, acting within the scope of the federal agency&#39;s authority, or by a regulated entity subject to the supervision and regulation of the federal housing finance agency; or </p>\n   <p class=\"indent\">(ii) in compliance with standards established by a federal agency, including standards established by the federal office of the national coordinator for health information technology, or by a regulated entity subject to the supervision and regulation of the federal housing finance agency, if the standards are substantially equivalent or more stringent than the requirements of this chapter; </p>\n   <p class=\"indent\">(2) conducting research to support an application for approval or certification from a federal agency, including the federal Aviation Administration, the federal Communications Commission, or the federal Food and Drug Administration or research to support an application otherwise subject to review by the federal agency; </p>\n   <p class=\"indent\">(3) performing work under, or in connection with, a contract with the United States Department of Commerce, the United States Department of Defense, or the National Aeronautics and Space Administration, unless the developer, deployer, or other person is performing the work on a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing; or </p>\n   <p class=\"indent\">(4) that is a covered entity within the meaning of the federal &quot;Health Insurance Portability and Accountability Act of 1996&quot;, 42 U.S.C. Secs. 1320d to 1320d-9, and the regulations promulgated under the federal act, as both may be amended from time to time, and is providing health-care recommendations that: </p>\n   <p class=\"indent\">(i) are generated by an artificial intelligence system; </p>\n   <p class=\"indent\">(ii) require a health-care provider to take action to implement the recommendations; and </p>\n   <p class=\"indent\">(iii) are not considered to be high risk. </p>\n   <p class=\"indent\">(f) nothing in this chapter applies to any artificial intelligence system that is acquired by or for the federal government or any federal agency or department, including the United States Department of Commerce, the United States Department of Defense, or the National Aeronautics and Space Administration, unless the artificial intelligence system is a high-risk artificial intelligence system that is used to make, or is a substantial factor in making, a decision concerning employment or housing. </p>\n   <p class=\"indent\">(g) an insurer, as defined in chapter 175, a fraternal benefit society, as defined in chapter 176, or a developer of an artificial intelligence system used by an insurer is in full compliance with this chapter if the insurer, the fraternal benefit society, or the developer is subject to the requirements of chapter 175 and any rules adopted by the commissioner of insurance. </p>\n   <p class=\"indent\">(h) (1) a bank, out-of-state bank, credit union chartered by the state of Massachusetts, federal credit union, out-of-state credit union, or any affiliate or subsidiary thereof, is in full compliance with this chapter if the bank, out-of-state bank, credit union chartered by the state of Massachusetts, federal credit union, out-of-state credit union, or affiliate or subsidiary is subject to examination by a state or federal prudential regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and the guidance or regulations: </p>\n   <p class=\"indent\">(i) impose requirements that are substantially equivalent to or more stringent than the requirements imposed in this chapter; and </p>\n   <p class=\"indent\">(ii) at a minimum, require the bank, out-of-state bank, credit union chartered by the state of Massachusetts, federal credit union, out-of-state credit union, or affiliate or subsidiary to: </p>\n   <p class=\"indent\">(A) regularly audit the bank&#39;s, out-of-state bank&#39;s, credit union chartered by the state of Massachusetts&#39;, federal credit union&#39;s, out-of-state credit union&#39;s, or affiliate&#39;s or subsidiary&#39;s use of high-risk artificial intelligence systems for compliance with state and federal anti-discrimination laws and regulations applicable to the bank, out-of-state bank, credit union chartered by the state of Massachusetts federal credit union, out-of-state credit union, or affiliate or subsidiary; and </p>\n   <p class=\"indent\">(B) mitigate any algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system. </p>\n   <p class=\"indent\">(2) as used in this subsection (8): </p>\n   <p class=\"indent\">(i) &quot;Affiliate&quot; has the meaning set forth in chapter 156D. </p>\n   <p class=\"indent\">(ii) &quot;Bank&quot; has the meaning set forth in chapter 167. </p>\n   <p class=\"indent\">(iii) &quot;Credit union&quot; has the meaning set forth in chapter 167. </p>\n   <p class=\"indent\">(iv) &quot;Out-of-state bank&quot; has the meaning set forth in chapter 167. </p>\n   <p class=\"indent\">(i) if a developer, a deployer, or other person engages in an action pursuant to an exemption set forth in this section, the developer, deployer, or other person bears the burden of demonstrating that the action qualifies for the exemption. </p>\n   <p class=\"indent\">Section 6. Enforcement by attorney general </p>\n   <p class=\"indent\">(a) the attorney general has exclusive authority to enforce this chapter. </p>\n   <p class=\"indent\">(b) except as provided in subsection (c) of this section, a violation of the requirements established in this chapter constitutes an unfair trade practice pursuant to chapter 93A. </p>\n   <p class=\"indent\">(c) in any action commenced by the attorney general to enforce this chapter, it is an affirmative that the developer, deployer, or other person: </p>\n   <p class=\"indent\">(1) discovers and cures a violation of this this chapter 93 as a result of: </p>\n   <p class=\"indent\">(i) feedback that the developer, deployer, or other person encourages deployers or users to provide to the developer, deployer, or other person; </p>\n   <p class=\"indent\">(ii) adversarial testing or red teaming, as those terms are defined or used by the national institute of standards and technology; or </p>\n   <p class=\"indent\">(iii) an internal review process; and </p>\n   <p class=\"indent\">(2) is otherwise in compliance with: </p>\n   <p class=\"indent\">(i) the latest version of the &quot;Artificial intelligence risk management framework&quot; published by the national institute of standards and technology in the United States Department of Commerce and Standard ISO/IEC 42001 of the International Organization for Standardization; </p>\n   <p class=\"indent\">(ii) another nationally or internationally recognized risk management framework for artificial intelligence systems, if the standards are substantially equivalent to or more stringent than the requirements of this chapter; or </p>\n   <p class=\"indent\">(iii) any risk management framework for artificial intelligence systems that the attorney general, in the attorney general&#39;s discretion, may designate and, if designated, shall publicly disseminate. </p>\n   <p class=\"indent\">(d) a developer, a deployer, or other person bears the burden of demonstrating to the attorney general that the requirements established in subsection (3) of this section have been satisfied. </p>\n   <p class=\"indent\">(e) nothing in this chapter, including the enforcement authority granted to the attorney general under this section, preempts or otherwise affects any right, claim, remedy, presumption, or defense available at law or in equity. A rebuttable presumption or affirmative defense established under this chapter applies only to an enforcement action brought by the attorney general pursuant to this section and does not apply to any right, claim, remedy, presumption, or defense available at law or in equity. </p>\n   <p class=\"indent\">(f) this chapter does not provide the basis for, and is not subject to, a private right of action for violations of this chapter or any other law. </p>\n   <p class=\"indent\">Section 7. Rules </p>\n   <p class=\"indent\">(a) the attorney general may promulgate rules as necessary for the purpose of implementing and enforcing this chapter, including: </p>\n   <p class=\"indent\">(1) the documentation and requirements for developers pursuant to section 2 (b); </p>\n   <p class=\"indent\">(2) the contents of and requirements for the notices and disclosures required by sections 2 (c) and (g); 3 (d), (e), (g), and (i); and 4; </p>\n   <p class=\"indent\">(3) the content and requirements of the risk management policy and program required by section 3 (b); </p>\n   <p class=\"indent\">(4) the content and requirements of the impact assessments required by section 3 (c); </p>\n   <p class=\"indent\">(5) the requirements for the rebuttable presumptions set forth in sections 2 and 3; and </p>\n   <p class=\"indent\">(6) the requirements for the affirmative defense set forth in section 6 (c), including the process by which the attorney general will recognize any other nationally or internationally recognized risk management framework for artificial intelligence systems. </p>\n   <p class=\"indent\">SECTION 2: This law shall take effect no later than 6 months after the passage of this bill. </p>\n  </div>\n </div>\n    </div>\t\n<table cellpadding=0 cellspacing=0 width=\"100%\">\n   <tr>\n      <td>\n          Copyright &copy; 2025 State Net\n      </td>\n<BR>\n<!--???MOVED INTO resources.cgi      <td align=\"right\">\n          <img src=\"https://custom.statenet.com/network/poweredby.gif\" alt=\"Powered by State Net\">\n      </td>-->\n\n    </tr>\n</table>\n</body></html>\n",
      "markdown": "The following has special meaning: _green underline denotes added text_ ~~red struck out text denotes deleted text~~ | [![Powered by State Net](https://custom.statenet.com/network/poweredby.gif)](https://www.lexisnexis.com/statenet/)  \n---|---  \n  \n2025 MA H 97 | | Author: | Rogers D  \n---|---  \nVersion: | Introduced  \nVersion Date: | 02/27/2025  \n  \nHOUSE No. 97\n\nThe Commonwealth of Massachusetts\n\nIn the One Hundred and Ninety-Fourth General Court\n\n(2025-2026)\n\nAN ACT ESTABLISHING THE COMPREHENSIVE MASSACHUSETTS CONSUMER DATA PRIVACY ACT.\n\nBe it enacted by the Senate and House of Representatives in General Court\nassembled, and by the authority of the same, as follows:\n\nSECTION 1. The General Laws, as appearing in the 2022 Official Edition, are\nhereby amended by inserting a new chapter:\n\nCHAPTER 93M. Consumer Protections in interactions with Artificial Intelligence\nSystems\n\nSection 1. Definitions\n\nThe following words shall, unless the context clearly requires otherwise, have\nthe following meanings:-\n\n\"Algorithmic discrimination\" means any condition in which the use of an\nartificial intelligence system results in an unlawful differential treatment\nor impact that disfavors an individual or group of individuals on the basis of\ntheir actual or perceived age, color, disability, ethnicity, genetic\ninformation, limited proficiency in the English language, national origin,\nrace, religion, reproductive health, sex, veteran status, or other\nclassification protected under the laws of this state or federal law.\n\n\"Algorithmic discrimination\" does not include:\n\n(1) the offer, license, or use of a high-risk artificial intelligence system\nby a developer or deployer for the sole purpose of:\n\n(i) the developer's or deployer's self-testing to identify, mitigate, or\nprevent discrimination or otherwise ensure compliance with state and federal\nlaw; or\n\n(ii) expanding an applicant, customer, or participant pool to increase\ndiversity or redress historical discrimination; or\n\n(2) an act or omission by or on behalf of a private club or other\nestablishment that is not in fact open to the public, as set forth in Title II\nof the federal \"Civil Rights Act of 1964\", 42 U.S.C. Sec. 2000a (e), as\namended.\n\n\"Artificial intelligence system\" means any machine-based system that, for any\nexplicit or implicit objective, infers from the inputs the system receives how\nto generate outputs, including content, decisions, predictions, or\nrecommendations, that can influence physical or virtual environments.\n\n\"Consequential decision\" means a decision that has a material legal or\nsimilarly significant effect on the provision or denial to any consumer of, or\nthe cost or terms of:\n\n(1) education enrollment or an education opportunity;\n\n(2) employment or an employment opportunity;\n\n(3) a financial or lending service;\n\n(4) an essential government service;\n\n(5) health-care services;\n\n(6) housing;\n\n(7) insurance; or\n\n(8) a legal service.\n\n\"Consumer\" means an individual who is a Massachusetts resident.\n\n\"Deploy\" means to use a high-risk artificial intelligence system.\n\n\"Deployer\" means a person doing business in this state that deploys a high-\nrisk artificial intelligence system.\n\n\"Developer\" means a person doing business in this state that develops or\nintentionally and substantially modifies an artificial intelligence system.\n\n\"Health-care services\" has the same meaning as provided in 42 U.S.C. Sec. 234\n(d)(2).\n\n\"High-risk artificial intelligence system\" means any artificial intelligence\nsystem that, when deployed, makes, or is a substantial factor in making, a\nconsequential decision.\n\n\"High-risk artificial intelligence system\" does not include:\n\n(1) an artificial intelligence system if the artificial intelligence system is\nintended to:\n\n(i) perform a narrow procedural task; or\n\n(ii) detect decision-making patterns or deviations from prior decision-making\npatterns and is not intended to replace or influence a previously completed\nhuman assessment without sufficient human review; or\n\n(2) the following technologies, unless the technologies, when deployed, make,\nor are a substantial factor in making, a consequential decision:\n\n(i) anti-fraud technology that does not use facial recognition technology;\n\n(ii) anti-malware;\n\n(iii) anti-virus;\n\n(iv) artificial intelligence-enabled video games;\n\n(v) calculators;\n\n(vi) cybersecurity;\n\n(vii) databases;\n\n(viii) data storage;\n\n(ix) firewall;\n\n(x) internet domain registration;\n\n(xi) internet website loading;\n\n(xii) networking;\n\n(xiii) spam- and robocall-filtering;\n\n(xiv) spell-checking;\n\n(xv) spreadsheets;\n\n(xvi) web caching;\n\n(xvii) web hosting or any similar technology; or\n\n(xviii) technology that communicates with consumers in natural language for\nthe purpose of providing users with information, making referrals or\nrecommendations, and answering questions and is subject to an accepted use\npolicy that prohibits generating content that is discriminatory or harmful.\n\n\"Intentional and substantial modification\" or \"intentionally and substantially\nmodifies\" means a deliberate change made to an artificial intelligence system\nthat results in any new reasonably foreseeable risk of algorithmic\ndiscrimination.\n\n\"Intentional and substantial modification\" or \"intentionally and substantially\nmodifies\" does not include a change made to a high-risk artificial\nintelligence system, or the performance of a high-risk artificial intelligence\nsystem, if:\n\n(1) the high-risk artificial intelligence system continues to learn after the\nhigh-risk artificial intelligence system is:\n\n(i) offered, sold, leased, licensed, given, or otherwise made available to a\ndeployer; or\n\n(ii) deployed;\n\n(2) the change is made to the high-risk artificial intelligence system as a\nresult of any learning described in paragraph (1)(i) of this subsection;\n\n(3) the change was predetermined by the deployer, or a third party contracted\nby the deployer, when the deployer or third party completed an initial impact\nassessment of such high-risk artificial intelligence system pursuant to\nsection 3 (c) (1); and\n\n(4) the change is included in technical documentation for the high-risk\nartificial intelligence system.\n\n\"Substantial factor\" means a factor that:\n\n(1) assists in making a consequential decision;\n\n(2) is capable of altering the outcome of a consequential decision; and\n\n(3) is generated by an artificial intelligence system.\n\n\"Substantial factor\" includes any use of an artificial intelligence system to\ngenerate any content, decision, prediction, or recommendation concerning a\nconsumer that is used as a basis to make a consequential decision concerning\nthe consumer.\n\n\"Trade secret\" has the meaning set forth in section 42 (4) of chapter 93 of\nthe General Laws, as appearing in the 2022 Official Edition.\n\nSection 2. Developer duty to avoid algorithmic discrimination - required\ndocumentation.\n\n(a) Not later than 6 months after the effective date of this act, a developer\nof a high-risk artificial intelligence system shall use reasonable care to\nprotect consumers from any known or reasonably foreseeable risks of\nalgorithmic discrimination arising from the intended and contracted uses of\nthe high-risk artificial intelligence system. In any enforcement action\nbrought not later than 6 months after the effective date of this act, by the\nattorney general pursuant to section 6, there is a rebuttable presumption that\na developer used reasonable care as required under this section if the\ndeveloper complied with this section and any additional requirements or\nobligations as set forth in rules promulgated by the attorney general pursuant\nto section 7.\n\n(b) Not later than 6 months after the effective date of this act, and except\nas provided in subsection (f) of this section, a developer of a high-risk\nartificial intelligence system shall make available to the deployer or other\ndeveloper of the high-risk artificial intelligence system:\n\n(1) a general statement describing the reasonably foreseeable uses and known\nharmful or inappropriate uses of the high-risk artificial intelligence system;\n\n(2) documentation disclosing:\n\n(i) high-level summaries of the type of data used to train the high-risk\nartificial intelligence system;\n\n(ii) known or reasonably foreseeable limitations of the high-risk artificial\nintelligence system, including known or reasonably foreseeable risks of\nalgorithmic discrimination arising from the intended uses of the high-risk\nartificial intelligence system;\n\n(iii) the purpose of the high-risk artificial intelligence system;\n\n(iv) the intended benefits and uses of the high-risk artificial intelligence\nsystem; and\n\n(v) all other information necessary to allow the deployer to comply with the\nrequirements of section 3;\n\n(3) documentation describing:\n\n(i) how the high-risk artificial intelligence system was evaluated for\nperformance and mitigation of algorithmic discrimination before the high-risk\nartificial intelligence system was offered, sold, leased, licensed, given, or\notherwise made available to the deployer;\n\n(ii) the data governance measures used to cover the training datasets and the\nmeasures used to examine the suitability of data sources, possible biases, and\nappropriate mitigation;\n\n(iii) the intended outputs of the high-risk artificial intelligence system;\n\n(iv) the measures the developer has taken to mitigate known or reasonably\nforeseeable risks of algorithmic discrimination that may arise from the\nreasonably foreseeable deployment of the high-risk artificial intelligence\nsystem; and\n\n(v) how the high-risk artificial intelligence system should be used, not be\nused, and be monitored by an individual when the high-risk artificial\nintelligence system is used to make, or is a substantial factor in making, a\nconsequential decision; and\n\n(4) any additional documentation that is reasonably necessary to assist the\ndeployer in understanding the outputs and monitor the performance of the high-\nrisk artificial intelligence system for risks of algorithmic discrimination.\n\n(c) (1) except as provided in subsection (f) of this section, a developer that\noffers, sells, leases, licenses, gives, or otherwise makes available to a\ndeployer or other developer a high-risk artificial intelligence system not\nlater than 6 months after the effective date of this act, shall make available\nto the deployer or other developer, to the extent feasible, the documentation\nand information, through artifacts such as model cards, dataset cards, or\nother impact assessments, necessary for a deployer, or for a third party\ncontracted by a deployer, to complete an impact assessment pursuant to section\n3 (c).\n\n(2) a developer that also serves as a deployer for a high-risk artificial\nintelligence system is not required to generate the documentation required by\nthis section unless the high-risk artificial intelligence system is provided\nto an unaffiliated entity acting as a deployer.\n\n(d) (1) Not later than 6 months after the effective date of this act, a\ndeveloper shall make available, in a manner that is clear and readily\navailable on the developer's website or in a public use case inventory, a\nstatement summarizing:\n\n(i) the types of high-risk artificial intelligence systems that the developer\nhas developed or intentionally and substantially modified and currently makes\navailable to a deployer or other developer; and\n\n(ii) how the developer manages known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the development or intentional\nand substantial modification of the types of high-risk artificial intelligence\nsystems described in accordance with subsection (d)(1)(i) of this section.\n\n(2) a developer shall update the statement described in subsection (d)(1) of\nthis section:\n\n(i) as necessary to ensure that the statement remains accurate; and\n\n(ii) no later than ninety days after the developer intentionally and\nsubstantially modifies any high-risk artificial intelligence system described\nin subsection (d)(1)(i) of this section.\n\n(e) Not later than 6 months after the effective date of this act, a developer\nof a high-risk artificial intelligence system shall disclose to the attorney\ngeneral, in a form and manner prescribed by the attorney general, and to all\nknown deployers or other developers of the high-risk artificial intelligence\nsystem, any known or reasonably foreseeable risks of algorithmic\ndiscrimination arising from the intended uses of the high-risk artificial\nintelligence system without unreasonable delay but no later than ninety days\nafter the date on which:\n\n(1) the developer discovers through the developer's ongoing testing and\nanalysis that the developer's high-risk artificial intelligence system has\nbeen deployed and has caused or is reasonably likely to have caused\nalgorithmic discrimination; or\n\n(2) the developer receives from a deployer a credible report that the high-\nrisk artificial intelligence system has been deployed and has caused\nalgorithmic discrimination.\n\n(f) nothing in subsections (b) to (e) of this section requires a developer to\ndisclose a trade secret, information protected from disclosure by state or\nfederal law, or information that would create a security risk to the\ndeveloper.\n\n(g) Not later than 6 months after the effective date of this act, the attorney\ngeneral may require that a developer disclose to the attorney general, no\nlater than ninety days after the request and in a form and manner prescribed\nby the attorney general, the statement or documentation described in\nsubsection (b) of this section. The attorney general may evaluate such\nstatement or documentation to ensure compliance with this chapter, and the\nstatement or documentation is not subject to disclosure under the\n\"Massachusetts Public Records Law\", chapter 66, section 10 of the General\nLaws. In a disclosure pursuant to this subsection (g), a developer may\ndesignate the statement or documentation as including proprietary information\nor a trade secret. To the extent that any information contained in the\nstatement or documentation includes information subject to attorney-client\nprivilege or work-product protection, the disclosure does not constitute a\nwaiver of the privilege or protection.\n\nSection 3. Deployer duty to avoid algorithmic discrimination - risk management\npolicy and program.\n\n(a) Not later than 6 months after the effective date of this act, a deployer\nof a high-risk artificial intelligence system shall use reasonable care to\nprotect consumers from any known or reasonably foreseeable risks of\nalgorithmic discrimination. In any enforcement action brought not later than 6\nmonths after the effective date of this act, by the attorney general pursuant\nto section 6, there is a rebuttable presumption that a deployer of a high-risk\nartificial intelligence system used reasonable care as required under this\nsection if the deployer complied with this section and any additional\nrequirements or obligations as set forth in rules promulgated by the attorney\ngeneral pursuant to section 7.\n\n(b) (1) Not later than 6 months after the effective date of this act, and\nexcept as provided in subsection (f) of this section, a deployer of a high-\nrisk artificial intelligence system shall implement a risk management policy\nand program to govern the deployer's deployment of the high-risk artificial\nintelligence system. The risk management policy and program must specify and\nincorporate the principles, processes, and personnel that the deployer uses to\nidentify, document, and mitigate known or reasonably foreseeable risks of\nalgorithmic discrimination. The risk management policy and program must be an\niterative process planned, implemented, and regularly and systematically\nreviewed and updated over the life cycle of a high-risk artificial\nintelligence system, requiring regular, systematic review and updates. A risk\nmanagement policy and program implemented and maintained pursuant to this\nsubsection (b) must be reasonable considering:\n\n(i) (A) the guidance and standards set forth in the latest version of the\n\"Artificial Intelligence Risk Management Framework\" published by the National\nInstitute of Standards and Technology in the United States Department of\nCommerce, standard ISO/IEC 42001 of the International Organization for\nStandardization, or another nationally or internationally recognized risk\nmanagement framework for artificial intelligence systems, if the standards are\nsubstantially equivalent to or more stringent than the requirements of this\nchapter; or\n\n(B) any risk management framework for artificial intelligence systems that the\nattorney general, in the attorney general's discretion, may designate;\n\n(ii) the size and complexity of the deployer;\n\n(iii) the nature and scope of the high-risk artificial intelligence systems\ndeployed by the deployer, including the intended uses of the high-risk\nartificial intelligence systems; and\n\n(iv) the sensitivity and volume of data processed in connection with the high-\nrisk artificial intelligence systems deployed by the deployer.\n\n(2) a risk management policy and program implemented pursuant to subsection\n(b)(1) of this section may cover multiple high-risk artificial intelligence\nsystems deployed by the deployer.\n\n(c) (1) except as provided in subsections (c)(4), (c)(5), and (f) of this\nsection:\n\n(i) a deployer, or a third party contracted by the deployer, that deploys a\nhigh-risk artificial intelligence system not later than 6 months after the\neffective date of this act, shall complete an impact assessment for the high-\nrisk artificial intelligence system; and\n\n(ii) Not later than 6 months after the effective date of this act, a deployer,\nor a third party contracted by the deployer, shall complete an impact\nassessment for a deployed high-risk artificial intelligence system at least\nannually and within ninety days after any intentional and substantial\nmodification to the high-risk artificial intelligence system is made\navailable.\n\n(2) an impact assessment completed pursuant to this subsection (c) must\ninclude, at a minimum, and to the extent reasonably known by or available to\nthe deployer:\n\n(i) a statement by the deployer disclosing the purpose, intended use cases,\nand deployment context of, and benefits afforded by, the high-risk artificial\nintelligence system;\n\n(ii) an analysis of whether the deployment of the high-risk artificial\nintelligence system poses any known or reasonably foreseeable risks of\nalgorithmic discrimination and, if so, the nature of the algorithmic\ndiscrimination and the steps that have been taken to mitigate the risks;\n\n(iii) a description of the categories of data the high-risk artificial\nintelligence system processes as inputs and the outputs the high-risk\nartificial intelligence system produces;\n\n(iv) if the deployer used data to customize the high-risk artificial\nintelligence system, an overview of the categories of data the deployer used\nto customize the high-risk artificial intelligence system;\n\n(v) any metrics used to evaluate the performance and known limitations of the\nhigh-risk artificial intelligence system;\n\n(vi) a description of any transparency measures taken concerning the high-risk\nartificial intelligence system, including any measures taken to disclose to a\nconsumer that the high-risk artificial intelligence system is in use when the\nhigh-risk artificial intelligence system is in use; and\n\n(vii) a description of the post-deployment monitoring and user safeguards\nprovided concerning the high-risk artificial intelligence system, including\nthe oversight, use, and learning process established by the deployer to\naddress issues arising from the deployment of the high-risk artificial\nintelligence system.\n\n(3) in addition to the information required under subsection (3)(b) of this\nsection, an impact assessment completed pursuant to this subsection (c)\nfollowing an intentional and substantial modification to a high-risk\nartificial intelligence system not later than 6 months after the effective\ndate of this act, must include a statement disclosing the extent to which the\nhigh-risk artificial intelligence system was used in a manner that was\nconsistent with, or varied from, the developer's intended uses of the high-\nrisk artificial intelligence system.\n\n(4) a single impact assessment may address a comparable set of high-risk\nartificial intelligence systems deployed by a deployer.\n\n(5) if a deployer, or a third party contracted by the deployer, completes an\nimpact assessment for the purpose of complying with another applicable law or\nregulation, the impact assessment satisfies the requirements established in\nthis subsection (c) if the impact assessment is reasonably similar in scope\nand effect to the impact assessment that would otherwise be completed pursuant\nto this subsection (c).\n\n(6) a deployer shall maintain the most recently completed impact assessment\nfor a high-risk artificial intelligence system as required under this\nsubsection (c), all records concerning each impact assessment, and all prior\nimpact assessments, if any, for at least three years following the final\ndeployment of the high-risk artificial intelligence system.\n\n(7) Not later than 6 months after the effective date of this act, and at least\nannually thereafter, a deployer, or a third party contracted by the deployer,\nmust review the deployment of each high-risk artificial intelligence system\ndeployed by the deployer to ensure that the high-risk artificial intelligence\nsystem is not causing algorithmic discrimination.\n\n(d) (1) Not later than 6 months after the effective date of this act, and no\nlater than the time that a deployer deploys a high-risk artificial\nintelligence system to make, or be a substantial factor in making, a\nconsequential decision concerning a consumer, the deployer shall:\n\n(i) notify the consumer that the deployer has deployed a high-risk artificial\nintelligence system to make, or be a substantial factor in making, a\nconsequential decision before the decision is made;\n\n(ii) provide to the consumer a statement disclosing the purpose of the high-\nrisk artificial intelligence system and the nature of the consequential\ndecision; the contact information for the deployer; a description, in plain\nlanguage, of the high-risk artificial intelligence system; and instructions on\nhow to access the statement required by subsection (5)(a) of this section; and\n\n(iii) provide to the consumer information, if applicable, regarding the\nconsumer's right to opt out of the processing of personal data concerning the\nconsumer for purposes of profiling in furtherance of decisions that produce\nlegal or similarly significant effects concerning the consumer.\n\n(2) Not later than 6 months after the effective date of this act, a deployer\nthat has deployed a high-risk artificial intelligence system to make, or be a\nsubstantial factor in making, a consequential decision concerning a consumer\nshall, if the consequential decision is adverse to the consumer, provide to\nthe consumer:\n\n(i) a statement disclosing the principal reason or reasons for the\nconsequential decision, including:\n\n(A) the degree to which, and manner in which, the high-risk artificial\nintelligence system contributed to the consequential decision;\n\n(B) the type of data that was processed by the high-risk artificial\nintelligence system in making the consequential decision; and\n\n(C) the source or sources of the data described in subsection (d)(2)(i)(B) of\nthis section;\n\n(ii) an opportunity to correct any incorrect personal data that the high-risk\nartificial intelligence system processed in making, or as a substantial factor\nin making, the consequential decision; and\n\n(iii) an opportunity to appeal an adverse consequential decision concerning\nthe consumer arising from the deployment of a high-risk artificial\nintelligence system, which appeal must, if technically feasible, allow for\nhuman review unless providing the opportunity for appeal is not in the best\ninterest of the consumer, including in instances in which any delay might pose\na risk to the life or safety of such consumer.\n\n(3) (i) except as provided in subsection (d)(3)(ii) of this section, a\ndeployer shall provide the notice, statement, contact information, and\ndescription required by subsections (c)(1) and (d)(2) of this section:\n\n(A) directly to the consumer;\n\n(B) in plain language;\n\n(C) in all languages in which the deployer, in the ordinary course of the\ndeployer's business, provides contracts, disclaimers, sale announcements, and\nother information to consumers; and\n\n(D) in a format that is accessible to consumers with disabilities.\n\n(ii) if the deployer is unable to provide the notice, statement, contact\ninformation, and description required by subsections (d)(1) and (d)(2) of this\nsection directly to the consumer, the deployer shall make the notice,\nstatement, contact information, and description available in a manner that is\nreasonably calculated to ensure that the consumer receives the notice,\nstatement, contact information, and description.\n\n(e) (1) Not later than 6 months after the effective date of this act, and\nexcept as provided in subsection (f) of this section, a deployer shall make\navailable, in a manner that is clear and readily available on the deployer's\nwebsite, a statement summarizing:\n\n(i) the types of high-risk artificial intelligence systems that are currently\ndeployed by the deployer;\n\n(ii) how the deployer manages known or reasonably foreseeable risks of\nalgorithmic discrimination that may arise from the deployment of each high-\nrisk artificial intelligence system described pursuant to subsection (e)(1)(i)\nof this section; and\n\n(iii) in detail, the nature, source, and extent of the information collected\nand used by the deployer.\n\n(2) a deployer shall periodically update the statement described in subsection\n(e)(1) of this section.\n\n(f) subsections (b), (c), and (e) of this section do not apply to a deployer\nif, at the time the deployer deploys a high-risk artificial intelligence\nsystem and at all times while the high-risk artificial intelligence system is\ndeployed:\n\n(1) the deployer:\n\n(i) employs fewer than fifty full-time equivalent employees; and\n\n(ii) does not use the deployer's own data to train the high-risk artificial\nintelligence system;\n\n(2) the high-risk artificial intelligence system:\n\n(i) is used for the intended uses that are disclosed to the deployer as\nrequired by section 2 (b)(1); and\n\n(ii) continues learning based on data derived from sources other than the\ndeployer's own data; and\n\n(3) the deployer makes available to consumers any impact assessment that:\n\n(i) the developer of the high-risk artificial intelligence system has\ncompleted and provided to the deployer; and\n\n(ii) includes information that is substantially similar to the information in\nthe impact assessment required under of this section.\n\n(g) if a deployer deploys a high-risk artificial intelligence system not later\nthan 6 months after the effective date of this act, and subsequently discovers\nthat the high-risk artificial intelligence system has caused algorithmic\ndiscrimination, the deployer, without unreasonable delay, but no later than\nninety days after the date of the discovery, shall send subsection (c)(2) to\nthe attorney general, in a form and manner prescribed by the attorney general,\na notice disclosing the discovery.\n\n(h) nothing in subsections (b) to (e) and (g) of this section requires a\ndeployer to disclose a trade secret or information protected from disclosure\nby state or federal law. To the extent that a deployer withholds information\npursuant to this subsection (h) or section 5 (e), the deployer shall notify\nthe consumer and provide a basis for the withholding.\n\n(i) Not later than 6 months after the effective date of this act, the attorney\ngeneral may require that a deployer, or a third party contracted by the\ndeployer, disclose to the attorney general, no later than ninety days after\nthe request and in a form and manner prescribed by the attorney general, the\nrisk management policy implemented pursuant to subsection (b) of this section,\nthe impact assessment completed pursuant to subsection (c) of this section, or\nthe records maintained pursuant to subsection (c)(6) of this section. The\nattorney general may evaluate the risk management policy, impact assessment,\nor records to ensure compliance with this chapter, and the risk management\npolicy, impact assessment, and records are not subject to disclosure under the\n\"Massachusetts Public Records Law\", chapter 66, section 10 of the General\nLaws. In a disclosure pursuant to this subsection (i), a deployer may\ndesignate the statement or documentation as including proprietary information\nor a trade secret. To the extent that any information contained in the risk\nmanagement policy, impact assessment, or records include information subject\nto attorney-client privilege or work-product protection, the disclosure does\nnot constitute a waiver of the privilege or protection.\n\nSection 4. Disclosure of an artificial intelligence system to consumer\n\n(a) Not later than 6 months after the effective date of this act, and except\nas provided in subsection (b) of this section, a deployer or other developer\nthat deploys, offers, sells, leases, licenses, gives, or otherwise makes\navailable an artificial intelligence system that is intended to interact with\nconsumers shall ensure the disclosure to each consumer who interacts with the\nartificial intelligence system that the consumer is interacting with an\nartificial intelligence system.\n\n(b) disclosure is not required under subsection (a) of this section under\ncircumstances in which it would be obvious to a reasonable person that the\nperson is interacting with an artificial intelligence system.\n\nSection 5. Compliance with other legal obligations - definitions\n\n(a) nothing in this chapter restricts a developer's, a deployer's, or other\nperson's ability to:\n\n(1) comply with federal, state, or municipal laws, ordinances, or regulations;\n\n(2) comply with a civil, criminal, or regulatory inquiry, investigation,\nsubpoena, or summons by a federal, a state, a municipal, or other governmental\nauthority;\n\n(3) cooperate with a law enforcement agency concerning conduct or activity\nthat the developer, deployer, or other person reasonably and in good faith\nbelieves may violate federal, state, or municipal laws, ordinances, or\nregulations;\n\n(4) investigate, establish, exercise, prepare for, or defend legal claims;\n\n(5) take immediate steps to protect an interest that is essential for the life\nor physical safety of a consumer or another individual;\n\n(6) by any means other than the use of facial recognition technology, prevent,\ndetect, protect against, or respond to security incidents, identity theft,\nfraud, harassment, malicious or deceptive activities, or illegal activity;\ninvestigate, report, or prosecute the persons responsible for any such action;\nor preserve the integrity or security of systems;\n\n(7) engage in public or peer-reviewed scientific or statistical research in\nthe public interest that adheres to all other applicable ethics and privacy\nlaws and is conducted in accordance with 45 CFR 46, as amended, or relevant\nrequirements established by the federal Food and Drug Administration;\n\n(8) conduct research, testing, and development activities regarding an\nartificial intelligence system or model, other than testing conducted under\nreal-world conditions, before the artificial intelligence system or model is\nplaced on the market, deployed, or put into service, as applicable; or\n\n(i) assist another developer, deployer, or other person with any of the\nobligations imposed under this chapter.\n\n(b) the obligations imposed on developers, deployers, or other persons under\nthis chapter do not restrict a developer's, a deployer's, or other person's\nability to:\n\n(1) effectuate a product recall; or\n\n(2) identify and repair technical errors that impair existing or intended\nfunctionality.\n\n(c) the obligations imposed on developers, deployers, or other persons under\nthis chapter do not apply where compliance with this chapter by the developer,\ndeployer, or other person would violate an evidentiary privilege under the\nlaws of this state.\n\n(d) nothing in this chapter imposes any obligation on a developer, a deployer,\nor other person that adversely affects the rights or freedoms of a person,\nincluding the rights of a person to freedom of speech or freedom of the press\nthat are guaranteed in:\n\n(1) the First Amendment to the United States constitution; or\n\n(2) Part the First, Article XVI of the state constitution.\n\n(e) nothing in this chapter applies to a developer, a deployer, or other\nperson:\n\n(1) insofar as the developer, deployer, or other person develops, deploys,\nputs into service, or intentionally and substantially modifies, as applicable,\na high-risk artificial intelligence system:\n\n(i) that has been approved, authorized, certified, cleared, developed, or\ngranted by a federal agency, such as the federal food and drug administration\nor the federal aviation administration, acting within the scope of the federal\nagency's authority, or by a regulated entity subject to the supervision and\nregulation of the federal housing finance agency; or\n\n(ii) in compliance with standards established by a federal agency, including\nstandards established by the federal office of the national coordinator for\nhealth information technology, or by a regulated entity subject to the\nsupervision and regulation of the federal housing finance agency, if the\nstandards are substantially equivalent or more stringent than the requirements\nof this chapter;\n\n(2) conducting research to support an application for approval or\ncertification from a federal agency, including the federal Aviation\nAdministration, the federal Communications Commission, or the federal Food and\nDrug Administration or research to support an application otherwise subject to\nreview by the federal agency;\n\n(3) performing work under, or in connection with, a contract with the United\nStates Department of Commerce, the United States Department of Defense, or the\nNational Aeronautics and Space Administration, unless the developer, deployer,\nor other person is performing the work on a high-risk artificial intelligence\nsystem that is used to make, or is a substantial factor in making, a decision\nconcerning employment or housing; or\n\n(4) that is a covered entity within the meaning of the federal \"Health\nInsurance Portability and Accountability Act of 1996\", 42 U.S.C. Secs. 1320d\nto 1320d-9, and the regulations promulgated under the federal act, as both may\nbe amended from time to time, and is providing health-care recommendations\nthat:\n\n(i) are generated by an artificial intelligence system;\n\n(ii) require a health-care provider to take action to implement the\nrecommendations; and\n\n(iii) are not considered to be high risk.\n\n(f) nothing in this chapter applies to any artificial intelligence system that\nis acquired by or for the federal government or any federal agency or\ndepartment, including the United States Department of Commerce, the United\nStates Department of Defense, or the National Aeronautics and Space\nAdministration, unless the artificial intelligence system is a high-risk\nartificial intelligence system that is used to make, or is a substantial\nfactor in making, a decision concerning employment or housing.\n\n(g) an insurer, as defined in chapter 175, a fraternal benefit society, as\ndefined in chapter 176, or a developer of an artificial intelligence system\nused by an insurer is in full compliance with this chapter if the insurer, the\nfraternal benefit society, or the developer is subject to the requirements of\nchapter 175 and any rules adopted by the commissioner of insurance.\n\n(h) (1) a bank, out-of-state bank, credit union chartered by the state of\nMassachusetts, federal credit union, out-of-state credit union, or any\naffiliate or subsidiary thereof, is in full compliance with this chapter if\nthe bank, out-of-state bank, credit union chartered by the state of\nMassachusetts, federal credit union, out-of-state credit union, or affiliate\nor subsidiary is subject to examination by a state or federal prudential\nregulator under any published guidance or regulations that apply to the use of\nhigh-risk artificial intelligence systems and the guidance or regulations:\n\n(i) impose requirements that are substantially equivalent to or more stringent\nthan the requirements imposed in this chapter; and\n\n(ii) at a minimum, require the bank, out-of-state bank, credit union chartered\nby the state of Massachusetts, federal credit union, out-of-state credit\nunion, or affiliate or subsidiary to:\n\n(A) regularly audit the bank's, out-of-state bank's, credit union chartered by\nthe state of Massachusetts', federal credit union's, out-of-state credit\nunion's, or affiliate's or subsidiary's use of high-risk artificial\nintelligence systems for compliance with state and federal anti-discrimination\nlaws and regulations applicable to the bank, out-of-state bank, credit union\nchartered by the state of Massachusetts federal credit union, out-of-state\ncredit union, or affiliate or subsidiary; and\n\n(B) mitigate any algorithmic discrimination caused by the use of a high-risk\nartificial intelligence system or any risk of algorithmic discrimination that\nis reasonably foreseeable as a result of the use of a high-risk artificial\nintelligence system.\n\n(2) as used in this subsection (8):\n\n(i) \"Affiliate\" has the meaning set forth in chapter 156D.\n\n(ii) \"Bank\" has the meaning set forth in chapter 167.\n\n(iii) \"Credit union\" has the meaning set forth in chapter 167.\n\n(iv) \"Out-of-state bank\" has the meaning set forth in chapter 167.\n\n(i) if a developer, a deployer, or other person engages in an action pursuant\nto an exemption set forth in this section, the developer, deployer, or other\nperson bears the burden of demonstrating that the action qualifies for the\nexemption.\n\nSection 6. Enforcement by attorney general\n\n(a) the attorney general has exclusive authority to enforce this chapter.\n\n(b) except as provided in subsection (c) of this section, a violation of the\nrequirements established in this chapter constitutes an unfair trade practice\npursuant to chapter 93A.\n\n(c) in any action commenced by the attorney general to enforce this chapter,\nit is an affirmative that the developer, deployer, or other person:\n\n(1) discovers and cures a violation of this this chapter 93 as a result of:\n\n(i) feedback that the developer, deployer, or other person encourages\ndeployers or users to provide to the developer, deployer, or other person;\n\n(ii) adversarial testing or red teaming, as those terms are defined or used by\nthe national institute of standards and technology; or\n\n(iii) an internal review process; and\n\n(2) is otherwise in compliance with:\n\n(i) the latest version of the \"Artificial intelligence risk management\nframework\" published by the national institute of standards and technology in\nthe United States Department of Commerce and Standard ISO/IEC 42001 of the\nInternational Organization for Standardization;\n\n(ii) another nationally or internationally recognized risk management\nframework for artificial intelligence systems, if the standards are\nsubstantially equivalent to or more stringent than the requirements of this\nchapter; or\n\n(iii) any risk management framework for artificial intelligence systems that\nthe attorney general, in the attorney general's discretion, may designate and,\nif designated, shall publicly disseminate.\n\n(d) a developer, a deployer, or other person bears the burden of demonstrating\nto the attorney general that the requirements established in subsection (3) of\nthis section have been satisfied.\n\n(e) nothing in this chapter, including the enforcement authority granted to\nthe attorney general under this section, preempts or otherwise affects any\nright, claim, remedy, presumption, or defense available at law or in equity. A\nrebuttable presumption or affirmative defense established under this chapter\napplies only to an enforcement action brought by the attorney general pursuant\nto this section and does not apply to any right, claim, remedy, presumption,\nor defense available at law or in equity.\n\n(f) this chapter does not provide the basis for, and is not subject to, a\nprivate right of action for violations of this chapter or any other law.\n\nSection 7. Rules\n\n(a) the attorney general may promulgate rules as necessary for the purpose of\nimplementing and enforcing this chapter, including:\n\n(1) the documentation and requirements for developers pursuant to section 2\n(b);\n\n(2) the contents of and requirements for the notices and disclosures required\nby sections 2 (c) and (g); 3 (d), (e), (g), and (i); and 4;\n\n(3) the content and requirements of the risk management policy and program\nrequired by section 3 (b);\n\n(4) the content and requirements of the impact assessments required by section\n3 (c);\n\n(5) the requirements for the rebuttable presumptions set forth in sections 2\nand 3; and\n\n(6) the requirements for the affirmative defense set forth in section 6 (c),\nincluding the process by which the attorney general will recognize any other\nnationally or internationally recognized risk management framework for\nartificial intelligence systems.\n\nSECTION 2: This law shall take effect no later than 6 months after the passage\nof this bill.\n\nCopyright (C) 2025 State Net  \n  \n---\n\n",
      "latest_version": true
    }
  ]
}